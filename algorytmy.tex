\documentclass[11pt,a4paper]{article}
% Kodowanie i obsługa języka polskiego
\usepackage[utf8]{inputenc}      % Kodowanie wejścia
\usepackage[T1]{fontenc}         % Kodowanie fontów
\usepackage[polish]{babel}       % Obsługa języka polskiego

% Pakiety matematyczne i inne przydatne
\usepackage[makeroom]{cancel}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{marginnote}
\usepackage{tikz}
\usetikzlibrary{trees}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{amsmath, amssymb, amsthm}  % Pakiety do matematyki
\usepackage{graphicx}                  % Obsługa grafiki
\usepackage{hyperref}                  % Linki i spis treści
\usepackage{geometry}                  % Ustawienie marginesów
\usepackage{algorithm}               % Algorytmy
\usepackage{algpseudocode}           % Pseudokod
\geometry{margin=2cm}                  % Ustawienie marginesów na 2 cm

% Dodatkowe ustawienia
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\title{Notatki z Algorytmów i Struktur Danych}
\author{Jakub Kogut}

\begin{document}

\maketitle

\tableofcontents  % Spis treści (opcjonalnie)
\newpage

\section{Wstęp}
To będa notatki z przedmiotu Algorytmy i struktury danych na Politechnice Wrocławskiej na kierunku Informatyka Algorytmiczna rok 2025 semestr letni.
\subsection{Informacje}
Prowadzący Przedmiot: \textbf{Zbychu Gołębiewski}
\begin{itemize}
    \item Należy kontaktować się przez maila: \href{mailto:zbigniew.golebiewski@pwr.edu.pl}{mail}
    \item Konsultacje \textbf{216/D1}:
        \begin{itemize}
            \item Wtorek 13:00-15:00
            \item Środa 9:00-11:00
        \end{itemize}
    \item Wiecej info na stronie \href{https://cs.pwr.edu.pl/golebiewski/teaching/aisd.php}{przedmiotu}
    \item Literatura
        \begin{itemize}
            \item Algorithms, Dasgupta, Papadimitriou, Vazirani
            \item Algorithms, Sedgewick, Wayne (strona internetowa książki)
            \item Algorithms Designs, Jon Kleinberg and Eva Trados
            \item Wprowadzenie do algorytmów, Cormen, Leiserson, Rivest, Stein
            \item Sztuka programowania (wszystkie tomy), Donald E. Knuth
        \end{itemize}
\end{itemize}
\subsection{Ocenianie}
Ocena z kursu składa się z:
\begin{itemize}
    \item Oceny z egzaminu -- E
    \item Oceny z ćwiczeń -- C
    \item Oceny z laboratorium -- L
\end{itemize}
Wszystkie oceny są z zakresu $[0,100]$. Ocena końcowa jest wyliczana ze wzoru:
\[
    K = \frac{1}{2}E + \frac{1}{4}C + \frac{1}{4}L
\]

\section{Wykład \date{2025-03-03}}
\subsection{Przykładowy Problem}
Sortowanie:
\begin{itemize}
    \item Input: $n$ liczb $a_1, a_2, \ldots, a_n, |A|$, gdzie $|A|$ to długośc tablicy
    \item Output: permutacja $a_1', a_2', \ldots, a_n'$ taka, że $a_1' \leq a_2' \leq \ldots \leq a_n'$
\end{itemize}
Najważniejsze w algorytmach jest to, żeby były POPRAWNE: edge case, ...

\subsection{Jak mierzyć złożoność algorytmów}
\begin{enumerate}
    \item Worst Case Analysis T(n) $\leftarrow$ stosowane najcześciej
    \item Average Case Analysis
        \begin{itemize}
            \item zakładamy pewnien rozkład prawdopodobieństwa na danych wejściowych
            \item $T$ -- zmienna losowa liczby operacji wykonanych przez algorytm
                \[
                    T(n) = \max\{\# \text{operacji dla danego wejścia}\}
                \]
            \item $E[T]$ -- wartość oczekiwana $T$ $\rightarrow$ średnia liczba operacji, to co nas interesuje
        \end{itemize}
\end{enumerate}
\subsection{Przykład algorytmu}
W tej sekcji mamy pokazany przykład jak pisać pseudo kod:
\begin{algorithm}[H]
    \caption{Merge Sort}\label{alg:merge_sort}
    \begin{algorithmic}[1]
    \Procedure{MergeSort}{A, 1, n}
        \If{|A[1..n]| == 1}
            \State \Return{A[1..n]}
        \Else
            \State $B = \text{MergeSort}(A, 1, \lfloor n/2 \rfloor)$
            \State $C = \text{MergeSort}(A, \lfloor n/2 \rfloor, n)$
            \State \Return{Merge(B, C)}
        \EndIf
    \EndProcedure
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
    \caption{Merge}\label{alg:merge}
    \begin{algorithmic}[1]
    \Procedure{Merge}{X[1..k], Y[1..n]}
        \If{$X = \emptyset$}
            \State \Return{$Y$}
        \ElsIf{$Y = \emptyset$}
            \State \Return{$X$}
        \ElsIf{$X[1] \leq Y[1]$}
            \State \Return{$[X[1]] \times \text{Merge}(X[2..k], Y[1..n])$}
        \Else
            \State \Return{$[Y[1]] \times \text{Merge}(X[1..k], Y[2..n])$}
        \EndIf
    \EndProcedure
    \end{algorithmic}
\end{algorithm}
\subsection{Przykład działania Merge Sort}
\textbf{Example: Sorting the array \([10,\, 2,\, 5,\, 3,\, 7,\, 13,\, 1,\, 6]\) step by step}

\begin{enumerate}
  \item \textbf{Initial split:}
  \[
    [\,10,\, 2,\, 5,\, 3,\, 7,\, 13,\, 1,\, 6\,]
    \quad\longrightarrow\quad
    [\,10,\, 2,\, 5,\, 3\,] \quad\text{and}\quad [\,7,\, 13,\, 1,\, 6\,].
  \]

  \item \textbf{Sort the left half \([10,\, 2,\, 5,\, 3]\):}
  \begin{enumerate}
    \item Split into \([10,\, 2]\) and \([5,\, 3]\).
    \item \(\text{MergeSort}([10,\, 2])\):
    \begin{itemize}
      \item Split into \([10]\) and \([2]\).
      \item Each is already sorted (single element).
      \item Merge: \([2,\, 10]\).
    \end{itemize}
    \item \(\text{MergeSort}([5,\, 3])\):
    \begin{itemize}
      \item Split into \([5]\) and \([3]\).
      \item Each is already sorted.
      \item Merge: \([3,\, 5]\).
    \end{itemize}
    \item Merge \([2,\, 10]\) and \([3,\, 5]\) to get \([2,\, 3,\, 5,\, 10]\).
  \end{enumerate}

  \item \textbf{Sort the right half \([7,\, 13,\, 1,\, 6]\):}
  \begin{enumerate}
    \item Split into \([7,\, 13]\) and \([1,\, 6]\).
    \item \(\text{MergeSort}([7,\, 13])\):
    \begin{itemize}
      \item Split into \([7]\) and \([13]\).
      \item Each is already sorted.
      \item Merge: \([7,\, 13]\).
    \end{itemize}
    \item \(\text{MergeSort}([1,\, 6])\):
    \begin{itemize}
      \item Split into \([1]\) and \([6]\).
      \item Each is already sorted.
      \item Merge: \([1,\, 6]\).
    \end{itemize}
    \item Merge \([7,\, 13]\) and \([1,\, 6]\) to get \([1,\, 6,\, 7,\, 13]\).
  \end{enumerate}

  \item \textbf{Final merge:} Merge the two sorted halves:
  \[
    [\,2,\, 3,\, 5,\, 10\,] \quad\text{and}\quad [\,1,\, 6,\, 7,\, 13\,]
    \quad\longrightarrow\quad [\,1,\, 2,\, 3,\, 5,\, 6,\, 7,\, 10,\, 13\,].
  \]
\end{enumerate}

\noindent
Hence, after all the recursive splits and merges, the final sorted array is:
\[
[\,1,\, 2,\, 3,\, 5,\, 6,\, 7,\, 10,\, 13\,].
\]

\subsection{Złożoność Merge Sort}
\begin{itemize}
    \item Złożoność czasowa
        \begin{itemize}
            \item $T(n) = 2T(n/2) + \Theta(n)$
            \item $T(n) = \Theta(n \log n)$
        \end{itemize}
    \item Złożoność pamięciowa
        \begin{itemize}
            \item $M(n) = n + M(n/2)$
            \item $M(n) = \Theta(n)$
        \end{itemize}
\end{itemize}

\section{Wykład \date{2025-03-10}}
\subsection{Notacja Asypmtotyczna}
Na wykładzie będziemy omawiali:
\begin{itemize}
    \item Notację dużego O $O(n)$ //ograniczenie górne
        \begin{itemize}
                \item Definicja $O(n)$:
                \[
                    O(g(n)) = \{ f(n) \mid \exists c > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq f(n) \leq c \cdot g(n) \}
                \]
            \item Uwaga! \newline
                Jeśli
                \[
                    \limsup_{n \to \infty} \frac{f(n)}{g(n)} < \infty
                \]
                to
                \[
                    \limsup_{n \to \infty} \frac{f(n)}{g(n)} = \lim_{n \to \infty} \frac{f(n)}{g(n)}
                \]
            \item Przykład:
                \begin{itemize}
                    \item $2n^2=O(n^3)$
                        dla $n_0 = 2, c = 1$ Definicja jest spełniona
                    \item $f(n) = n^3 + O(n^2)$ jest to jeden z sposobów użycia $O(n)$
                        \[
                            \exists h(n) = O(n^2) \quad \text{takie, że} \quad f(n) = n^3 + h(n)
                        \]
                \end{itemize}
        \end{itemize}
    \item Notację omega //ograniczenie dolne
        \begin{itemize}
            \item Definicja
                \[
                    \Omega(g(n)) = \{ f(n) \mid \exists c > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq c \cdot g(n) \leq f(n) \}
                \]
            \item Przykład
                \begin{itemize}
                    \item $n^3 = \Omega(2n^2)$
                    \item $n = \Omega(\log n)$
                \end{itemize}
        \end{itemize}
    \item Notację theta $\theta(n)$ //ograniczenie z dwóch stron
        \begin{itemize}
            \item Definicja
                \[
                    \Theta(g(n)) = \{ f(n) \mid \exists c_1, c_2 > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n) \}
                \]
            \item Przykład
                \begin{itemize}
                    \item $n^3 = \Theta(n^3)$
                    \item $n^3 = \Theta(n^3 + 2n^2)$
                    \item $log n +8 + \frac{1}{12n} = \Theta(\log n)$
                \end{itemize}
            \item Uwaga!
                \[
                    f(n) = \Theta(g(n)) \iff f(n) = O(g(n)) \land f(n) = \Omega(g(n))
                \]
                Można to zapisać jako klasy funkcji:
                \[
                    \Theta(g(n)) = O(g(n)) \cap \Omega(g(n))
                \]
        \end{itemize}
    \item Patologiczny przykład:
        mamy funkcje $g(n) = n$ oraz $f(n) = n^{1+sin{\frac{\pi n}{2}}}$, a więc
        \[
            f(n) = \begin{cases}
                n^2 & \text{dla n parzystych} \\
                n & \text{dla n nieparzystych}
            \end{cases}
        \]
        wtedy
        \[
            \limsup_{n \to \infty} \frac{f(n)}{g(n)} = \infty
        \]
        \[
            \limsup_{n \to \infty} \frac{g(n)}{f(n)} = \infty
        \]
        zatem
        $f \neq O(g)$ oraz $g \neq O(f)$
    \item o małe
        \begin{itemize}
            \item Definicja
                \[
                    o(g(n)) = \{ f(n) \mid \forall c > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq f(n) < c \cdot g(n) \}
                \]
                Równoważnie
                \[
                    lim_{n \to \infty} \frac{f(n)}{g(n)} = 0
                \]
            \item Przykład
                \begin{itemize}
                    \item $n^2 = o(n^3)$ i $n^2 O(n^3)$ ale $n^2 \neq o(n^2)$
                    \item $n = o(n^2)$
                \end{itemize}
        \end{itemize}
\end{itemize}

\subsection{Rekurencja}
\begin{itemize}
    \item Metoda podstawienia (metoda dowodu indukcyjnego)
        \begin{enumerate}
        \item Zadnij Odpowiedź (bez stałych)
        \item Sprawdź przez indukcję czy odpowiedź jest poprawna
        \item Wylicz stałe
        \end{enumerate}
        \begin{itemize}
            \item Przykład
                \begin{itemize}
                    \item $T(n) = T(\frac{n}{2}) + n$
                    \item Pierwotny strzał: $T(n) = O(n^3)$
                    \item cel: Pokazać, że $\exists c>0: T(n) \leq c \cdot n^3$
                        \begin{itemize}
                            \item warunek początowy: $T(1) = 1 \leq c$
                            \item krok indukcyjny: załóżmy, że $\forall k \leq n: T(k) \leq ck^3$
                        \end{itemize}
                        \[
                            T(n) = 4T(\frac{n}{2}) + n \leq 4c(\frac{n}{2})^3 + n = \frac{1}{2}cn^3 + n \leq cn^3 \quad \text{dla} \quad c \geq 2
                        \]
                        jednakże ``Przestrzeliliśmy'' znacznie, spróbojmy wzmocnić założenie indukcyjne:
                        \[
                            T(n) \leq c_1k^2 -c_2k, k < n
                        \]
                        wtedy mamy:
                        \[
                            T(n) = 4T(\frac{n}{2}) +n \leq 4(c_1(\frac{n}{2})^2 - c_2(\frac{n}{2})) + n = c_1n^2 - 2c_2n + n \leq c_1n^2 - c_2n
                        \]
                        zatem $c_1 = 1, c_2 = 1$ i $T(n) = O(n^2)$ \qed
                \end{itemize}
            \item Przykład
                \begin{itemize}
                    \item $T(n) = 2T(\sqrt{n}) + \log n$
                        \newline
                        załóżmy, że $n$ jest potęgą liczby $2$, czyli $n = 2^m$
                        \[
                            T(2^m) = 2T(2^{\frac{m}{2}}) + m
                        \]
                        Co implikuje
                        \[
                            T(2^\frac{m}{2}) \rightarrow S(m)
                        \]
                        wtedy
                        \[
                            S(m) = 2S(\frac{m}{2}) + m
                        \]
                        rozwiązując rekurencję otrzymujemy
                        \[
                            S(m) = m \log m
                        \]
                        zatem
                        \[
                            T(n) = \log n \log \log n
                        \]
                \end{itemize}
        \end{itemize}

\end{itemize}
\section{Wykład \date{2025-03-17}}
\subsection{Drzewo rekursji}
Przykład dzewa rekursji:
\begin{itemize}
    \item $T(n) = T(\frac{n}{2})+T(\frac{n}{4}) + n^2$
\end{itemize}
\begin{center}
\begin{tikzpicture}
    [level distance=1.5cm,
    level 1/.style={sibling distance=4cm},
    level 2/.style={sibling distance=2cm}]
    \node {$n^2$}
        child {node {$\frac{n^2}{4}$}
            child {node {$\frac{n^2}{16}$}}
            child {node {$\frac{n^2}{64}$}}
        }
        child {node {$\frac{n^2}{16}$}
            child {node {$\frac{n^2}{64}$}}
            child {node {$\frac{n^2}{256}$}}
        };
      \node[draw=none] at (-4.5,0) {$n^2$};
      \node[draw=none] at (-4.5,-1.5) {$\frac{5}{16}n^2$};
      \node[draw=none] at (-4.5,-3) {$\frac{25}{256}n^2$};
\end{tikzpicture}
\end{center}
\subsubsection*{Uwaga!}
Nie jest to formalne rozwiązanie problemu. Nie można używać drzewa rekursji do dowodzenia złożoności algorytmów. Jest to jedynie intuicyjne podejście do problemu. Trzeba policzyć to na piechote, aby było formalnie.\newline
Aby policzyć $T(n)$ musimy policzyć sumę wszystkich wierzchołków w drzewie rekursji.
\[
    T(n) = \sum^{\infty}_{k=0} \left(\frac{5}{16}\right)^k \cdot n^2 = n^2 \sum^{\infty}_{k=0} \left(\frac{5}{16}\right)^k = n^2 \frac{1}{1-\frac{5}{16}} = n^2 \frac{16}{11} = \frac{16}{11}n^2
\]
A wiec $T(n) = O(n^2)$
\newline
Możemy to policzyć dokładniej dostajac mniejsze wyrazy w sumie.
\[
    T(n) = O(\hat{T}(n)) = O(\check{T}(n))
\]
\[
    T(n) = \Omega(\check{T}(n))
\]
\[
    T(n) = \Theta(n^2) = \frac{16}{11}n^2 + o(n^2)
\]

\subsection{Metoda iteracyjna}
Weźmy na przykład taką rekurencję:
\[
    T(n) = 3T(\frac{n}{4}) + n
\]
Zobaczmy co się dzieje po podstawieniu rekurencji do samej siebie:
\begin{enumerate}
    \item $T(n) = 3T(\frac{n}{4}) + n$
    \item $T(n) = 3(3T(\frac{n}{16}) + \frac{n}{4}) + n = 3^2T(\frac{n}{16}) + \frac{3}{4}n + n$
    \item $T(n) = 3^2(3T(\frac{n}{64}) + \frac{n}{16}) + \frac{3}{4}n + n = 3^3T(\frac{n}{64}) + \frac{3}{16}n + \frac{3}{4}n + n$
    \item \dots \footnote{Warto zauważyć, że jest to analogicznie do liczenia sumy wszystkich nodów drzewa rekursji}
\end{enumerate}
A więc ogólnie wychodzi:
\[
    %T(n) = \sum^{\log_{2}n}_{k=0} \frac{{3}{4}
\]
\subsection{Master Theorem}
Niech $a \geq 1, b > 1, f(n), d \in \mathbb{N}$ oraz $f(n)$ będzie funkcją nieujemną. Rozważmy rekurencję:
\[
    T(n) = aT(\frac{n}{b}) + \Theta(n^d)
\]
gdzie $a$ i $b$ są stałymi, a $f(n)$ jest funkcją nieujemną. Wtedy:
\begin{enumerate}
    \item $\Theta(n^d)$ jeśli $d > \log_b a$
    \item $\Theta(n^d \log n)$ jeśli $d = \log_b a$
    \item $\Theta(n^{\log_b a})$ jeśli $d < \log_b a$
\end{enumerate}
\subsubsection*{Szkic D-d}
Do przedstawienia problemu użyjemy drzewa rekursji. Rozważmy rekurencję:
\[
    T(n) = aT(\frac{n}{b}) + \Theta(n^d)
\]
\begin{center}
    \begin{tikzpicture}
        [level distance=1.5cm,
        level 1/.style={sibling distance=4cm},
        level 2/.style={sibling distance=2cm},
        level 3/.style={sibling distance=1cm}]
        \node {$c \cdot n^d$}
            child {node {$c \cdot \left(\frac{n}{b}\right)^d$}
                child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                }
                child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                }
            }
            child {node {$c \cdot \left(\frac{n}{b}\right)^d$}
                child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                }
                child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                }
            };
        %opis
        \node[draw=none] at (-4.5,1.5) {wielkość};
        \node[draw=none] at (0,1.5) {drzewo};
        \node[draw=none] at (4,1.5) {liczba problemów};
        %wielkosc
        \node[draw=none] at (-4.5,0) {$n^d$};
        \node[draw=none] at (-4.5,-1.5) {$\frac{n^d}{b^d}$};
        \node[draw=none] at (-4.5,-3) {$\frac{n^d}{b^{2d}}$};
        %liczba problemów
        \node[draw=none] at (4.5,0) {$1$};
        \node[draw=none] at (4.5,-1.5) {$a$};
        \node[draw=none] at (4.5,-3) {$a^2$};
        %dodaj linie oddzielajaca miedzy opisem a drzewem
        \draw[thin] (-6,1) -- (6,1);
        %dodaj linie między wielkością a drzewem
        \draw[thin] (-4,0.5) -- (-4,-3.5);
        %dodaj linie między drzewem a liczbą problemów
        \draw[thin] (4,0.5) -- (4,-3.5);
    \end{tikzpicture}
\end{center}
\begin{enumerate}
    \item suma kosztów w $k$--tym kroku
        \[
            a^k c (\frac{n}{b^k})^d = c (\frac{a}{b^d})^k n^d
        \]
        gdzie $c(\frac{n}{b^k})^d$ to koszt jednego podproblemu w $k$--tym kroku
    \item obliczenie wysokości drzewa:
        \[
            \frac{n}{b^h} = 1 \rightarrow h = \log_b n
        \]
    \item Obliczenie $T(n)$
        \begin{equation*}
            T(n) = \Theta(\sum^{\log_b n}_{k=0} c\frac{a}{b^k}n^d) \\
                 &= \Theta(c \cdot n^d \sum^{\log_b n}_{k=0} (\frac{a}{b^d})^k) \\
                 &= \Theta(c \cdot n^d \frac{1-(\frac{a}{b^d})^{\log_b n + 1}}{1-\frac{a}{b^d}}) \implies T(n) = \Theta(n^d)
        \end{equation*}
    \item rozważmy 3 przypadki:
        \begin{enumerate}
            \item $d > \log_b a$ \marginpar{root -- heavy}
                \[
                    T(n) = \Theta(n^d)
                \]
            \item $d = \log_b a$ \marginpar{równo}
                \[
                    T(n) = \Theta(n^d \log n)
                \]
            \item $d < \log_b a$ \marginpar{leaf -- heavy}
                \[
                    T(n) = \Theta(n^{\log_b a})
                \]
        \end{enumerate}
\end{enumerate}

\subsubsection*{Przykłady}
\begin{itemize}
    \item $T(n) = 4T(\frac{n}{2}) + 11n$ \newline
        Wtedy kożystając z \textbf{Master Theorem} mamy:
        \[
            a = 4, b = 2, d = 1
        \]
        Jak i również
        \[
            \log_b a = \log_2 4 = 2 > 1 = d \implies T(n) = \Theta(n^2)7
        \]
    \item $T(n) = 4T(\frac{n}{3}) + 3n^2$ \newline
        Wtedy
        \[
            a = 4, b = 3, d = 2
        \]
        Jak i również
        \[
            \log_b a = \log_3 4 > 2 = d \implies T(n) = \Theta(n^{\log_3 4})
        \]
    \item $T(n) = 27T(\frac{n}{3}) + \frac{n^2}{3}$ \newline
        Wtedy
        \[
            a = 27, b = 3, d = 2
        \]
        Jak i również
        \[
            \log_b a = \log_3 27 = 3 > 2 = d \implies T(n) = \Theta(n^3\log n)
        \]
\end{itemize}

\subsection{Metoda dziel i zwyciężaj (D\&C)}
Na czym ona polega?
\begin{enumerate}
    \item Podział problemu na mniejsze podproblemy \footnote{W zapisie rekurencyjnym $T(n) = cT(cn) + \underline{n^d}$}
    \item Rozwiazanie rekurencyjnie mniejsze podpoblemy
    \item połącz rozwiązania podproblemów w celu rozwiązania problemu wejściowego
\end{enumerate}
\subsubsection{Algorytm -- Binary Search}
\begin{itemize}
    \item \textbf{Input}: posortowania tablica \texttt{A[1..n]} oraz element \texttt{x}
    \item \textbf{Output}: indeks \texttt{i} taki, że \texttt{A[i] = x} lub \texttt{0} jeśli \texttt{x} nie występuje w \texttt{A}
    \item przebieg algorytmu: %pseudokod
        \begin{algorithm}[H]
            \caption{Binary Search}
            \begin{algorithmic}[1]
                \Procedure{BinarySearch}{A, x}
                \State $l = 1$
                \State $r = |A|$
                \While{$l \leq r$}
                \State $m = \lfloor \frac{l+r}{2} \rfloor$
                \If{$A[m] = x$}
                \State \Return{$m$}
                \ElsIf{$A[m] < x$}
                \State $l = m + 1$
                \Else
                \State $r = m - 1$
                \EndIf
                \EndWhile
                \State \Return{0}
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item \textbf{Asypmtotyka}
        Algorytm spełnia następująca rekurencje:
        \[
            T(n) = T(\frac{n}{2}) + \Theta(1)
        \]
        Rozwiązując za pomocą \textbf{Master Theorem} otrzymujemy:
        \[
            T(n) = \Theta(\log n)
        \]
\end{itemize}

\subsubsection{Algorytm -- potęgowanie liczby do naturalnej potęgi}
\begin{itemize}
    \item \textbf{Problem}: obliczanie $x^n$\\
        Można rozbić mnożenie $n$ $x$ na odpowiednie podproblemy:
        %podkresl n/2 xksów
        \[
            x^n = \underbrace{x \cdot x \cdot \dots \cdot x}_{\frac{n}{2}} \cdot \underbrace{x \cdot x \cdot \dots \cdot x}_{\frac{n}{2}}
        \]
        A więc mamy:
        \[
            x^n = \begin{cases}
                x^{\frac{n}{2}} \cdot x^{\frac{n}{2}} & \text{dla n parzystych} \\
                x^{\frac{n-1}{2}} \cdot x^{\frac{n-1}{2}} \cdot x & \text{dla n nieparzystych}
            \end{cases}
        \]
    \item \textbf{Asymptotyka}: \\
        Algorytm spełnia następującą rekurencję:
        \[
            T(n) = T(\frac{n}{2}) + \Theta(1)
        \]
        Rozwiązując za pomocą \textbf{Master Theorem} otrzymujemy:
        \[
            T(n) = \Theta(\log n)
        \]
\end{itemize}

\subsubsection{Obliczenie n-tej liczby Fibonacciego}
\begin{itemize}
    \item \textbf{Problem}:
        \[
            F_n = \begin{cases}
                0 & \text{dla n = 0} \\
                1 & \text{dla n = 1} \\
                F_{n-1} + F_{n-2} & \text{dla n > 1}
            \end{cases}
        \]
    \item \textbf{Algorytmy}:
        \begin{enumerate}
            \item Naiwna rekurencja używająca definicji.
                %drzewo rekursji dla n = 4
                \begin{center}
                    \begin{tikzpicture}
                        [level distance=1.5cm,
                        level 1/.style={sibling distance=4cm},
                        level 2/.style={sibling distance=2cm},
                        level 3/.style={sibling distance=1cm}]
                        \node {$F_4$}
                            child {node {$F_3$}
                                child {node {$F_2$}
                                    child {node {$F_1$}}
                                    child {node {$F_0$}}
                                }
                                child {node {$F_1$}}
                            }
                            child {node {$F_2$}
                                child {node {$F_1$}}
                                child {node {$F_0$}}
                            };
                        %caption
                        \node[draw=none] at (0,1) {Obliczanie $F_4$};
                    \end{tikzpicture}
                \end{center}
                Kontynułując dostajemy asymptotyke rzędu $\Theta(\phi^n)$
            \item \textit{bottom up} -- iteracyjne obliczanie kolejnych liczb Fibonacciego. Asymptotyka wynosi $\Theta(n)$
            \item Kożystanie z wzoru wynikającego z rozwiązanej rekurencji:
                \[
                    F_n = \frac{1}{\sqrt{5}} \left( \left(\frac{1+\sqrt{5}}{2}\right)^n - \left(\frac{1-\sqrt{5}}{2}\right)^n \right)
                \]
                Problem z tym podejsciem polega na niedokładnym przybilżeniu przez komputery wartości $\phi$
            \item Kożystając z lematu:
                \[
                    \begin{pmatrix}
                        1 & 1 \\
                        1 & 0
                    \end{pmatrix}^n
                    =
                    \begin{pmatrix}
                        F_{n+1} & F_n \\
                        F_n & F_{n-1}
                    \end{pmatrix},
                \]
                \begin{proof}
                    \begin{enumerate}
                        \item Warunek początkowy: dla $n = 0$
                            \[
                                \begin{pmatrix}
                                    1 & 1 \\
                                    1 & 0
                                \end{pmatrix}^0
                                =
                                \begin{pmatrix}
                                    1 & 0 \\
                                    0 & 1
                                \end{pmatrix}
                                =
                                \begin{pmatrix}
                                    F_1 & F_0 \\
                                    F_0 & F_{-1}
                                \end{pmatrix}
                            \]
                        \item Krok indukcyjny:\\
                            załóżmy, że dla pewnego $k$ zachodzi:
                            \[
                                \begin{pmatrix}
                                    1 & 1 \\
                                    1 & 0
                                \end{pmatrix}^k
                                =
                                \begin{pmatrix}
                                    F_{k+1} & F_k \\
                                    F_k & F_{k-1}
                                \end{pmatrix}
                            \]
                            wtedy dla $k+1$ mamy:
                            \[
                                \begin{pmatrix}
                                    1 & 1 \\
                                    1 & 0
                                \end{pmatrix}^{k+1}
                                =
                                \begin{pmatrix}
                                    F_{k+2} & F_{k+1} \\
                                    F_{k+1} & F_k
                                \end{pmatrix}
                            \]
                            \[
                                \begin{pmatrix}
                                    1 & 1 \\
                                    1 & 0
                                \end{pmatrix}^{k+1}
                                =
                                \begin{pmatrix}
                                    F_{k+1} & F_k \\
                                    F_k & F_{k-1}
                                \end{pmatrix}
                                \begin{pmatrix}
                                    1 & 1 \\
                                    1 & 0
                                \end{pmatrix}
                                =
                                \begin{pmatrix}
                                    F_{k+1} + F_k & F_{k+1} \\
                                    F_k + F_{k-1} & F_k
                                \end{pmatrix}
                                =
                                \begin{pmatrix}
                                    F_{k+2} & F_{k+1} \\
                                    F_{k+1} & F_k
                                \end{pmatrix}
                            \]
                    \end{enumerate}
                \end{proof}
                Algorytm ten ma złożoność $\Theta(n\log n)$
        \end{enumerate}
\end{itemize}

\subsubsection{Mnożenie liczb}
\begin{itemize}
    \item \textbf{Input}: $x, y$ takie, że $\max\{|x|, |y|\}$
    \item \textbf{Output}: $x \cdot y$
    \item \textbf{Algorytmy}:
        \begin{enumerate}
            \item standardowe mnożenie szkolne -- mnożenia w słupku jego asyptotyka wynosi $\Theta(n^2)$
            \item Podejście metodą \textbf{D\&C}
                \begin{itemize}
                    \item \textbf{Podejście}: Rozbijamy liczby na dwie równe części, a następnie mnożymy je przez siebie \\
                        Możemy zapisać $x$ oraz $y$ jako:
                        \[
                            x = \underbrace{x_L \cdot 2^{\frac{n}{2}}}_{\frac{n}{2} \text{bitów}} +\underbrace{x_R}_{\frac{n}{2} \text{bitów}}
                        \]
                        \[
                            y = \underbrace{y_L \cdot 2^{\frac{n}{2}}}_{\frac{n}{2} \text{bitów}} +\underbrace{y_R}_{\frac{n}{2} \text{bitów}}
                        \]
                        Proces mnożenia wygląda następująco:
                        \begin{equation*}\begin{split}
                            x \cdot y &= (x_L \cdot 2^n + x_R) \cdot (y_L \cdot 2^n + y_R) \\
                                      &= x_L y_L \cdot 2^{2n} + ((x_L + x_R)(y_L + y_R) - x_L y_L - x_R y_R) \cdot 2^n \\
                                      &\quad + x_R y_R
                        \end{split}\end{equation*}
                        Generalnie wszytkie wykonywane powyżej operacje są giga tanie bo opreacje takie jak mnożenie przez $2^k$ wiąże się jedynie z przesunięciem bitowym.
                    \item \textbf{Asymptotyka}: Nasz algorytm spełnia następującą rekurencje na podstawie zapisanego wyżej równania
                        \[
                            T(n) = 4T(\frac{n}{2}) + \Theta(n)
                        \]
                        Kożystając ponownie z \textbf{Master Theorem} można wywnioskować, że algorytm ma złożoność $\Theta(n^2)$. Zatem nie ma żadnego znacznego przyśpieszenia, nawet prawdopodobnie stała ukryta w $\Theta(n^2)$ jest gorsza niż w standardowym podjesciu
                \end{itemize}
            \item Metoda Gaussa
                \begin{itemize}
                    \item Rozważmy mnożenie liczb zespolonych
                        \[
                            (a+ib)(c+id) = ac + i(ad+bc) + bd
                        \]
                        \[
                            bc + ad = (a+b)(c+d) - ac - bd
                        \]
                        zatem
                        \[
                            x \cdot y = x_Ly_L \cdot 2^n + ((x_L + x_R)(y_L + y_R) - x_Ly_L - x_Ry_R) \cdot 2^{\frac{n}{2}} + x_Ry_R
                        \]
                    \item \textbf{Asymptotyka}: algorytm ten spełnia rekurencje
                        \[
                            T(n) = 3T(\frac{n}{2}) + \Theta(n)
                        \]
                        Z \textbf{Master Theorem} otrzymujemy, że algorytm ma złożoność $\Theta(n^{\log_2 3})$, a $\log_2 3 \approx 1.58$
                \end{itemize}
            \item Istneją jeszcze szybsze, nowsze algorytmy mnożenia liczb, takie jak algorytm Schönhage'a-Strassena bazuje ono na szybkiej transformacie Fouriera \textit{Fast Fourier Transform}, który ma złożoność $\Theta(n \log n \log \log n)$. Jednakże, trzeba wziąść pod uwagę stałą ukrytą w $\Theta$. W praktyce, dla liczb o rozmiarze do $10^6$ lepiej jest użyć standardowego algorytmu mnożenia.
        \end{enumerate}
\end{itemize}
Trochę pseudo kodu dla mnożenia liczb:
\begin{algorithm}
    \caption{Mnożenie liczb}
    \begin{algorithmic}[1]
        \Procedure{Multiply}{x, y}
        \State $n = \max\{|x|, |y|\}$
        \If{$n = 1$}
        \State \Return{$x \cdot y$}
        \EndIf
        \State $x_L, x_R = \text{LetfMost}\left\lceil\frac{n}{2}\right\rceil, \text{RightMost}\left\lceil\frac{n}{2}\right\rceil \text{bits of $x$}$
        \State $y_L, y_R = \text{LetfMost}\left\lceil\frac{n}{2}\right\rceil, \text{RightMost}\left\lceil\frac{n}{2}\right\rceil \text{bits of $y$}$
        \State $p_1 = \text{Multiply}(x_L, y_L)$
        \State $p_2 = \text{Multiply}(x_R, y_R)$
        \State $p_3 = \text{Multiply}(x_L + x_R, y_L + y_R)$
        \State \Return{$p_1 \cdot 2^{2n} + (p_3 - p_1 - p_2) \cdot 2^n + p_2$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\subsubsection{Mnożenie macierzy}
\begin{itemize}
    \item \textbf{Input}: dwie macierze $A, B$ rozmiaru $n \times n$
    \item \textbf{Output}: macierz $C = A \cdot B$
    \item \textbf{Algorytmy}:
        \begin{enumerate}
            \item Naiwne mnożenie macierzy -- jego złożoność wynosi $\Theta(n^3)$ bo aby policzyć jedną komórkę macierzy $C$ musimy wykonać $n$ mnożeń (i $n-1$ dodawań \footnote{w sumie $n^2$ operacji}), a skoro macierz $C$ ma $n^2$ komórek to złożoność wynosi $\Theta(n^3)$
            \item Algorytm Strassena -- \textbf{D\&C}
                \begin{itemize}
                    \item \textbf{Podejście}: Rozbijamy macierze na 4 równe części
                        \[
                            A = \begin{pmatrix}
                                A_{11} & A_{12} \\
                                A_{21} & A_{22}
                            \end{pmatrix}
                        \]
                        \[
                            B = \begin{pmatrix}
                                B_{11} & B_{12} \\
                                B_{21} & B_{22}
                            \end{pmatrix}
                        \]
                        \[
                            C = \begin{pmatrix}
                                C_{11} & C_{12} \\
                                C_{21} & C_{22}
                            \end{pmatrix}
                        \]
                        gdzie
                        \[
                            C_{11} = A_{11}B_{11} + A_{12}B_{21}
                        \]
                        \[
                            C_{12} = A_{11}B_{12} + A_{12}B_{22}
                        \]
                        \[
                            C_{21} = A_{21}B_{11} + A_{22}B_{21}
                        \]
                        \[
                            C_{22} = A_{21}B_{12} + A_{22}B_{22}
                        \]
                    \item \textbf{Asymptotyka}: Algorytm ten spełnia rekurencje
                        \[
                            T(n) = 7T(\frac{n}{2}) + \Theta(n^2)
                        \]
                        Z \textbf{Master Theorem} otrzymujemy, że algorytm ma złożoność $\Theta(n^{\log_2 7})$, a $\log_2 7 \approx 2.81$
                        \footnote{Aby zejść do rekurencji $T(n) = 7T(\frac{n}{2}) + \Theta(n^2)$ trzeba wykonać pewne, bardziej wyrafinowane triki, które nie są dokładnie opisane tutaj. Z algorytmu zapisanego wyżej wynika że rekurencja to $T(n) = 8T(\frac{n}{2}) + \Theta(n^2)$, a wieć złożoność wynosi $\Theta(n^3)$}
                \end{itemize}
        \end{enumerate}
\end{itemize}

\subsubsection{Quick Sort}
Algortym \textbf{Merge Sort} ociera się o minimalną granicę złożoności sortowania, która wynosi $\Theta(n \log n)$, jednaże jest z nim problem związany z pamięcia: nie sortuje w miejscu, a więc wymaga dodatkowej pamięci.
\begin{itemize}
    \item \textbf{Input}: tablica $A[1..n]$
    \item \textbf{Output}: posortowana tablica $A$
    \item \textbf{Algorytm}: \texttt{QuickSort(A, p, q)}
        \begin{enumerate}
            \item Podziel tablicę \texttt{A[p...q]} na dwie podtablice \texttt{A[p...k-1]} oraz \texttt{A[k+1...q]}, gdzie \texttt{A[k]} jest elementem rozdzielającym -- \textit{pivotem}\footnote{o tym jak ten \textif{pivot} jest wybierany będziemy mówić później} tak, że:
                \[
                    \forall i \in [p...k-1]: A[i] \leq A[k]: \forall j \in [k+1...q]: A[j] \geq A[k]
                \]
            \item Odpalamy rekurencyjnie \texttt{QuickSort(A, p, k-1)} oraz \texttt{QuickSort(A, k+1, q)}
        \end{enumerate}
    \item \textbf{Przyklad}:
        \begin{enumerate}
            \item mamy dane $A = [6,1,4,3,5,7,2,8]$, wybieramy \textit{pivot} jako $6$
            \item Przebieg partycjonowania:
            \[
                A = [1,4,3,5,2,6,7,8]
            \]
            Elementy mniejsze niż $6$ znalazły się po lewej stronie, większe po prawej. Pozycja pivota: $A[6] = 6$.
            \item Rekurencyjnie sortujemy dwie części:
            \begin{itemize}
                \item \texttt{QuickSort(A, 1, 5)} dla tablicy $[1,4,3,5,2]$, np. wybieramy pivot $1$:
                \[
                    A = [1,4,3,5,2]
                \]
                Po dalszym sortowaniu otrzymamy $[1,2,3,4,5]$
                \item \texttt{QuickSort(A, 7, 8)} dla $[7,8]$, który już jest posortowany.
            \end{itemize}
            \item Finalna posortowana tablica:
            \[
                A = [1,2,3,4,5,6,7,8]
            \]
        \end{enumerate}
        Na koniec przykład w drzewie rekursji:
        \begin{center}
            \begin{tikzpicture}[
                level distance=2.2cm,
                level 1/.style={sibling distance=8cm},
                level 2/.style={sibling distance=5cm},
                level 3/.style={sibling distance=3cm},
                level 4/.style={sibling distance=2cm},
                every node/.style={align=center}
                ]

                \node {QS([\textcolor{orange}{6}, \textcolor{blue}{1,4,3,5,2}, \textcolor{red}{7,8}])}
                    child {
                        node {QS([\textcolor{orange}{1}, \textcolor{red}{4,3,5,2}])}
                        child {
                            node {QS([])}
                        }
                        child {
                            node {QS([\textcolor{orange}{4}, \textcolor{blue}{3,2}, \textcolor{red}{5}])}
                            child {
                                node {QS([\textcolor{orange}{3}, \textcolor{blue}{2}])}
                                child {
                                    node {QS([2])}
                                }
                                child {
                                    node {QS([])}
                                }
                            }
                            child {
                                node {QS([5])}
                            }
                        }
                    }
                    child {
                        node {QS([\textcolor{orange}{7}, \textcolor{red}{8}])}
                        child {
                            node {QS([])}
                        }
                        child {
                            node {QS([8])}
                        }
                    };

            \end{tikzpicture}
        \end{center}

\end{itemize}

\section{Wykład \date{2025-03-24}}
\subsection{Quick Sort}
\subsubsection{Lemuto Partition}
\begin{itemize}
    \item \textbf{Input}: tablica $A[1..n]$
    \item \textbf{Output}: posortowana tablica $A$
    \item \textbf{Algorytm}: \texttt{Lemuto(A, p, q)}
        \begin{algorithm}
            \caption{Lemuto Partition}
            \begin{algorithmic}[1]
                \Procedure{Lemuto}{A, p, q}
                \State $\text{pivot} = A[p]$
                \State $i = p$
                \For{$j = p+1$ to $q$}
                \If{$A[j] < \text{pivot}$}
                \State $i = i + 1$
                \State \text{swap} $A[i] \leftrightarrow A[j]$
                \EndIf
                \EndFor
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item \textbf{Przykład:}
        \begin{enumerate}
            \item zaczynamy z nieposotowaną tablicą $A = [6,10,13,5,8,3,2,11]$
                \item wybieramy pivot $A[1] = 6$
            \item inicjalizujemy $i = 1$
            \item iterujemy przez tablicę od $j = 2$ do $j = 8$:
                \begin{itemize}
                    \item $j = 2$: $A[2] = 10$ (nie mniejsze od pivot)
                    \item $j = 3$: $A[3] = 13$ (nie mniejsze od pivot)
                    \item $j = 4$: $A[4] = 5$ (mniejsze od pivot)
                        \begin{itemize}
                            \item $i = i + 1 = 2$
                            \item zamiana $A[2] \leftrightarrow A[4] \Rightarrow A = [6, 5, 13, 10, 8, 3, 2, 11]$
                        \end{itemize}
                    \item $j = 5$: $A[5] = 8$ (nie mniejsze od pivot)
                    \item $j = 6$: $A[6] = 3$ (mniejsze od pivot)
                        \begin{itemize}
                            \item $i = i + 1 = 3$
                            \item zamiana $A[3] \leftrightarrow A[6] \Rightarrow A = [6, 5, 3, 10, 8, 13, 2, 11]$
                        \end{itemize}
                    \item $j = 7$: $A[7] = 2$ (mniejsze od pivot)
                        \begin{itemize}
                            \item $i = i + 1 = 4$
                            \item zamiana $A[4] \leftrightarrow A[7] \Rightarrow A = [6, 5, 3, 2, 8, 13, 10, 11]$
                        \end{itemize}
                    \item $j = 8$: $A[8] = 11$ (nie mniejsze od pivot)
                \end{itemize}
            \item zamiana pivot $A[1] \leftrightarrow A[4] \Rightarrow A = [2, 5, 3, 6, 8, 13, 10, 11]$
            \item pivot $6$ jest na pozycji $4$
        \end{enumerate}
    \item \textbf{Asymptotyka}:
        Algorytm ten wykonuje w głownej pętli $n-1$ porównań, natomiast wersja Lemuto Partition wymaga dodatkowo $n-1$ zamian elementów.

\end{itemize}

\subsubsection{Hoare Partition}
\begin{itemize}
    \item \textbf{Input}: Tablica $A[1..n]$
    \item \textbf{Output}: Posortowana Tablica $A$
    \item \textbf{Algorytm}: \texttt{Hoare(A, p, q)}
        \begin{algorithm}
            \caption{Hoare Partition}
            \begin{algorithmic}[1]
                \Procedure{Hoare}{A, p, q}
                \State $\text{pivot} = A[\frac{p+q}{2}]$
                \State $i = p-1$
                \State $j = q+1$
                \While{True}
                \State $i = i + 1$
                \While{$A[j] > \text{pivot}$}
                \State $j = j - 1$
                \If{$\geq j$}
                \State \textbf{break}
                \EndIf
                \EndWhile
                \State $\text{swap} A[i] \leftrightarrow A[j]$
                \EndWhile
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item \textbf{Przykład:}
        Generalnie algorytm ten działa na zasadzie zamiany elementów w tablicy względem \textit{pivotu} tak, że jeżeli jest element mniejszy od \textit{pivotu} to zamieniamy go z elementem większym od \textit{pivotu} z drugiej strony tablicy. Algorytm kończy się gdy wszytkie elementy mniejsze od \textit{pivotu} są po lewej stronie, a większe po prawej.
        \begin{enumerate}
            \item zaczynamy z nieposotowaną tablicą $A = [6,10,13,5,8,3,2,11]$
                 \item wybieramy pivot $A[\frac{1+8}{2}] = A[4] = \textcolor{blue}{5}$
            \item inicjalizujemy $i = 0$ i $j = 9$
            \item iterujemy aż $i \geq j$:
                \begin{itemize}
                    \item $i = 1$: $A[1] = 6$ (większe od pivot)
                    \item $j = 8$: $A[8] = 11$ (większe od pivot)
                        \begin{itemize}
                            \item $j = 7$: $A[7] = \textcolor{red}{2}$ (mniejsze od pivot)
                            \item zamiana $A[1] \leftrightarrow A[7] \Rightarrow \\
                            \begin{array}{|c|c|c|c|c|c|c|c|}
                            \hline
                            2 & \textcolor{red}{10} & 13 & \textcolor{blue}{5} & 8 & 3 & \textcolor{red}{6} & 11 \\
                            \hline
                            \end{array}$
                        \end{itemize}
                    \item $i = 2$: $A[2] = 10$ (większe od pivot)
                    \item $j = 6$: $A[6] = 6$ (większe od pivot)
                        \begin{itemize}
                            \item $j = 5$: $A[5] = \textcolor{red}{3}$ (mniejsze od pivot)
                            \item zamiana $A[2] \leftrightarrow A[5] \Rightarrow \\
                            \begin{array}{|c|c|c|c|c|c|c|c|}
                            \hline
                            2 & 3 & \textcolor{red}{13} & \textcolor{blue}{5} & 8 & \textcolor{red}{10} & 6 & 11 \\
                            \hline
                            \end{array}$
                        \end{itemize}
                    \item $i = 3$: $A[3] = 13$ (większe od pivot)
                    \item $j = 4$: $A[4] = 8$ (większe od pivot)
                        \begin{itemize}
                            \item $j = 3$: $A[3] = 13$ (większe od pivot)
                            \item zamiana $A[3] \leftrightarrow A[3] \Rightarrow \\
                            \begin{array}{|c|c|c|c|c|c|c|c|}
                            \hline
                            2 & 3 & \textcolor{blue}{5} & 13 & 8 & 10 & 6 & 11 \\
                            \hline
                            \end{array}$
                        \end{itemize}
                    \item $i = 4$: $A[4] = 8$ (większe od pivot)
                    \item $j = 3$: $A[3] = 5$ (pivot), kończymy algorytm
                \end{itemize}
            \item pivot $5$ jest na pozycji $3$ i wszystkie elementy są podzielone względem niego
        \end{enumerate}
    \item \textbf{Asymptotyka}:
        \textit{Hoare Partition} wykonue $n\pm c$ porównań -- o stałą więcej niżeli \textit{Lemuto Partition}, ale za to wykonuje mniej zamian elementów. W praktyce \textit{Hoare Partition} jest szybszy. Całościowa Asymptotyka wynosi $\Theta(n)$
\end{itemize}

\subsubsection{Analiza Worst Case}
Algorytm sortowania Quick Sort zachowuje się najgorzej w przypadku, gdy dostaje tablicę odwrotnie posortowaną. Wszystkie elementy będą znajdowały się po złej stronie \textit{pivotu}. \\
Zostaje spełniana rekurencja:
\[
    T(n) = T(n-1) + \underbrace{T(0)}_{\text{pusta lewa tablica}} + \Theta(n)
\]
Można zauważyć, że nie zadziała tu \textbf{Master Theorem}, trzeba rozwiązać ja na przykład drzewem rekursji:
\begin{center}
\begin{tikzpicture}
    \node {$cn$}
        child {node {$c(n-1)$}
            child {node {$c(n-2)$}
                child {node {$\vdots$}
                    child {node {$\Theta(1)$}}
                    child {node {$\Theta(1)$}}
                }
                child {node {$\Theta(1)$}}
            }
            child {node {$\Theta(1)$}}
        }
        child {node {$\Theta(1)$}};
    \draw [decorate,decoration={brace,mirror,raise=5pt}]
    (-1,0.5) -- (-4.5,-6.5) node [black,midway,xshift=-1.5cm] {$h = n-1$};
  \end{tikzpicture}
\end{center}
Z drzewa rekursji wynika, że powyższa rekurencja to:
\begin{equation*}\begin{aligned}
    T(n) &= T(n-1 + \Theta(n) \\
         &\leq \sum^{n}_{k=0} \left(c(n-k) + \Theta(1)\right) \\
         &= c \sum^{n}_{k=0} (n-k) + \Theta(n) \\
         &= c \sum^{n}_{k=0} k + \Theta(n) \\
         &= \Theta(n^2)
\end{aligned}\end{equation*}
Ograniczenie dolne analogicznie...
\subsubsection{Best Case Analysis}
Algorytm sortowania Quick Sort zachowuje się najlepiej w przypadku, gdy dostaje tablicę posortowaną. Wszystkie elementy będą znajdowały się po dobrej stronie \textit{pivotu}. \\
Zostaje spełniana rekurencja:
\[
    T(n) = T(\frac{n}{2}) +T(\frac{n}{2}) +\Theta(n)
\]
Można zauważyć, z \textbf{Master Theorem}, że asymptotyka wynosi:
\[
    T(n) = \Theta(n \log n)
\]
Rozważmy przypadek, w którym algorytm wykonuje się nie koniecznie optymalną ilość razy. Powiedzmy, że spełnia on taką rekurencje:
\[
    T(n) =T(\frac{n}{10}) +T(\frac{9n}{10}) +\Theta(n)
\]
Rozważając drzewo rekursji możemy zauważyć, że

\begin{center}\begin{tikzpicture}[
    level distance=2.2cm,
    level 1/.style={sibling distance=8cm},
    level 2/.style={sibling distance=5cm},
    level 3/.style={sibling distance=3cm},
    level 4/.style={sibling distance=2cm},
    every node/.style={align=center}
    ]
    \node {$cn$}
        child {node {$c\frac{n}{10}$}
            child {node {$c\frac{n}{100}$}
                child {node {$\vdots$}}
                child {node {$\vdots$}}
            }
            child {node {$c\frac{9n}{100}$}
                child {node {$\vdots$}}
                child {node {$\vdots$}}
            }
        }
        child {node {$c\frac{9n}{10}$}
            child {node {$c\frac{9n}{100}$}
                child {node {$\vdots$}}
                child {node {$\vdots$}}
            }
            child {node {$c\frac{81n}{100}$}
                child {node {$\vdots$}}
                child {node {$\vdots$}}
            }
        };
\end{tikzpicture}\end{center}
Każdy wiersz tego drzewa sumuje się do $cn$. Wysokość drzewa wynosi $\log_{10/9} n$, zatem złożoność wynosi $\Theta(n \log_{10/9} n)$, co jest tak naprawdę równe $\Theta(n \log n)$.
\subsubsection{Rozważenie przypadku mieszanego}
Rozważmy przypadek, w którym algorytm raz wykonuje się z best casem -- dzieli się tablica na pół, a raz z worst casem -- dzieli się tablica na 1 i $n-1$ elementów. \\
Zostaje spełniana rekurencja:
\[
    L(n) = 2U(\frac{n}{2}) + \Theta(n)
\]
\[
    U(n) = L(n-1) + \Theta(n)
\]
gdzie $L$ symbolizuje best case, natomiast $U$ worst case. Rozwiązując powyższą rekurencje otrzymujemy:
\begin{equation*}
    \begin{aligned}
        L(n)
        &= 2(L(\frac{n}{2} -1) +\Theta(n)) + \Theta(n) \\
        &= 2L(\frac{n}{2} -1) + \Theta(n) \\
        &= \Theta(n \log n)
    \end{aligned}
\end{equation*}

\subsubsection{Average Case Analysis} \label{sec:average_case_analysis}
Algorytm Quick Sort da się ``zabezpieczyć'' przed złym rozkładem danych poprzez losowym wybraniem pivota i następnie swapnięcie go z naszym deterministycznym miejscem. W ten sposób bedzięmy mieli zawsze jednostajnie losowy rozkład danych.\\
Wprowadźmy
\[
    T_n  \text{ -- zmienna losowa liczby porównań w Quick Sorcie sortowanej tablicy } A, |A|=n
\]
Do dziś nie jest znany rozkład zmiennej losowej $T_n$. \\
Niech $X$ będzie zmienną indykatorową:
\begin{equation*}
    X^{(n)}_{k} =
    \begin{cases}
        1 \text{ jeśli partition podzieli tablice n-elementową na }(k,(n-k-1)) \\
        0 \text{ w przeciwnym przypadku}
    \end{cases}
\end{equation*}
Teraz rozważmy zachowanie zmiennej losowej $T_n$:
\begin{equation*}
    T_n \stackrel{\mathclap{\normalfont\mbox{d}}}{=}
    \begin{cases}
        T_0+T_{n-1}+n-1 \text{ jeśli } (0,n-1) \text{ jest partitionem} \\
        T_1+T_{n-2}+n-1 \text{ jeśli } (1,n-2) \text{ jest partitionem} \\
        \vdots \\
        T_k+T_{n-k-1}+n-1 \text{ jeśli } (k,n-k-1) \text{ jest partitionem} \\
        \vdots \\
        T_{n-1}+T_0+n-1 \text{ jeśli } (n-1,0) \text{ jest partitionem}
    \end{cases}
\end{equation*}
Stosując zmienną indykatową $X$ otrzymujemy
\[
    T_n = \sum^{n-1}_{k=0} X^{(n)}_{k} (T_k + T_{n-k-1} + n-1)
\]
Rozważmy niezależność zmiennych $X^{(n}_k}$ i $T_k$. Są one niezależne, ponieważ ilość porównań nie jest zależna od tego jak poźniej bedzie dzielić się tablica. Zatem można zapisać
\[
    \mathbb{E}[X^{(n)}_{k} T_k] = \mathbb{E}[X^{(n)}_{k}] \mathbb{E}[T_k]
\]
Skoro przyjmujemy jednostajny rozkład danych wejścowych to wartość oczekiwana $X^{(n)}_{k}$ wynosi:
\[
    \mathbb{E}[X^{(n)}_{k}] = 1 \cdot P(X^{(n)}_{k} = 1) + 0 \cdot P(X^{(n)}_{k} = 0) = P(X^{(n)}_{k} = 1) = \frac{(n-1)!}{n!} = \frac{1}{n}
\]
Teraz policzmy wartość oczekiwaną $T_n$
\begin{equation*}\begin{aligned}
    \mathbb{E}[T_n]
    &= \mathbb{E}\left[\sum^{n-1}_{k=0} X^{(n)}_{k} (T_k + T_{n-k-1} + n-1)\right] \\
    &= \sum^{n-1}_{k=0} \mathbb{E}[X^{(n)}_{k} (T_k + T_{n-k-1} + n-1)] \\
    &= \sum^{n-1}_{k=0} \mathbb{E}[X^{(n)}_{k}] \mathbb{E}[T_k + T_{n-k-1} + n-1] \\
    &= \sum^{n-1}_{k=0} \frac{1}{n} \mathbb{E}[T_k + T_{n-k-1} + n-1] \\
    &= \frac{1}{n} \sum^{n-1}_{k=0} \mathbb{E}[T_k] + \mathbb{E}[T_{n-k-1}] + \mathbb{E}[n-1] \\
    &= \frac{1}{n} \sum^{n-1}_{k=0} \mathbb{E}[T_k] + \frac{1}{n} \sum^{n-1}_{k=0} \mathbb{E}[T_{n-k-1}] + \frac{1}{n} \sum^{n-1}_{k=0} n-1 \\
\end{aligned}\end{equation*}
Można zauważyć, że $\sum^{n-1}_{k=0} \mathbb{E}[T_k] = \sum^{n-1}_{k=0} \mathbb{E}[T_{n-k-1}]$ ponieważ jest to doawanie tych samych rzeczy w innej kolejności (od przodu i od tyłu). Zatem
\begin{equation*}\begin{aligned}
    \mathbb{E}[T_n]
    &= \frac{2}{n} \sum^{n-1}_{k=0} \mathbb{E}[T_k] + \frac{1}{n} \sum^{n-1}_{k=0} n-1 \\
    &= \frac{2}{n} \sum^{n-1}_{k=0} \mathbb{E}[T_k] + n-1
\end{aligned}\end{equation*}
Przyjmijmy oznaczenie $\mathbb{E}[T_n] = t_n$, wtedy
\[
    t_n = \frac{2}{n} \sum^{n-1}_{k=0} t_k + n-1
\]
Jest to rekurencja %TODO sprawdzic jaka to rekurencja
Można ją rozwiązać w następujący sposób:
\[
    t_n = \frac{2}{n} \sum^{n-1}_{k=0} t_k + n-1 \quad | \cdot n
\]
\[
    nt_n = 2 \sum^{n-1}_{k=0} t_k + n(n-1)
\]
Podstawmy za $n \rightarrow n-1$
\[
    (n-1)t_{n-1} = 2 \sum^{n-2}_{k=0} t_k + (n-1)(n-2)
\]
Odejmując stronami równanie otrzymujemy:
\[
    nt_n - (n-1)t_{n-1} = 2 \sum^{n-1}_{k=0} t_k + n(n-1) - 2 \sum^{n-2}_{k=0} t_k - (n-1)(n-2)
\]
\[
    nt_n - (n-1)t_{n-1} = 2t_{n-1} + 2(n-1)
\]
Przekształcając otrzymujemy:
\[
    nt_n = (n+1)t_{n-1} + 2(n-1) \quad | : n(n+1)
\]
\[
    \frac{t_n}{n+1} = \frac{t_{n-1}}{n} + \frac{2(n-1)}{n(n+1)}
\]
Przyjmijmy kolejne oznaczenie $s_n = \frac{t_n}{n+1}$, wtedy
\[
    s_n = s_{n-1} + 2 \frac{n-1}{n(n+1)}
\]
jest już prostą rekurencją, którą można łatwo rozwiązać iteracyjnie.
\begin{equation*}\begin{aligned}
    s_n
    &= 2 \sum^n_{k=1} \frac{k-1}{k(k+1)} \\
    &= 2 \sum^n_{k=1} \left( \frac{2}{k+1} - \frac{1}{k} \right) \\
    &= 4 \sum^n_{k=1} \frac{1}{k+1} - 2 \sum^n_{k=1} \frac{1}{k} \\
    &= 4 \left( H_n + \frac{1}{n+1} - 1 \right) - 2 H_n \\
    &= 4H_n + 4 \frac{1}{n+1} - 4 - 2H_n \\
    &= 2H_n + \frac{4}{n+1} - 4
\end{aligned}\end{equation*}
gdzie $H_n$ to $n$-ty element ciągu harmonicznego. Podstawiając z powrotem $t_n \leftarrow s_n(n+1)$ otrzymujemy
\[
    t_n = 2(n+1)H_n + 4(n+1) -4 = 2nH_n +2H_n +4n
\]
Kożystając z faktu, że $H_n = \ln n + \gamma + O(\frac{1}{n})$ otrzymujemy
\[
    \mathbb{E}[T_n] = 2n \log n + 2 \gamma n + \Theta(n)
\]

\section{Wykład \date{2025-03-25}}
\subsection{Dual Pivot Quick Sort}
W roku 1975 Sedgewick pokazał, że
\[
    \mathbb{E}\left[\text{\#porównań w dual pivot partition}\right] \approx \frac{16}{9}n \implies
\]
\[
    \implies \mathbb{E}\left[\text{\#porównań w dual pivot Quick Sortcie Sedgwick}\right] \approx \frac{32}{15}n \log n
\]

Jak się to liczy? \\
Niech $T_n$ - \#porównań w dual pivot Quick Sortcie Sedgwick oraz $P_n$ - \#porównań w dual pivot partition. Rekurencja spełnia takie równanie:
\[
    \mathbb{E}\left[T_n\right] = \mathbb{E}\left[P_n\right] + \frac{1}{\binom{n}{2}} \sum_{1 \leq p \leq q \leq n} \left(\mathbb{E}[T_{p-1}] + \mathbb{E}[T_{q-p-1}] + \mathbb[T_{n-q}]\left)
\]
Poźniej rozwiązuje się to analogicznie jak na poprzednim wykładzie rozwiązywana była rekurencja dla $T_n$ \ref{sec:average_case_analysis}

Natomiast w roku 2009 pan Yaroslavsky, Bentley, Blach opracował poprzez testowanie w Javie (nie miał backgroundu matematycznego) lepszy algortytm Quick Sort.
Poźniej w 2012 Sebastian Wild, Nedel pokazali, że
\[
    \mathbb{E}\left[\text{\#porównań w Dual Pivot Quick Sorcie Yaroslavskiego}\right] \approx 1.9n \log n
\]
2015, Aufmüller, Dietzfelbinger zaprojektowali \textit{Strategie Count} oraz pokazali jej optymalność
\[
    \mathbb{E}\left[\text{\#porównań w Count Partition}\right] \approx \frac{3}{2}n \implies
\]
\[
    \implies \mathbb{E}\left[\text{\#porównań w Dual Pivot z Count Partition}\right] \approx 1.8n \log n
\]
Wartość oczekiwana pojawia się w tych asymptotykach ponieważ jest element losowści zwiazany z porównaniami elementu z pivotami. Nie zawsze trzeba bedzię go porównywać z jednym i drugim pivotem.
\subsubsection{Strategia Count Partition}
\begin{itemize}
    \item Zakładamy $p<q$
        \[
            \left[ \,
                a \,\middle|\,
                \underbrace{\cdots}_{S_{i-1}} \, p \,\middle|\,
                \cdots \, q \,\middle|\,
                \underbrace{\cdots}_{L_{i-1}} \, \boxed{i} \,\middle|\,
                b
            \,\right]
        \]
    \item rozpatrzmy $i$-ty element tablicy
    \item jeśli $s_{i-1} \geq l_{i-1}$ to porównujemy $A[i]$ najpierw z $p$ jeżeli jest potrzeba to z $q$
    \item jesli $s_{i-1} \leq l_{i-1}$ to porównujemy $A[i]$ najpierw z $q$ jeżeli jest potrzeba to z $p$\footnote{jest to swoiste przewidywanie przeszłości na podstawie ilości elementów, które są już posortowane}
\end{itemize}

\subsection{Comparison Model}
\subsubsection{Drzewo decyzyjne}
Jak wygląda to na przykładzie? Mamy daną tablice $[a_1, a_2, a_3]$ i chcemy ją posortować. W drzewie decyzyjnym każdy węzeł to porównanie dwóch elementów. W liściu znajduje się permutacja elementów.
\begin{center}
    \begin{tikzpicture}[
        level 1/.style={sibling distance=70mm, level distance=40mm},
        level 2/.style={sibling distance=40mm, level distance=40mm},
        level 3/.style={sibling distance=20mm, level distance=40mm},
        root/.style={fill=orange!30},
        internal/.style={fill=blue!20},
        leaf/.style={fill=green!20, draw, ellipse}
        ]
        \node[root] {$a_1 < a_3$}
            child {node[internal] {$a_1 < a_2$}
                child {node[internal] {$a_2 < a_3$}
                    child {node[leaf] {$[a_1,a_2,a_3]$}
                    edge from parent node[engine, sloped] {$T$}}
                    child {node[leaf] {$[a_1,a_3,a_2]$}
                    edge from parent node[engine, sloped] {$F$}}
                    edge from parent node[engine, sloped] {$T$}
                }
                child {node[leaf] {$[a_3,a_1,a_2]$}
                edge from parent node[engine, sloped] {$F$}}
                edge from parent node[engine, sloped] {$T$}
            }
            child {node[internal] {$a_2 < a_3$}
                child {node[internal] {$a_1 < a_3$}
                    child {node[leaf] {$[a_2,a_1,a_3]$}
                    edge from parent node[engine, sloped] {$T$}}
                    child {node[leaf] {$[a_2,a_3,a_1]$}
                    edge from parent node[engine, sloped] {$F$}}
                    edge from parent node[engine, sloped] {$T$}
                }
                child {node[leaf] {$[a_3,a_2,a_1]$}
                edge from parent node[engine, sloped] {$F$}}
                edge from parent node[engine, sloped] {$F$}
            };
    \end{tikzpicture}
\end{center}
\begin{itemize}
    \item dla dowolnego algorytmu sortowania w \textit{Comparison Model} można znaleść drogę na drzewie decyzyjnym
    \item W tym drzewie występuje $n!$ liści będacych permutacjami tablicy
    \item \textit{Worst Case} odpowiada najdłuższej ścieżce w drzewie decyzyjnym
    \item drzewo binarne pełne odpowiada najlepszemu algorytmowi sortowania
    \item drzewo binarne pełne o wysokości $h$ ma conajwyżej $2^h$ liści, ale liści w drzewie decyzyjnym powinno być przynajmniej $n!$ zatem
        \[
            2^h \geq n! \implies h \geq \log_2 n!
        \]
\end{itemize}
\subsubsection{Twierdzenie}
Dolne ograniczenie na liczbę porównań w problemie sortowania w \textit{Comparison Model} wynosi $\Omega(n \log n)$
\begin{proof}
    Rozważmy drzewo decyzyjne dla sortowania $n$ elementów. Każda ścieżka w drzewie decyzyjnym odpowiada jednej permutacji. Zatem drzewo decyzyjne ma przynajmniej $n!$ liści. Trzeba pokazać, że wysokość drzewa decyzyjnego jest przynajmniej $\log_2 n!$.
    \[
        2^h \geq n! \implies h \geq \log n!
    \]
    Używając wzoru Stirlinga
    \begin{equation*}\begin{aligned}
        h \geq \log n! &= \log \left( \sqrt{2\pi n} \left( \frac{n}{e} \right)^n \left(1+ O(1)\right) \right)\\
                       &= \log \left(\frac{n}{e}\right)^n + \log \left(\sqrt{2\pi n}(1+O(1))\right) \\
                       &= \underbrace{n \log n}_{\Theta(n \log n)} - \underbrace{n \log e}_{\Theta(n)} + \underbrace{\log \sqrt{2\pi n}}_{\Theta(\log n)} + \underbrace{\log (1+O(1))}_{\Theta(1)} \\
                       &= \Omega(n \log n)
    \end{aligned}\end{equation*}
    Zatem dolne ograniczenie na liczbę porównań w problemie sortowania w \textit{Comparison Model} wynosi $\Omega(n \log n)$ \\
\end{proof}

\subsubsection{Counting Sort}
\begin{itemize}
    \item \textbf{Input}: Tablica $A, |A|=n, \forall i A[i] \in \{0,1,2,...,k\}$, $k$ - stała
    \item \textbf{Output}: Posortowana tablica $A$
    \item \textbf{Algorytm}:
        \begin{algorithm}
            \caption{Counting Sort}
            \begin{algorithmic}[1]
                \Procedure{CountingSort}{A, n, k}
                \For{$i = 1$ to $k$} \Comment{$\Theta(k)$}
                    \State $C[i] = 0$
                \EndFor
                \For{$j = 1$ to $n$} \Comment{$\Theta(n)$}
                    \State $C[A[j]] = C[A[j]] + 1$
                \EndFor
                \For{$i = 2$ to $k$} \Comment{$\Theta(k)$}
                    \State $C[i] = C[i] + C[i-1]$
                \State $C[i] = C[i] + C[i-1]$ \Comment{liczba elementów mniejszych lub równych $i$}
                \EndFor
                \For{$j = n$ downto $1$} \Comment{$\Theta(n)$}
                    \State $B[C[A[j]]] = A[j]$
                    \State $C[A[j]] = C[A[j]] - 1$
                \EndFor
                \State \textbf{return} $B$ \Comment{$\Theta(1)$}
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item \textbf{Przykład}:
        \begin{enumerate}
            \item $A = [4,1,3,4,3]$, $C=[0,0,0,0]$, $B=[0,0,0,0,0]$ (tablice indeksowane od $1$)
            \item $A = [4,1,3,4,3]$, $C=[1,0,2,2]$, $B=[0,0,0,0,0]$
            \item $A = [4,1,3,4,3]$, $C=[1,1,3,5]$, $B=[0,0,0,0,0]$
            \item Ostatnia pętla, skupmy się na tablicy $B$:
                \begin{enumerate}
                    \item $j = 5$: $B[5] = A[5] = 3$, $C[3] = 4$
                    \item $j = 4$: $B[4] = A[4] = 4$, $C[4] = 3$
                    \item $j = 3$: $B[3] = A[3] = 3$, $C[3] = 3$
                    \item $j = 2$: $B[3] = A[2] = 1$, $C[1] = 1$
                    \item $j = 1$: $B[1] = A[1] = 4$, $C[4] = 2$
                \end{enumerate}
                $B = [1,3,3,4,4]$ - posortowana tablica
        \end{enumerate}
    \item \textbf{Asymptotyka}: $\Theta(n+k)$, gdzie $k = O(n)$
\end{itemize}

\subsection{Stable Sorting Property}
Algorytm zachowuje kolejność równych sobie elementów z tablicy wejściowej

\subsubsection{Radix Sort}
\begin{itemize}
    \item \textbf{Input}: Tablica $A, |A|=n, \forall i A[i] \in \{0,1,2,...,k\}$, $k$ - stała
    \item \textbf{Output}: Posortowana tablica $A$
    \item \textbf{Algorytm}: Zastosuj \textif{Counting Sort} dla każdej cyfry liczby
    \item \textbf{Przykład}:
        \[
            %Zapisz A jako macierz 7x1
            A = \begin{bmatrix}
                32\textcolor{red}{9} \\
                45\textcolor{red}{7} \\
                65\textcolor{red}{7} \\
                83\textcolor{red}{9} \\
                43\textcolor{red}{6} \\
                72\textcolor{red}{0} \\
                35\textcolor{red}{5}
                \end{bmatrix} \xrightarrow{\text{Counting Sort}} \begin{bmatrix}
                7\textcolor{red}{2}0 \\
                3\textcolor{red}{5}5 \\
                4\textcolor{red}{3}6 \\
                4\textcolor{red}{5}7 \\
                6\textcolor{red}{5}7 \\
                3\textcolor{red}{2}9 \\
                8\textcolor{red}{3}9
                \end{bmatrix} \xrightarrow{\text{Counting Sort}} \begin{bmatrix}
                \textcolor{red}{3}29 \\
                \textcolor{red}{3}55 \\
                \textcolor{red}{4}36 \\
                \textcolor{red}{4}57 \\
                \textcolor{red}{6}57 \\
                \textcolor{red}{7}20 \\
                \textcolor{red}{8}39
            \end{bmatrix}
        \]
        Algorytm zachowuje kolejność równych sobie elementów z tablicy wejściowej -- \textit{Stable Sorting Property}, co umożliwia sortowanie po kolejnych cyfrach.
    \item \textbf{Poprawność}: Indukcja po $t$ -- numer cyfry
        \begin{enumerate}
            \item jeśli $t=1$ to poprawność \textit{Counting Sort} jest trywialna
            \item załóżmy, że \textif{Counting Sort} jest poprawny dla $t-1$ -- cyfrowej liczby
            \item Krok indukcyjny:
                \begin{enumerate}
                    \item $t$-ta cyfra ... liczby jest ...:
                        to z założenia indukcyjnego oraz \textit{stable counting property Counting Sorta} liczby do $t$--tej cyfty dajej poprawnie posortowane
                    \item $t$-ta cyfra różna: z poprawności \textit{Counting Sort} ok.
                \end{enumerate}
        \end{enumerate}
    \item \textbf{Złożoność obliczeniowa}:
        \begin{itemize}
            \item $n$ $b$-bitowych liczb
            \item liczba $b$-bitowa dzielę na $r$-bitowych cyfr ($\frac{b}{r}$ takich liczb):
            %daj reprezentacje w postaci tablicy
                \[
                    \underbrace{\underline{\mid \quad \quad \mid} \underline{\quad \quad \mid} \underline{ \quad \dots \quad \mid} \overbrace{\underline{\quad \quad \mid}}^{r\text{-bitów}}}_{b\text{-bitów}}
                \]
            \item cyfry są z $\mid \left\{ 0,\dots,2^r -1\right\} \mid = 2^r$
            \item \textit{Counting Sort} sortujemy $\underline{n}$ liczb względem jednej liczby. A więc wykonujemy
                \[
                    \Theta(n+2^r)
                \]
                operacji.
        \end{itemize}
        Zatem \textit{Radix Sort} będzie miał złożoność obliczeniową
        \[
            \Theta\left(\frac{b}{r}(n+2^r)\right)
        \]
        machając trochę rękoma wybieramy $r$ jako $r=\log n$, wtedy
        \[
            \Theta\left(\frac{b}{\log n} \left( n +2^{\log n}\right)\right) = \Theta\left(\frac{bn}{\log n}\right)
        \]
        Jeśli $\{0, \dots, n^d-1\}$ - zakres sortowanych liczb, wtedy $b=\log n^d = d \log n$, a więc
        \[
            \Theta\left(\frac{d\log n \space n}{ \log n} \right) = \Theta\left(\frac{\cancel{\log n} n}{\cancel{\log n}}\right) = \Theta(d \cdot n)
        \]
\end{itemize}
%TO KONIEC PROBLEMU SORTOWANIA!!!!!!!!!!!!!!111

\section{Wykład \date{2025-03-31}}
Problem którym się teraz zajmiemy to tak zwany \textbf{Statystyka Pozycyjna}
\subsection{Definicja Statystyki Pozycyjnej}
\subsubsection{Definicja}
$k$-tą statystyką pozycyjną nazywamy $k$-tą najmniejszą wartością z zadnaego zbioru.
\subsubsection{Przykład}
\begin{itemize}
    \item $k=1 \rightarrow O(n)$
    \item $k=n \rightarrow O(n)$
    \item $k=\left\lfloor\frac{n-1}{2}\right\rfloor \or k=\left\lfloor\frac{n+1}{2}\right\rfloor \rightarrow \text{ sortowanie } O(n \log n)$
\end{itemize}
\subsection{Algorytm Random Select}
\begin{itemize}
    \item \textbf{Input}: Tablica $A, |A|=n$, liczba $k$
    \item \textbf{Output}: $k$-ta statystyka pozycyjna
    \item \textbf{Algorytm}:
        \begin{algorithm}
            \caption{Random Select}
            \begin{algorithmic}[1]
                \Procedure{RandomSelect}{A, p, q, i}
                \If{$p=q$}
                    \State \textbf{return} $A[p]$
                \EndIf
                \State $r = \text{RandomPartition}(A,p,q)$ \Comment{losowa partycja}
                \State $k = r-p+1$
                \If{$i=k$}
                \State \textbf{return} $A[r]$ \Comment{$A[r]$ jest $i$-tą statystyką pozycyjną}
                \ElsIf{$i<k$}
                \State \textbf{return} $\text{RandomSelect}(A,p,r-1,i)$
                \Else
                \State \textbf{return} $\text{RandomSelect}(A,r+1,q,i-k)$
                \EndIf
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item \textbf{Przykład}: $A = [6,10,13,5,8,3,2,11]$, RandomSelect($A,1,8,4$)
        Dla tablicy
        \[
            A = \begin{array}{|c|c|c|c|c|c|c|c|}
                \hline
                6 & 10 & 13 & 5 & 8 & 3 & 2 & 11 \\ \hline
            \end{array}
        \]
        wywołujemy procedurę \texttt{RandomSelect(A, 1, 8, 4)} aby znaleźć 4-ty najmniejszy element.

        \bigskip

        \textbf{Krok 1.}\\
        Zakładamy, że procedura \texttt{RandomPartition} wybiera pivot $8$.\\
        Po partycjonowaniu tablica wygląda następująco:
        \[
            A = \begin{array}{cccccccc}
                6 \quad & 5 \quad & 3 \quad & 2 \quad & \underline{8} \quad & 13 \quad & 10 \quad & 11
            \end{array}
        \]
        Pivot znajduje się na pozycji $r=5$, a $k = r - p + 1 = 5 - 1 + 1 = 5$. Ponieważ poszukiwany rząd $i=4$ jest mniejszy od $k=5$, następuje wywołanie rekurencyjne:
        \[
            \texttt{RandomSelect}(A,1,4,4)
        \]
        dla lewego podzbioru
        \[
            \begin{array}{|c|c|c|c|}
                \hline
                6 & 5 & 3 & 2 \\ \hline
            \end{array}.
        \]

        \bigskip

        \textbf{Krok 2.} \\
        Dla podtablicy
        \[
            B = \begin{array}{|c|c|c|c|}
                \hline
                6 & 5 & 3 & 2 \\ \hline
            \end{array},
        \]
        zakładamy, że pivot został wybrany jako $2$.\\
        Po partycjonowaniu otrzymujemy:
        \[
            B = \begin{array}{|c|c|c|c|}
                \hline
                \underline{2} \quad & 5 \quad & 3 \quad & 6 \\ \hline
            \end{array}
        \]
        gdzie pivot $2$ znajduje się teraz na pozycji $r=1$. Obliczamy $k = r - p + 1 = 1 - 1 + 1 = 1$.\\
        Ponieważ $i=4 > 1$, odejmujemy $k$ od $i$, czyli nowa wartość to $i = 4 - 1 = 3$, i wywołujemy:
        \[
            \texttt{RandomSelect}(B,2,4,3)
        \]
        dla podtablicy
        \[
            B' = \begin{array}{|c|c|c|}
                \hline
                5 & 3 & 6 \\ \hline
            \end{array}.
        \]

        \bigskip

        \textbf{Krok 3.} \\
        Dla podtablicy
        \[
            C = \begin{array}{|c|c|c|}
                \hline
                5 & 3 & 6 \\ \hline
            \end{array},
        \]
        zakładamy, że pivotem jest $6$.\\
        Po partycjonowaniu otrzymujemy:
        \[
            C = \begin{array}{|c|c|c|}
                \hline
                5 & 3 & \underline{6} \\ \hline
            \end{array}
        \]
        gdzie pivot $6$ znajduje się na pozycji $r=3$ (przyjmując indeksowanie od 1). Wówczas $k = r - p + 1 = 3 - 1 + 1 = 3$.\\
        Skoro $i=3$ jest równe $k$, zwracamy pivot:
        \[
            \texttt{RandomSelect}(C,1,3,3)=6.
        \]

        \bigskip

        \textbf{Wniosek:}\\
        Procedura zwraca wartość $\boxed{6}$, czyli 4-ty najmniejszy element w tablicy $A$.
    \item \textbf{Złożoność obliczeniowa}:
        \begin{itemize}
            \item \textit{Best Case}:
                W najlepszym przypadku dzielimy tablicę na dwie równe części, zatem
                \[
                    T(n) = T\left(\frac{n}{2}\right) + \Theta(n)
                \]
                Używając \textit{Master Theorem} otrzymujemy
                \[
                    T(n) = \Theta(n)
                \]
            \item \textit{Worst Case}:
                W najgorszym przypadku dzielimy tablicę na $n-1$ i $1$ elementów (-- pivot wybrany przez partition), a więc
                \[
                    T(n) = T(n-1) + \Theta(n)
                \]
                Używając \textit{Master Theorem} otrzymujemy
                \[
                    T(n) = \Theta(n^2)
                \]
            \item \textit{Average Case}:
                W średnim przypadku musimy policzyć wartość oczekiwaną takiej zmiennej losowej:
                \[
                    T_n = \begin{cases}
                        T_{n-1} + n-1: (0, n-1) \\
                        T_{n-2} + n-1: (1, n-2) \\
                        \vdots \\
                        T_1 + n-1: (n-2, 1) \\
                    \end{cases}
                \]
                Aby formalizować analizę, wprowadzamy zmienną indykatorową:
                \[
                    X^{(n)}_k =
                    \begin{cases}
                        1, & \text{jeśli przy partycjonowaniu tablicy $n$-elementowej nastąpiło rozbicie na } \\
                           & \quad (k, n-k-1) \text{ oraz dalsze wywołanie rekurencyjne odbywa się na} \\
                           & \quad \text{podtablicy zawierającej szukany element,} \\
                        0, & \text{w przeciwnym przypadku.}
                    \end{cases}
                \]
                Wówczas możemy zapisać:
                \[
                    T_n = \sum_{k=0}^{n-1} X^{(n)}_k \left( T\Bigl(\max(k,\, n-k-1)\Bigr) + n-1 \right).
                \]

                Przyjmując, że wybór pivotu jest jednostajny, mamy:
                \[
                    \mathbb{E}\Bigl[X^{(n)}_k\Bigr] = \frac{1}{n} \quad \text{dla } k=0,1,\dots,n-1.
                \]
                Zatem, stosując wartość oczekiwaną i liniowość tej wartości, otrzymujemy:
                \[
                    \mathbb{E}[T_n] = \frac{1}{n} \sum_{k=0}^{n-1} \left( \mathbb{E}\Bigl[T\bigl(\max(k,\, n-k-1)\bigr)\Bigr] + n-1 \right).
                \]

                Ponieważ gdy pivot jest szukanym elementem (dla $k=0$ lub $k=n-1$, w sensie odpowiedniego ustawienia) nie następuje rekurencja, przyjmujemy $T_0 = 0$. Dla pozostałych przypadków (czyli dla $1 \le k \le n-2$) dalsze wywołanie odbywa się na jednej z podtablic o rozmiarze nie większym niż $\lceil n/2 \rceil$ (ze względu na symetrię rozbicia). Możemy więc oszacować:
                \[
                    t_n = \mathbb{E}[T_n] \le n-1 + \frac{n-2}{n}\, t_{\lceil n/2 \rceil}.
                \]

                Łatwo (przez indukcję) pokazać, że taka rekurencja implikuje, iż
                \[
                    t_n = O(n).
                \]

                \bigskip

                \textbf{Wniosek:} Średnia liczba porównań w algorytmie RandomSelect wynosi $\Theta(n)$, czyli algorytm działa w czasie liniowym w średnim przypadku.

        \end{itemize}
\end{itemize}

\subsection{Algorytm Select}
Algorytm zwany jest także algorytmem \textit{Magicznych Piątek}
\begin{itemize}
    \item \textbf{Input}: Tablica $A, |A|=n$, liczba $k$
    \item \textbf{Output}: $k$-ta statystyka pozycyjna
    \item \textbf{Algorytm}:
        \begin{enumerate}
            \item dzielimy $A[p \dots q]$ na $\left\lfloor\frac{n}{5}\right\rfloor$ pięcio elementowe częsci oraz ostatnią część z resztą \marginnote{$\Theta(n)$}
        \item Sortujemy każdą pięcio elementową część i wybieramy z nich mediany: \marginnote{$T(\frac{\left\lceil n \right\rceil}{5}\right)$}
                \[
                    M = \left\{ m_1, m_2, \dots, m_{\left\lfloor\frac{n}{5}\right\rfloor} \right\}
                \]
            \item Znaleść medianę z tablicy $M$: wywołujemy Select($M, 1, \left\lceil\frac{n}{5}\right\rceil, \left\lfloor\frac{\left\lceil\frac{n}{10}\right\rceil}{2}\right\rfloor$) i oznaczmy ją jako $X$\footnote{nazywana też mediana median}
            \item Ustaw $X$ jako pivot w Partition \marginnote{$\Theta(n)$}
                %TODO przepisac od wojtasa
            \item idz do lewej albo prawej podtablicy ... od ... pivota i szukamy ... \marginnote{$T(?)$}
        \end{enumerate}
    \item \textbf{Asymptotyka}:
        Algorytm spełnia rekurencję:
        \[
            T(n) = T\left(\frac{n}{5}\right) + T\left(?\right) + \Theta(n)
        \]
        Gdzie $T(?)$ oznacza czas rekurynceyjnego wywołania algorytmu Select dla reszty tablicy (pozotałych elementów niżeli napewno mniejsze/większe od $X$). Rozmiar \textit{?} jest zależny od dystrybucji elementów w tablicy. W najgorszym przypadku ograniczenie dolne na \textit{pewną część} jest rzędu:
        \[
            \geq \left( \frac{1}{2} \frac{\left\lceil n \right\rceil}{5} \right) -1 -1\right) \cdot 3 \geq \frac{3n}{10} - 6
        \]
        Zatem górne ograniczenie na \textit{?} jest rzędu:
        \[
            n - \left(\frac{3n}{10} - 6\right)-1 \leq \frac{7n}{10} + 5
        \]
        A więc Algorytm \textit{Select} spełnia rekurencję:
        \[
            T(n) = T\left(\frac{n}{5}\right) + T\left(\frac{7n}{10} + 5\right) + \Theta(n)
        \]
        Dla uproszczenia możemy przyjąć $T\left(\frac{7n}{10} + 5\right) = T\left(\frac{3n}{4}\right)$, ponieważ $\frac{3}{4}n \geq \frac{7}{10}n + 5$ dla $n \geq 20$. Przyjmujemy, że dla n<20 algorytm działa w czasie stałym. Reukrencja algorytmu \textit{Select} jest teraz następująca:
        \[
            T(n) = T\left(\frac{n}{5}\right) + T\left(\frac{3n}{4}\right) + \Theta(n)
        \]
        Rozwiążemy ją stosując indukcję:
        \begin{itemize}
            \item Base case: $T(n) = \Theta(1)$ dla $n < 20$
            \item Założenie indukcyjne:
                \[
                    \forall k \leq n: T(k) \leq c k
                \]
            \item Krok indukcyjny:
                %TODO przejrzec czy ten dowod jest poprawny, od szymiego
                \begin{equation*}\begin{aligned}
                    T(n) &\leq T\left(\frac{n}{5}\right) + T\left(\frac{3n}{4}\right) + cn \\
                         &\leq c \cdot \frac{n}{5} + c \cdot \frac{3n}{4} + cn \\
                         &= \frac{c}{5}n + \frac{3c}{4}n + cn \\
                         &= \left(1+\frac{3}{4}+\frac{1}{5}\right)cn \\
                         &= \left(\frac{20+15+12}{20}\right)cn = \frac{47}{20}cn
                \end{aligned}\end{equation*}
        \end{itemize}
\end{itemize}

\section{Wykład \date{2025-04-07}}
\subsection{Set Interface}
Zakładamy, że każda struktura ma pole nazwane kluczem $a \in A \rightarrow \exists 1 \geq a.key \geq \text{length()}$ , które jest unikalne dla każdego elementu. Klucz jest liczbą całkowitą, a jego wartość jest niezmienna. Wartości kluczy są różne dla różnych elementów, a więc $a.key \neq b.key$ dla $a \neq b$.\\
Do budowy zbiorów używamy \textit{Set Interface}, który definiuje następujące operacje:
\begin{itemize}
    \item build($A$) - buduje ``set'' z danych zawartych w tablicy $A$
    \item find($k$) - zwraca element o kluczu $k$ z ``setu''
    \item length() - zwraca liczbę elementów w ``secie''
    \item insert($a$) - dodaje element $a$ do ``setu''
    \item delete($a$) - usuwa element $a$ z ``setu''
    \item find\_min() - zwraca element o najmniejszym kluczu
    \item find\_max() - zwraca element o największym kluczu
    \item find\_next($k$) - zwraca element o kluczu większym od $k$
    \item find\_prev($k$) - zwraca element o kluczu mniejszym od $k$
    \item order() - zwraca elementy w ``secie'' w kolejności rosnącej kluczy
\end{itemize}
Zobaczmy sobie kilka przykładow asymptotyki operacji dla różnych struktur danych:
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{Struktura} & \textbf{build} & \textbf{find} & \textbf{input, delete} & \textbf{find\_max find\_min} & \textbf{find\_next find\_prev} & \textbf{order()} \\ \hline
        unsorted array & $\Theta(n)$ & $\Theta(n)$ & $\Theta(n)\footnote{nie $\Theta(1)$ ponieważ potrzeba czasu na realokacje pamięci itp.}$ & $\Theta(n)$ & $\Theta(n)$ & $\Theta(n \log n)$ \\ \hline
        sorted array & $\Theta(n \log n)$ & $\Theta(\log n)$ & $\Theta(n)$ & $\Theta(1)$ & $\Theta(\log n)$ & $\Theta(n)$ \\ \hline
        unsorted linked list & $\Theta(n)$ & $\Theta(n)$ & $\Theta(1), \Theta(n)\footnote{należy znaleść element do usunięcia, samo usuwanie $\Theta(1)$}$ & $\Theta(n)$ & $\Theta(n)$ & $\Theta(n \log n)$ \\ \hline
        BST

    \end{tabular}
\end{center}

\subsubsection{Binary Search Tree -- BST}
Po polsku drzewo przeszukiwań binarnych. Struktura ta zawiera następujące pola:
\begin{itemize}
    \item parent -- wskaźnik na rodzica
    \item left -- wskaźnik na lewe dziecko
    \item right -- wskaźnik na prawe dziecko
    \item key -- klucz
    \item ... -- inne pola
\end{itemize}
Drzewa BST mają następujące właściwości, niech $T$ będzie drzewem BST, $x \in T$ -- $x$ jest węzłem drzewa T
\begin{itemize}
    \item każdy $y \in x.left$ ma klucz mniejszy od klucza $x$
    \item każdy $y \in x.right$ ma klucz większy od klucza $x$
\end{itemize}

Przykład drzewa BST:
\begin{center}
    \begin{tikzpicture}[
        level distance=1.5cm,
        every node/.style={circle, draw},
        level 1/.style={sibling distance=2cm},
        level 2/.style={sibling distance=1cm},
        level 3/.style={sibling distance=0.5cm}
        ]
        \node {10}
            child {node {5}
                child {node {3}}
            child {node {7}}}
            child {node {15}
                child {node {12}}
            child {node {20}}};
    \end{tikzpicture}
\end{center}
\subsubsection{Operacje na BST}
\begin{itemize}
    \item \textbf{InorderTreeWalk}: operacja do wypisania wszystkich elementów w drzewie w kolejności rosnącej kluczy. Algorytm działa rekurencyjnie, odwiedzając najpierw lewe poddrzewo, następnie węzeł, a na końcu prawe poddrzewo. \\
        \begin{algorithm}
            \caption{InorderTreeWalk}
            \begin{algorithmic}[1]
                \Procedure{InorderTreeWalk}{x}
                \If{$x \neq NIL$}
                    \State InorderTreeWalk($x.left$)
                    \State print($x.key$)
                    \State InorderTreeWalk($x.right$)
                \EndIf
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
        Na powyższym przykładzie wywołanie \texttt{InorderTreeWalk($T$)} wypisze 3, 5, 7, 10, 12, 15, 20. \\
        \textbf{Asymptotyka}: w zależności od rozmiaru lewego poddrzewa -- $k$, algorytm spełnia rekurencję:
        \[
            T(n) = T(k) + \Theta(1) + T(n-1-k)
        \]
        Rozwiążemy to rekurencyjnie:
        \begin{itemize}
            \item założenie indukcyjne: $\forall k < n: T(k) \leq ck$
            \item krok indykcyjny:
                \begin{equation*}\begin{aligned}
                    T(n) &= T(j) + \Theta(1) + T(n-1-j)\\
                         &\leq cj + \Theta(1) +c(n-1-j) \\
                         &= cn -c +\Theta(1) \\
                         &\leq cn
                \end{aligned}\end{equation*}
                Zatem asymptotyka algorytmu to $T(n) = \Theta(n)$
        \end{itemize}

    \item \textbf{TreeSearch}: operacja do wyszukiwania elementu w drzewie. Algorytm działa rekurencyjnie, porównując klucz szukanego elementu z kluczem węzła. Jeśli klucz jest mniejszy, algorytm przeszukuje lewe poddrzewo, jeśli większy -- prawe poddrzewo. \\
        \begin{algorithm}
            \caption{TreeSearch}
            \begin{algorithmic}[1]
                \Procedure{TreeSearch}{x, k}
                \If{$x = null$ or $k = x.key$}
                    \State \textbf{return} $x$
                \ElsIf{$k < x.key$}
                    \State \textbf{return} TreeSearch($x.left$, $k$)
                \Else
                    \State \textbf{return} TreeSearch($x.right$, $k$)
                \EndIf
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
        Na powyższym przykładzie wywołanie \texttt{TreeSearch($T$, 12)} zwróci węzeł o kluczu 12.\\
        \textbf{Asymptotyka}: złożoność algorytmu jest zależna jedynie od wysokości drzewa, a więc:
        \[
            T(n) = O(h)
        \]
    \item \textbf{TreeMin, TreeMax} operacje proste idące jedynie w lewo lub w prawo, odpowiednio. Na przykład
        \begin{algorithm}
            \caption{TreeMin}
            \begin{algorithmic}[1]
                \Procedure{TreeMin}{x}
                \While{$x.left \neq null$}
                    \State $x = x.left$
                \EndWhile
                \State \textbf{return} $x$
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
        Algorytm działa iteracyjnie, przeszukując lewe poddrzewo.\\
        Złożoność algorytmu to $\Theta(h)$, analogicznie jak w przypadku \texttt{TreeMax}.
    \item \textbf{TreeSuccessor} zwraca następnika węzła $x$ w drzewie BST. Algorytm działa rekurencyjnie, porównując klucz węzła z kluczem $x$. Jeśli klucz jest mniejszy, algorytm przeszukuje lewe poddrzewo, jeśli większy -- prawe poddrzewo. \\
        \begin{algorithm}
            \caption{TreeSuccessor}
            \begin{algorithmic}[1]
                \Procedure{TreeSuccessor}{x}
                \If{$x.right \neq null$}
                    \State \textbf{return} TreeMin($x.right$)
                \Else
                    \State $y = x.parent$
                    \While{$y \neq null$ and $x = y.right$}
                        \State $x = y$
                        \State $y = y.parent$
                    \EndWhile
                    \State \textbf{return} $y$
                \EndIf
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
        Złożoność algorytmu to $\Theta(h)$.
\end{itemize}





\section{Ćwiczenia}
tu beda pojawialy sie notatki z cwiczen do przedmiotu Algorytmy i struktury danych na Politechnice Wrocławskiej na kierunku Informatyka Algorytmiczna rok 2025 semestr letni.

\subsection{Lista 2}
robiona na zajęciach \date{2025-03-10}
\subsubsection{zadanie 1}
Wylicz ile linijek wypisze poniższy program (podaj wynik będacy funkcją od n w postaci asymptotycznej $\Theta(\cdot)$). Można założyć, że $n$ jest potęgą liczby $3$.
\begin{algorithm}
\begin{algorithmic}[1]
\State \textbf{function} f(n)
\If{$n > 1$}
    \State print\_line('still going')
    \State f(n/3)
    \State f(n/3)
\EndIf
\end{algorithmic}
\end{algorithm}
w pseudo kodzie pojawia sie nastepujaca rekurencja:
\[
    T(n) = 2T(\frac{n}{3}) + 1
\]
rozwiąże ją używając metody podstawienia. Niech $n=3^k, k = \log_3 n$, wtedy:
\[
    T(3^k) = 2T(3^{k-1}) + 1
\]
Zatem przyjmując $S(k) = T(3^k)$ mamy:
\[
    S(k) = 2S(k-1) + 1
\]
rozwiązując rekurencję otrzymujemy:
\[
    S(k) = 2^k - 1
\]
zatem
\[
    T(n) = 2^{\log_3 n} - 1 = n^{\log_3 2} - 1 = \Theta(n^{\log_3 2})
\]
analogicznie liczmy jaka jest wykonana ``praca'' wykonana przez program w drzweie rekursji.

\subsubsection{zadanie 2}
Niech $f(n)$ i $g(n)$ będą funkcjami asymptotycznie nieujemnymi (tzn. nieujemnymi dla dostatecznie dużego $n$). Korzystając z definicji notacji $\Theta$, udowodnij, że:
\[
\max\{f(n), g(n)\} = \Theta(f(n) + g(n)).
\]
\begin{proof}
    Z definicji notacji $\Theta$ mamy:
    \[
        f(n)=\Theta(g(n)) \iff \exists c_1, c_2 > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n)
    \]
    skoro $f(n)$ i $g(n)$ są asymptotycznie nieujemne to:
    \[
        \exists n_f: \forall n \geq n_f, f(n) \geq 0
    \]
    \[
        \exists n_g: \forall n \geq n_g, g(n) \geq 0
    \]
    zatem
    \[
        n_0=\max\{n_f, n_g\}
    \]
    a więc
    \[
        f(n) \leq \max\{f(n), g(n)\}
    \]
    \[
        g(n) \leq \max\{f(n), g(n)\}
    \]
    dodając obie nierówności otrzymujemy:
    \[
        f(n) + g(n) \leq 2 \cdot \max\{f(n), g(n)\}
    \]
    zatem
    \[
        \forall n \geq n_0: \max\{f(n), g(n)\} \leq f(n) + g(n) \leq 2 \cdot \max\{f(n), g(n)\}
    \]
    a więc z definicji mamy
    \[
        \max\{f(n), g(n)\} = \Theta(f(n) + g(n))
    \]
\end{proof}

\subsubsection{zadanie 3}
Wylicz asymptotyczną złożoność (używając notacji $\Theta$) poniższych fragmentów programów:

\begin{algorithm}
\caption{Pierwszy fragment kodu}
\begin{algorithmic}[1]
\For{$i = 1$ to $n$}
    \State $j = i$
    \While{$j < n$}
        \State $sum = P(i, j)$
        \State $j = j + 1$
    \EndWhile
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Drugi fragment kodu}
\begin{algorithmic}[1]
\For{$i = 1$ to $n$}
    \State $j = i$
    \While{$j < n$}
        \State $sum = R(i, j)$
        \State $j = j + j$
    \EndWhile
\EndFor
\end{algorithmic}
\end{algorithm}
Gdzie:
\begin{itemize}
    \item koszt wykonania procedury $P(i,j)$ wynosi $\Theta(1)$,
    \item koszt wykonania procedury $R(i,j)$ wynosi $\Theta(j)$.
\end{itemize}

\begin{proof}
    \begin{itemize}
        \item Pierwszy fragment kodu
            \begin{itemize}
                \item Wewnętrzna pętla wykonuje się $n-i$ razy
                \item Koszt wykonania procedury $P(i,j)$ wynosi $\Theta(1)$
                \item Zatem koszt wykonania wewnętrznej pętli wynosi $\Theta(n-i)$
                \item Zatem koszt wykonania całego fragmentu wynosi
                    \[
                        \sum_{i=1}^{n} \Theta(n-i) = \Theta(n^2)
                    \]
            \end{itemize}
        \item Drugi fragment kodu
            \begin{itemize}
                \item Wewnętrzna pętla wykonuje się $\log_2 n$ razy
                \item Koszt wykonania procedury $R(i,j)$ wynosi $\Theta(j)$
                \item Zatem koszt wykonania wewnętrznej pętli wynosi $\Theta(\log_2 n)$
                \item Zatem koszt wykonania całego fragmentu wynosi
                    \[
                        \sum_{i=1}^{n} \Theta(\log_2 n) = \Theta(n \log_2 n)
                    \]
            \end{itemize}
    \end{itemize}
\end{proof}
Dla pewnosci sprawdzone empirycznie:
%zalacz wykres wykresAlgoZad3.png
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{wykresAlgoZad3.png}
\end{figure}

\subsubsection{zadanie 4}
Wyznacz asymptotyczne oszacowanie górne dla następujących rekurencji:

\begin{itemize}
    \item $T(n) = 2T(n/2) + 1$
    \item $T(n) = 2T(n/2) + n$
    \item $T(n) = 3T(n/2) + n \log n$
\end{itemize}

\bigskip
\hrule
\bigskip

Kożystając z \textbf{Master Theorem} możemy wyznaczyć ograniczenie dla tych rekurencji.
\begin{itemize}
    \item $T(n) = 2T(n/2) + 1$
        \begin{proof}
            \[
                a = 2, b = 2, d = 0
            \]
            \[
                \log_b a = \log_2 2 = 1 > 0 = d
            \]
            \[
                T(n) = \Theta(n)
            \]
        \end{proof}
    \item $T(n) = 2T(n/2) + n$
        \begin{proof}
            \[
                a = 2, b = 2, d = 1
            \]
            \[
                \log_b a = \log_2 2 = 1 = d
            \]
            \[
                T(n) = \Theta(n \log n)
            \]
        \end{proof}
    \item $T(n) = 3T(n/2) + n \log n$
        \begin{proof}
            \text{Dolne ograniczenie}
            \[
                T(n) = 3T(n/2) + n \implies^{\text{Master Theorem}} T(n) = \Theta(n^{\log_2 3})
            \]
            \text{Górne ograniczenie}
            \[
                T(n) = 3T(n/2) + n^{1.1} \implies^{\text{Master Theorem}} T(n) = \Theta(n^{1.1})
            \]
        \end{proof}
\end{itemize}

\subsubsection{zadanie 5}
Zaprojektuj algorytm wczytujący z wejścia tablicę liczb $A[1], \ldots, A[N]$ i przygotowujący tablicę $B$ tak, że na jej podstawie będzie potrafił odpowiadać na pytania:
\begin{enumerate}
    \item ile wynosi suma elementów tablicy $A$ od miejsca $i$ do miejsca $j$ włącznie, dla $i < j$.
    \item Jaka jest złożoność czasowa Twojego algorytmu? Ile pamięci zajmuje tablica $B$?
    \item Ile zajmuje odpowiedź na jedno pytanie?
\end{enumerate}

\bigskip
\hrule
\bigskip

Przykładowy algorytm mógłby wyglądać następująco:
\begin{algorithm}
    \caption{Algorytm do zadania 5.}
    \begin{algorithmic}[1]
        \State $B[1] = A[1]$
        \For{$i = 2$ to $N$}
            \State $B[i] = B[i-1] + A[i]$
        \EndFor
        \Procedure{Sum}{i, j} %uwaga na edge case
        \If {$i=1$}
            \State \Return $B[j]$
        \Else
            \State \Return $B[j] - B[i-1]$
        \EndIf
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

Co tu się dzieje?
\begin{itemize}
    \item W pierwszej pętli obliczamy sumy prefiksowe tablicy $A$ i zapisujemy je w tablicy $B$.
    \item W procedurze \texttt{Sum} zwracamy różnicę między dwoma elementami tablicy $B$.
\end{itemize}

\begin{itemize}
    \item Złożoność czasowa algorytmu wynosi $\Theta(n)$.
    \item Tablica $B$ zajmuje $\Theta(n)$ pamięci.
    \item Odpowiedź na jedno pytanie zajmuje $\Theta(1)$ czasu.
\end{itemize}

\subsubsection{zadanie 6}
Pokaż, jak grać w grę w "10 pytań", w której wiadomo, że wybrana liczba jest dodatnia, ale nie jest na początku znane górne ograniczenie jej wartości. Ile pytań potrzebujesz, żeby zgadnąć dowolną liczbę (liczba pytań może zależeć od wielkości liczby)?

\bigskip
\hrule
\bigskip

W grze "10 pytań" możemy zadać pytania w stylu "czy liczba jest większa od $x$?". W ten sposób możemy zredukować przestrzeń poszukiwań. W pierwszym pytaniu zapytajmy, czy liczba jest większa od $1$. Jeśli tak, to zapytajmy, czy liczba jest większa od $2$. W ten sposób możemy zredukować przestrzeń poszukiwań do $2^k$ dla pewnego $k$. W ten sposób możemy znaleźć dowolną liczbę w $k$ pytaniach.
\begin{algorithm}
    \caption{Algorytm do zadania 6.}
    \begin{algorithmic}[1]
        \State{$k = 0$}
        \While{$2^k < x$}
            \State{$k = k + 1$}
        \EndWhile
        \State{$p=2^{k-1}$}
        \State{$q=2^k$}
        \Procedure{BinarySearch}{p, q}
    \end{algorithmic}
\end{algorithm}


\subsubsection{zadanie 7}
Używając algorytmu \textbf{divide-and-conquer} do mnożenia liczb wykonaj mnożenie dwóch liczb binarnych 11011, 1010.

\bigskip
\hrule
\bigskip

Algorytm \textbf{divide-and-conquer} do mnożenia liczb działa w następujący sposób:
\begin{enumerate}
    \item Podziel liczby na dwie równe części.
    \item Rekurencyjnie pomnóż te części.
    \item Połącz wyniki.
\end{enumerate}

Mnożenie dwóch liczb binarnych $11011$ i $1010$ możemy zrealizować w następujący sposób:
\begin{enumerate}
    \item Podziel liczby na dwie równe części: $1101$, $1$ oraz $10$, $10$.
    \item Rekurencyjnie pomnóż te części: $1101 \cdot 10 = 11010$.
    \item Połącz wyniki: $11010 + 110100 = 1000000$.
\end{enumerate}

\begin{algorithm}
\caption{Algorytm do zadania 7 (pokazany na wykładzie)}
    \begin{algorithmic}[1]
        \Procedure{Mul}{x, y}
        \State{$n = \max\{|x|, |y|\}$}
        \If{$n = 1$}
        \State \Return{$x \cdot y$}
        \EndIf
        \State{$x_L, x_R = \text{LetfMost}\left\lceil\frac{n}{2}\right\rceil, \text{RightMost}\left\lceil\frac{n}{2}\right\rceil \text{bits of $x$}$}
        \State{$y_L, y_R = \text{LetfMost}\left\lceil\frac{n}{2}\right\rceil, \text{RightMost}\left\lceil\frac{n}{2}\right\rceil \text{bits of $y$}$}
        \State{$p_1 = \text{Mul}(x_L, y_L)$}
        \State{$p_2 = \text{Mul}(x_R, y_R)$}
        \State{$p_3 = \text{Mul}(x_L + x_R, y_L + y_R)$}
        \State \Return{$p_1 \cdot 2^{2n} + (p_3 - p_1 - p_2) \cdot 2^n + p_2$}
    \end{algorithmic}
\end{algorithm}

\subsection{Lista 3}
robiona na zajęciach \date{2025-03-24}

\subsubsection{zadanie 1}
Podaj algorytm scalający $k$ posortowanych list tak aby powstała jedna posortowana lista $nb$ (liczba wszystkich elementów na listach to n) działający w czasie $O(n \log k)$.

\bigskip
\hrule
\bigskip

Algorytm ten można zrealizować w następujący sposób:
\begin{algorithm}
    \caption{Algorytm do zadania 1.}
    \begin{algorithmic}[1]
        \Procedure{MergeLists}{$L_1, L_2, \ldots, L_k$}
        \State{$n = \sum_{i=1}^{k} |L_i|$}
        \State{$B = \text{tablica} [1 \ldots n]$}
        \State{$\text{heap} = \text{MinHeap}$}
        \For{$i = 1$ to $k$}
            \State{$\text{heap}.\text{insert}(L_i.\text{pop}())$}
        \EndFor
        \For{$i = 1$ to $n$}
            \State{$B[i] = \text{heap}.\text{pop}()$}
            \State{$\text{heap}.\text{insert}(L_i.\text{pop}())$}
        \EndFor
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\subsubsection{zadanie 2}
Zdefiniujmy algorytm \textit{k-MergeSort} jako uogólnienie algorytmu sortowania przez scalanie. Różni się od omawianego na wykładzie algorytmu sortowania przez scalanie tym, że dzieli sortowana tablice rekurencyjnie na k równych części (zakładamy, że liczba elementów w tablicy jest potęgą k $(n = k^l)$). \\
Używając wyniku z zadania 1 proszę wykazać dla jakiego k algorytm ma najmniejsza asymptotyczną złożoność obliczeniową liczby porównań (górne ograniczenie $O(\cdot)$).

\bigskip
\hrule
\bigskip

Algorytm \textit{k-MergeSort} spełnia rekurencję:
\[
    T(n) = kT(\frac{n}{k}) + \Theta(n \log k)
\]
gdzie $\Theta(n \log k)$ to koszt scalania $k$ posortowanych list (zadanie 1). Z \textbf{Master Theorem} otrzymujemy, że algorytm ma złożoność:
\[
    T(n) = \Theta(n \log_k n)
\]

\subsubsection{zadanie 3}
Załóżmy że tablica $A = [a_1, \dots, a_n]$ jest do pewnego momentu $k$ posortowana malejąco i dalej rosnąco (tzn. dla $\forall i < k: a_i > a_{i+1}$ oraz $\forall i \geq k: a_i < a_{i+1}$). Zaprojektuj algorytm znajdujący minimalny element w tablicy $A$, którego złożoność obliczeniowa będzie wynosić $O(\log n)$. Udowodnij poprawność działania zaproponowanego algorytmu.

\bigskip
\hrule
\bigskip

\begin{itemize}
    \item Algorytm ten można zrealizować w następujący sposób:
        \begin{algorithm}
            \caption{Algorytm do zadania 3.}
            \begin{algorithmic}[1]
                \Procedure{FindMin}{$A, p, q$}
                \If{$p = q$}
                \State \Return{$A[p]$}
                \EndIf
                \State{$m = \left\lfloor\frac{p+q}{2}\right\rfloor$}
                \If{$A[m] > A[m+1]$}
                \State \Return{$\text{FindMin}(A, m+1, q)$}
                \Else
                \State \Return{$\text{FindMin}(A, p, m)$}
                \EndIf
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item Przykład:
        \begin{itemize}
            \item $A = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]$
            \item $m = 5$
            \item $A[m] = 5$, $A[m+1] = 4$
            \item $\text{FindMin}(A, 6, 10)$
            \item $m = 8$
            \item $A[m] = 2$, $A[m+1] = 1$
            \item $\text{FindMin}(A, 9, 10)$
            \item $m = 9$
            \item $A[m] = 1$, $A[m+1] = 1$
            \item \textbf{Zwracamy} $A[m] = 1$
        \end{itemize}
    \item Złożoność obliczeniowa: Algorytm spełnia rekurencję:
        \[
            T(n) = T\left(\frac{n}{2}\right) + \Theta(1)
        \]
        Z \textbf{Master Theorem} otrzymujemy, że algorytm ma złożoność:
        \[
            T(n) = \Theta(\log n)
        \]
        \qed
\end{itemize}

\subsubsection{zadanie 4}
Zastąpienie użycia QuickSort'a, dla tablic małych rozmiarów, algorytmem InsertionSort jest częstym sposobem na zwiększenie efektywności algorytmu rozwiązującego problem sortowania. Pokaż, że jeśli zmodyfikujesz bazowy przypadek rekurencji w algorytmie QuickSort w taki sposób, że dla tablic o $\leq k$ elementach wywoływany będzie InsertionSort (zamiast rekurencyjnego wywołania QuickSort'a), to wartość oczekiwana liczby porównań będzie wynosić $\Theta\left(nk + n \log \frac{n}{k}\right)$. Jakie $k$ należy wybrać, aby zminimalizować tę złożoność?

\bigskip
\hrule
\bigskip

Należy rozważyć wartość oczekiwaną zmiennej losowej $\text{compInsertSort}_n \equiv R_n$
\[
    \mathbb{E}[R_n]=\frac{n(n-1)}{4}
\]
Niech $T_n$ -- liczba porównan w \textit{Hybrid Sorcie} oraz
\[
    X_k^n = \begin{cases}
        1 \impliedby \text{partition podzielił tablice na} \mid k \mid n-k \text{split($k,n-k$)}\\
        0 \impliedby \text{w przeciwnym przypadku}
    \end{cases}
\]
bedzie funkcja indykatorową. Wtedy
\[
    T_n = \sum_{k=0}^{n-1} X_k \cdot \left(T_k + T_{n-l} + n-1 \right)
\]
Przyjmimy oznaczenie $\mathbb{E}[T_n]=t_n$, wtedy
\begin{equation*}\begin{aligned}
    t_n &= \sum_{k=0}^{n-1} \mathbb{E}[X_k] \cdot \left(t_k + t_{n-k} + n-1 \right) \\
        &= \sum_{k=0}^{n-1} \frac{1}{n} \cdot \left(t_k + t_{n-k} + n-1 \right) \\
        &= \frac{1}{n} \sum_{k=0}^{n-1} \left(t_k + t_{n-k} + n-1 \right) \\
        &= \frac{1}{n} \sum_{k=0}^{n-1} t_k + \frac{1}{n} \sum_{k=0}^{n-1} t_{n-k} + \frac{1}{n} \sum_{k=0}^{n-1} n-1 \\
        &= \frac{2}{n} \sum_{k=0}^{n-1} t_k + n-1
\end{aligned}\end{equation*}
Mnożąc obie strony równania przez $n$ otrzymujemy
\[
    n t_n = 2 \sum_{k=0}^{n-1} t_k + n^2 - n
\]
Zamieniając $n \rightarrow n-1$ otrzymujemy
\[
    (n-1) t_{n-1} = 2 \sum_{k=0}^{n-2} t_k + n^2 - 3n + 2
\]
Odejmując stronami otrzymujemy
\[
    n t_n - (n-1) t_{n-1} = 2t_{n-1} + 2n - 2
\]
Upraszczając:
\[
    n t_n = (n+1) t_{n-1} + 2(n - 1)
\]
Dzieląc obie strony przez $n(n+1)$ otrzymujemy
\[
    \frac{t_n}{n+1} = \frac{t_{n-1}}{n} + \frac{2(n-1)}{n(n+1)}
\]
teraz przyjmujemy oznaczenie $\varphi_n = \frac{t_n}{n+1}$, wtedy
\[
    \varphi_n = \varphi_{n-1} + \frac{2(n-1)}{n(n+1)}
\]
należy rozwiązać powyższą rekurencję itracyjnie, biorąc pod uwagę, że w pewnym momencie następuje zmiana z algorytmu \textit{QuickSort} na \textit{InsertionSort}, niech $k$ tym momentem, wtedy
\begin{equation*}\begin{aligned}
    \varphi_n &= \varphi_{n-1} + \frac{2(n-1)}{n(n+1)} =\\
              &= \sum_{i=k+1}^{n} \frac{2(i-1)}{i(i+1)} + \frac{k(k-1)}{4(k+1)} \\
              &= \sum_{i=k+1}^{n} \left(\frac{2}{i+1} - \frac{1}{i}\right) + \frac{k(k-1)}{4(k+1)} =\\
              &= 2\sum_{i=k+1}^{n} \frac{1}{i+1} - \sum_{i=k+1}^{n} \frac{1}{i} + \frac{k(k-1)}{4(k+1)} =\\
              &= 2\left( H_{n+1} - H_{k} \right) - \left( H_{n} - H_{k} \right) + \frac{k(k-1)}{4(k+1)} =\\
              &= H_{n} + \frac{2}{n+1} -  H_{k} - \frac{k(k-1)}{4(k+1)}
\end{aligned}\end{equation*}
Podstawiając $t_n = (n+1) \varphi_n$ otrzymujemy
\begin{equation*}\begin{aligned}
    t_n &= (n+1) \left( H_{n} + \frac{2}{n+1} -  H_{k} - \frac{k(k-1)}{4(k+1)} \right) = \\
        &= (n+1) H_{n} + 2 - (n+1) H_{k} - \frac{k(k-1)}{4(k+1)}(n+1)
\end{aligned}\end{equation*}
Ostatecznie prowadzi to do asymptotycznego wzoru na liczbę porównań
\[
    \mathbb{E}[T_n] = \Theta\left(nk + n \log \frac{n}{k}\right)
\]
\qed



\subsubsection{zadanie 5}
Załóżmy, że masz do wyboru jeden z trzech algorytmów rozwiązujących postawiony Ci problem wielkości $n$:
\begin{enumerate}
  \item \textbf{Algorytm A}: rozwiązuje problem dzieląc go rekurencyjnie na 5 pod-problemów o połowę mniejszych i scalając ich rozwiązania w czasie $\Theta(n \log n)$.
  \item \textbf{Algorytm B}: rozwiązuje problem dzieląc go rekurencyjnie na 2 pod-problemy rozmiaru $n - 1$ i scala ich rozwiązania w czasie stałym.
  \item \textbf{Algorytm C}: rozwiązuje problem dzieląc go rekurencyjnie na 9 pod-problemów rozmiaru $n/3$ i scalając ich rozwiązania w czasie $\Theta(n^2)$.
\end{enumerate}

Jaka jest złożoność obliczeniowa tych algorytmów? Który z nich byś wybrał? Odpowiedź uzasadnij.

\bigskip
\hrule
\bigskip


\begin{enumerate}
    \item Spełniona zostaje rekurencja:
        \[
            T(n) = 5T\left(\frac{n}{2}\right) + \Theta(n\log n)
        \]
        Ograniczmy ją sobie z dołu i z góry:
        \[
            T(n) = \Omega(n\log n)
        \]
        \[
            T(n) = O(n\log n)
        \]
        Zatem z \textbf{Master Theorem} otrzymujemy, że złożoność obliczeniowa algorytmu A wynosi \\$\Theta(n\log n)$.
    \item Spełniona zostaje rekurencja:
        \[
            T(n) = 2T(n-1) + \Theta(1)
        \]
        Ograniczmy ją sobie z dołu i z góry:
        \[
            T(n) = \Omega(n)
        \]
        \[
            T(n) = O(n)
        \]
        Zatem z \textbf{Master Theorem} otrzymujemy, że złożoność obliczeniowa algorytmu B wynosi $\Theta(n)$.
    \item Spełniona zostaje rekurencja:
        \[
            T(n) = 9T\left(\frac{n}{3}\right) + \Theta(n^2)
        \]
        Ograniczmy ją sobie z dołu i z góry:
        \[
            T(n) = \Omega(n^2)
        \]
        \[
            T(n) = O(n^2)
        \]
        Zatem z \textbf{Master Theorem} otrzymujemy, że złożoność obliczeniowa algorytmu C wynosi $\Theta(n^2)$.
\end{enumerate}

\subsubsection{zadanie 6}
Powiedzmy, że masz do wykonania $n$ zadań, gdzie każde z nich wymaga $t_j$ minut pracy. Chcesz wykonać wszystkie zadania maksymalizując zadowolenie przełożonego poprzez minimalizację średniego czasu zakończenia każdego zadania. Uzasadnij, w jakiej kolejności powinieneś wykonywać zadania.

\bigskip
\hrule
\bigskip

Możemy przyjąć, że mamy tablicę $T=[t_1, t_2, \ldots, t_n]$ z czasami trwania zadań. Możemy zrealizować algorytm w następujący sposób:
\begin{itemize}
    \item Zadania powinny być wykonywane w kolejności rosnącej czasu pracy. Posortować tablicę $T$ rosnąco i wykonywać zadania pokoleji.
    \item Dla każdego zadania $j$ zakończenie zadania $j$ w czasie $t_j$ minimalizuje średni czas zakończenia każdego zadania. Dlaczego się tak dzieje?\\
        Rozpatrzmy najpierw mały przykład: \\
        Mamy 3 zadania
        \[
            T= [t_1=3, t_2=5, t_3=4].
        \]
        Posortowane zadania to
        \[
            T= [t_1=3, t_3=4, t_2=5].
        \]
        teraz czasy zakonczenia wykonywania zadań, to
        \[
            t= [t_1, t_1+t_3, t_1+t_3+t_2]
        \]
        \[
            t=[3, 3+4, 3+4+5]
        \]
        Średni czas zakończenia zadania to
        \[
            \frac{3+7+12}{3}=7\frac{1}{3}
        \]
        Rozważmy teraz ogólny problem:
        \begin{itemize}
            \item \textbf{Input}: tablica z długoścami trwania wykonywania zadań $T=[t_1, t_2, \dots, t_n]$
            \item \textbf{Output}: pewna permutacja tablicy $T$, która minimalizuje średni czas zakończenia zadania -- $\sigma(T)$
            \item \textbf{Problem}: należy znaleść pewne minimum:
                \[
                    \min \left\{ f(\sigma(T)): \sigma \in S_n \right\}
                \]
                gdzie $f: \mathcal{P}\left(\mathbb{N}\right) \rightarrow \mathbb{R}$ jest funkcją postaci:
            \[
                f(T) = \frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{i} t_j
            \]
            można to zapisać jako:
            \[
                f(T) = \frac{nt_1 + (n-1)t_2 + \dots + 2t_{n-1} + t_n}{n}
            \]
        \item \textbf{Rozwiązanie}: minimalizacja funkcji $f$ polega na posortowaniu tablicy $T$ rosnąco, nalezy jednak udowodnić, że permutacja $\sigma$ sortująca tablicę $T$ jest optymalna.
            %należy wziąść dowolną permutację $\tau$ i biorac permutacje $\tau'$, ktora rozni sie od tau tylko miejscami i i j, i < j, tak, ze t_i > t_j, to mozemy pokazac, ze f(tau') > f(tau), co oznacza i iterujac ten proces dojdziemy do permutacji optymalnej -- sortujacej
            \textbf{Dowód:} \\[1mm]
            Załóżmy, że istnieje optymalna permutacja $\tau$, w której występuje para indeksów $i < j$ taka, że $t_i > t_j$. Rozważmy nową permutację $\tau'$, otrzymaną przez zamianę miejscami zadań $i$ oraz $j$, czyli:
            \[
                \tau' = (\dots, t_j, \dots, t_i, \dots)
            \]
            Przyjrzyjmy się wpływowi tej zamiany na funkcję celu
            \[
                f(T) = \frac{1}{n}\sum_{k=1}^{n} \sum_{l=1}^{k} t_{\tau(l)}.
            \]
            W wyniku zamiany, zadania $t_i$ i $t_j$ pojawiają się na pozycjach $i$ oraz $j$, odpowiednio, co wpływa na sumy częściowe w następujący sposób:
            \[
                \Delta = f(\tau') - f(\tau) = (t_i - t_j)(j-i).
            \]
            Skoro $j-i > 0$ oraz przy założeniu $t_i > t_j$, mamy
            \[
                \Delta > 0.
            \]
            Oznacza to, że funkcja celu $f$ jest mniejsza dla permutacji $\tau'$ niż dla $\tau$, czyli:
            \[
                f(\tau') < f(\tau).
            \]
            Sprzeczność z założeniem, że $\tau$ była optymalna, wynika z faktu, iż zawsze można dokonać zamiany pary, gdzie wcześniejsze zadanie trwa dłużej niż późniejsze, co zmniejsza średni czas zakończenia. \\[1mm]
            W związku z tym, aby nie istniały żadne takie "inwersje", permutacja optymalna musi spełniać warunek
            \[
                t_{\tau(1)} \le t_{\tau(2)} \le \dots \le t_{\tau(n)},
            \]
            czyli musi być uporządkowana rosnąco. \\[1mm]
            \textbf{Wniosek:} Minimalizacja średniego czasu zakończenia zadań jest osiągana przez sortowanie tablicy $T$ w porządku rosnącym.
        \end{itemize}
\end{itemize}

\subsubsection{zadanie 7}
Stwórz algorytm znajdujący najczęściej powtarzający się element w $n$ elementowej tablicy (unikając sortowania tablicy), mający złożoność $O(n \log n)$ (zakładamy, że ten element powtarza się ponad $\frac{n}{2}$ razy).

\bigskip
\hrule
\bigskip

\begin{algorithm}[H]
    \caption{Algorytm do zadania 7.}
    \begin{algorithmic}[1]
        \Procedure{Dominat}{$A, p, q$}
        \If{$p = q$}
        \State \Return{$A[p]$}
        \EndIf
        \State{$dom_L = \text{Dominat}(A, p, \left\lfloor\frac{p+q}{2}\right\rfloor)$}
        \State{$dom_R = \text{Dominat}(A, \left\lfloor\frac{p+q}{2}\right\rfloor + 1, q)$}
        \If{$dom_L = dom_R$}
        \State \Return{$dom_L$}
        \EndIf
        \State{$count_L = \text{count}(A, p, q, dom_L)$}
        \State{$count_R = \text{count}(A, p, q, dom_R)$}
        \If{$count_L > count_R$}
        \State \Return{$dom_L$}
        \Else
        \State \Return{$dom_R$}
        \EndIf
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

Złożoność obliczeniowa algorytmu:
\[
    T(n) = 2T\left(\frac{n}{2}\right) + \Theta(n)
\]
Z \textbf{Master Theorem} otrzymujemy, że złożoność obliczeniowa algorytmu wynosi $\Theta(n \log n)$.\\
Jest to problem z kategorii \textit{Majority element}.

Algorytm działający w czasie liniowym:
\begin{algorithm}[H]
    \caption{Algorytm do zadania 7. działajcy w czasie liniowym.}
    \begin{algorithmic}[1]
        \Procedure{Majority}{$A$}
        \State{$count = 0$}
        \State{$candidate = None$}
        \For{$i = 0$ to $n-1$}
            \If{$count = 0$}
                \State{$candidate = A[i]$}
            \EndIf
            \If{$A[i] = candidate$}
                \State{$count = count + 1$}
            \Else
                \State{$count = count - 1$}
            \EndIf
        \EndFor
        \State \Return{$candidate$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\subsubsection{zadanie 8}
Doktor Freud ma wahania nastrojów, które zapisuje sobie ilustrując nastrój danego dnia nieujemną liczbą całkowitą. Po $n$ dniach zgromadził tablice $n$ liczb opisujących swój nastrój i postanowił znaleść (spójny) przedział czasu (dni), w których był najszczęśliwszy. Doktor Freud zdefiniował sobie szczęśliwość przedziału czasu jako sumę wartości występujących w dniach tego przedziału przemnożony przez najmniejszą wartość występującą w zadanym przedziale. Stwórz algorytm D\&C znajdujący najszczęśliwszy przedział w złożoności $O(n \log n)$

\bigskip
\hrule
\bigskip

%TODO sprobowac zrobic samemu

\subsubsection{zadanie 9}
Wykaż, że nie istnieje algorytm sortujący, który działa w czasie liniowym dla co najmniej połowy z $n!$ możliwych danych wejściowych długości $n$. Czy odpowiedź ulegnie zmianie jeśli zapytamy o ułamek $\frac{1}{n}$ lub $\frac{1}{n^2}$ wszystkich permutacji?

\bigskip
\hrule
\bigskip

Do pokazania tego faktu skożystamy z drzewa decyzyjnego, podobnie jak na wykładzie. Rozważmy nierówność gdzie $h$-wysokość drzewa, $l$-# liści
\begin{enumerate}
    \item
        \[
            \frac{n!}{2} \leq l \leq 2^h \implies 2^{h+1} \geq n! \implies h \geq \log_2 n! -1
        \]
        zatem $h = \Omega (n \log n)$
    \item
        \[
            \frac{n!}{n} \leq 2^h \implies h \geq \log_2 n! - \log_2 n \implies h = \Omega(n \log n)
        \]
    \item
        \[
            \frac{n!}{2^n} \leq 2^h \implies h+n \geq \log_2 n! \implies h \geq log_2 n! - n \implies h = \Omega(n \log n)
        \]
\end{enumerate}

\subsubsection{zadanie 10}
Zaprojektuj algorytm, który sortuje $n$ liczb całkowitych z przedziału od $1$ do $n^2$ w czasie $O(n)$.

\bigskip
\hrule
\bigskip

\begin{algorithm}[H]
    \caption{Algorytm do zadania 10.}
    \begin{algorithmic}[1]
        \Procedure{Sort}{$A, n$}
        \For{$i = 1$ to $n$}
        \State{$B[i][1] = (A[i]-1) \text{div} n$}
        \State{$B[i][2] = (A[i]-1) \text{mod} n$}
        \EndFor
        \EndProcedure
    \end{algotithmic}
\end{algorithm}


\end{document}
