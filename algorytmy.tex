\documentclass[11pt,a4paper]{article}

% Kodowanie i obsługa języka polskiego
\usepackage[utf8]{inputenc}      % Kodowanie wejścia
\usepackage[T1]{fontenc}         % Kodowanie fontów
\usepackage[polish]{babel}       % Obsługa języka polskiego

% Pakiety matematyczne i inne przydatne
\usepackage{marginnote}
\usepackage{tikz}
\usetikzlibrary{trees}
\usepackage{amsmath, amssymb, amsthm}  % Pakiety do matematyki
\usepackage{graphicx}                  % Obsługa grafiki
\usepackage{hyperref}                  % Linki i spis treści
\usepackage{geometry}                  % Ustawienie marginesów
\usepackage{algorithm}               % Algorytmy
\usepackage{algpseudocode}           % Pseudokod
\geometry{margin=2cm}                  % Ustawienie marginesów na 2 cm

% Dodatkowe ustawienia
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\title{Notatki z Algorytmów i Struktur Danych}
\author{Jakub Kogut}

\begin{document}

\maketitle

\tableofcontents  % Spis treści (opcjonalnie)
\newpage

\section{Wstęp}
To będa notatki z przedmiotu Algorytmy i struktury danych na Politechnice Wrocławskiej na kierunku Informatyka Algorytmiczna rok 2025 semestr letni.
\subsection{Informacje}
Prowadzący Przedmiot: \textbf{Zbychu Gołębiewski}
\begin{itemize}
    \item Należy kontaktować się przez maila: \href{mailto:zbigniew.golebiewski@pwr.edu.pl}{mail}
    \item Konsultacje \textbf{216/D1}:
        \begin{itemize}
            \item Wtorek 13:00-15:00
            \item Środa 9:00-11:00
        \end{itemize}
    \item Wiecej info na stronie \href{https://cs.pwr.edu.pl/golebiewski/teaching/aisd.php}{przedmiotu}
    \item Literatura
        \begin{itemize}
            \item Algorithms, Dasgupta, Papadimitriou, Vazirani
            \item Algorithms, Sedgewick, Wayne (strona internetowa książki)
            \item Algorithms Designs, Jon Kleinberg and Eva Trados
            \item Wprowadzenie do algorytmów, Cormen, Leiserson, Rivest, Stein
            \item Sztuka programowania (wszystkie tomy), Donald E. Knuth
        \end{itemize}
\end{itemize}
\subsection{Ocenianie}
Ocena z kursu składa się z:
\begin{itemize}
    \item Oceny z egzaminu -- E
    \item Oceny z ćwiczeń -- C
    \item Oceny z laboratorium -- L
\end{itemize}
Wszystkie oceny są z zakresu $[0,100]$. Ocena końcowa jest wyliczana ze wzoru:
\[
    K = \frac{1}{2}E + \frac{1}{4}C + \frac{1}{4}L
\]

\section{Wykład \date{2025-03-03}}
\subsection{Przykładowy Problem}
Sortowanie:
\begin{itemize}
    \item Input: $n$ liczb $a_1, a_2, \ldots, a_n, |A|$, gdzie $|A|$ to długośc tablicy
    \item Output: permutacja $a_1', a_2', \ldots, a_n'$ taka, że $a_1' \leq a_2' \leq \ldots \leq a_n'$
\end{itemize}
Najważniejsze w algorytmach jest to, żeby były POPRAWNE: edge case, ...

\subsection{Jak mierzyć złożoność algorytmów}
\begin{enumerate}
    \item Worst Case Analysis T(n) $\leftarrow$ stosowane najcześciej
    \item Average Case Analysis
        \begin{itemize}
            \item zakładamy pewnien rozkład prawdopodobieństwa na danych wejściowych
            \item $T$ -- zmienna losowa liczby operacji wykonanych przez algorytm
                \[
                    T(n) = \max\{\# \text{operacji dla danego wejścia}\}
                \]
            \item $E[T]$ -- wartość oczekiwana $T$ $\rightarrow$ średnia liczba operacji, to co nas interesuje
        \end{itemize}
\end{enumerate}
\subsection{Przykład algorytmu}
W tej sekcji mamy pokazany przykład jak pisać pseudo kod:
\begin{algorithm}[H]
    \caption{Merge Sort}\label{alg:merge_sort}
    \begin{algorithmic}[1]
    \Procedure{MergeSort}{A, 1, n}
        \If{|A[1..n]| == 1}
            \State \Return{A[1..n]}
        \Else
            \State $B = \text{MergeSort}(A, 1, \lfloor n/2 \rfloor)$
            \State $C = \text{MergeSort}(A, \lfloor n/2 \rfloor, n)$
            \State \Return{Merge(B, C)}
        \EndIf
    \EndProcedure
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
    \caption{Merge}\label{alg:merge}
    \begin{algorithmic}[1]
    \Procedure{Merge}{X[1..k], Y[1..n]}
        \If{$X = \emptyset$}
            \State \Return{$Y$}
        \ElsIf{$Y = \emptyset$}
            \State \Return{$X$}
        \ElsIf{$X[1] \leq Y[1]$}
            \State \Return{$[X[1]] \times \text{Merge}(X[2..k], Y[1..n])$}
        \Else
            \State \Return{$[Y[1]] \times \text{Merge}(X[1..k], Y[2..n])$}
        \EndIf
    \EndProcedure
    \end{algorithmic}
\end{algorithm}
\subsection{Przykład działania Merge Sort}
\textbf{Example: Sorting the array \([10,\, 2,\, 5,\, 3,\, 7,\, 13,\, 1,\, 6]\) step by step}

\begin{enumerate}
  \item \textbf{Initial split:}
  \[
    [\,10,\, 2,\, 5,\, 3,\, 7,\, 13,\, 1,\, 6\,]
    \quad\longrightarrow\quad
    [\,10,\, 2,\, 5,\, 3\,] \quad\text{and}\quad [\,7,\, 13,\, 1,\, 6\,].
  \]

  \item \textbf{Sort the left half \([10,\, 2,\, 5,\, 3]\):}
  \begin{enumerate}
    \item Split into \([10,\, 2]\) and \([5,\, 3]\).
    \item \(\text{MergeSort}([10,\, 2])\):
    \begin{itemize}
      \item Split into \([10]\) and \([2]\).
      \item Each is already sorted (single element).
      \item Merge: \([2,\, 10]\).
    \end{itemize}
    \item \(\text{MergeSort}([5,\, 3])\):
    \begin{itemize}
      \item Split into \([5]\) and \([3]\).
      \item Each is already sorted.
      \item Merge: \([3,\, 5]\).
    \end{itemize}
    \item Merge \([2,\, 10]\) and \([3,\, 5]\) to get \([2,\, 3,\, 5,\, 10]\).
  \end{enumerate}

  \item \textbf{Sort the right half \([7,\, 13,\, 1,\, 6]\):}
  \begin{enumerate}
    \item Split into \([7,\, 13]\) and \([1,\, 6]\).
    \item \(\text{MergeSort}([7,\, 13])\):
    \begin{itemize}
      \item Split into \([7]\) and \([13]\).
      \item Each is already sorted.
      \item Merge: \([7,\, 13]\).
    \end{itemize}
    \item \(\text{MergeSort}([1,\, 6])\):
    \begin{itemize}
      \item Split into \([1]\) and \([6]\).
      \item Each is already sorted.
      \item Merge: \([1,\, 6]\).
    \end{itemize}
    \item Merge \([7,\, 13]\) and \([1,\, 6]\) to get \([1,\, 6,\, 7,\, 13]\).
  \end{enumerate}

  \item \textbf{Final merge:} Merge the two sorted halves:
  \[
    [\,2,\, 3,\, 5,\, 10\,] \quad\text{and}\quad [\,1,\, 6,\, 7,\, 13\,]
    \quad\longrightarrow\quad [\,1,\, 2,\, 3,\, 5,\, 6,\, 7,\, 10,\, 13\,].
  \]
\end{enumerate}

\noindent
Hence, after all the recursive splits and merges, the final sorted array is:
\[
[\,1,\, 2,\, 3,\, 5,\, 6,\, 7,\, 10,\, 13\,].
\]

\subsection{Złożoność Merge Sort}
\begin{itemize}
    \item Złożoność czasowa
        \begin{itemize}
            \item $T(n) = 2T(n/2) + \Theta(n)$
            \item $T(n) = \Theta(n \log n)$
        \end{itemize}
    \item Złożoność pamięciowa
        \begin{itemize}
            \item $M(n) = n + M(n/2)$
            \item $M(n) = \Theta(n)$
        \end{itemize}
\end{itemize}

\section{Wykład \date{2025-03-10}}
\subsection{Notacja Asypmtotyczna}
Na wykładzie będziemy omawiali:
\begin{itemize}
    \item Notację dużego O $O(n)$ //ograniczenie górne
        \begin{itemize}
                \item Definicja $O(n)$:
                \[
                    O(g(n)) = \{ f(n) \mid \exists c > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq f(n) \leq c \cdot g(n) \}
                \]
            \item Uwaga! \newline
                Jeśli
                \[
                    \limsup_{n \to \infty} \frac{f(n)}{g(n)} < \infty
                \]
                to
                \[
                    \limsup_{n \to \infty} \frac{f(n)}{g(n)} = \lim_{n \to \infty} \frac{f(n)}{g(n)}
                \]
            \item Przykład:
                \begin{itemize}
                    \item $2n^2=O(n^3)$
                        dla $n_0 = 2, c = 1$ Definicja jest spełniona
                    \item $f(n) = n^3 + O(n^2)$ jest to jeden z sposobów użycia $O(n)$
                        \[
                            \exists h(n) = O(n^2) \quad \text{takie, że} \quad f(n) = n^3 + h(n)
                        \]
                \end{itemize}
        \end{itemize}
    \item Notację omega //ograniczenie dolne
        \begin{itemize}
            \item Definicja
                \[
                    \Omega(g(n)) = \{ f(n) \mid \exists c > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq c \cdot g(n) \leq f(n) \}
                \]
            \item Przykład
                \begin{itemize}
                    \item $n^3 = \Omega(2n^2)$
                    \item $n = \Omega(\log n)$
                \end{itemize}
        \end{itemize}
    \item Notację theta $\theta(n)$ //ograniczenie z dwóch stron
        \begin{itemize}
            \item Definicja
                \[
                    \Theta(g(n)) = \{ f(n) \mid \exists c_1, c_2 > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n) \}
                \]
            \item Przykład
                \begin{itemize}
                    \item $n^3 = \Theta(n^3)$
                    \item $n^3 = \Theta(n^3 + 2n^2)$
                    \item $log n +8 + \frac{1}{12n} = \Theta(\log n)$
                \end{itemize}
            \item Uwaga!
                \[
                    f(n) = \Theta(g(n)) \iff f(n) = O(g(n)) \land f(n) = \Omega(g(n))
                \]
                Można to zapisać jako klasy funkcji:
                \[
                    \Theta(g(n)) = O(g(n)) \cap \Omega(g(n))
                \]
        \end{itemize}
    \item Patologiczny przykład:
        mamy funkcje $g(n) = n$ oraz $f(n) = n^{1+sin{\frac{\pi n}{2}}}$, a więc
        \[
            f(n) = \begin{cases}
                n^2 & \text{dla n parzystych} \\
                n & \text{dla n nieparzystych}
            \end{cases}
        \]
        wtedy
        \[
            \limsup_{n \to \infty} \frac{f(n)}{g(n)} = \infty
        \]
        \[
            \limsup_{n \to \infty} \frac{g(n)}{f(n)} = \infty
        \]
        zatem
        $f \neq O(g)$ oraz $g \neq O(f)$
    \item o małe
        \begin{itemize}
            \item Definicja
                \[
                    o(g(n)) = \{ f(n) \mid \forall c > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq f(n) < c \cdot g(n) \}
                \]
                Równoważnie
                \[
                    lim_{n \to \infty} \frac{f(n)}{g(n)} = 0
                \]
            \item Przykład
                \begin{itemize}
                    \item $n^2 = o(n^3)$ i $n^2 O(n^3)$ ale $n^2 \neq o(n^2)$
                    \item $n = o(n^2)$
                \end{itemize}
        \end{itemize}
\end{itemize}

\subsection{Rekurencja}
\begin{itemize}
    \item Metoda podstawienia (metoda dowodu indukcyjnego)
        \begin{enumerate}
        \item Zadnij Odpowiedź (bez stałych)
        \item Sprawdź przez indukcję czy odpowiedź jest poprawna
        \item Wylicz stałe
        \end{enumerate}
        \begin{itemize}
            \item Przykład
                \begin{itemize}
                    \item $T(n) = T(\frac{n}{2}) + n$
                    \item Pierwotny strzał: $T(n) = O(n^3)$
                    \item cel: Pokazać, że $\exists c>0: T(n) \leq c \cdot n^3$
                        \begin{itemize}
                            \item warunek początowy: $T(1) = 1 \leq c$
                            \item krok indukcyjny: załóżmy, że $\forall k \leq n: T(k) \leq ck^3$
                        \end{itemize}
                        \[
                            T(n) = 4T(\frac{n}{2}) + n \leq 4c(\frac{n}{2})^3 + n = \frac{1}{2}cn^3 + n \leq cn^3 \quad \text{dla} \quad c \geq 2
                        \]
                        jednakże ``Przestrzeliliśmy'' znacznie, spróbojmy wzmocnić założenie indukcyjne:
                        \[
                            T(n) \leq c_1k^2 -c_2k, k < n
                        \]
                        wtedy mamy:
                        \[
                            T(n) = 4T(\frac{n}{2}) +n \leq 4(c_1(\frac{n}{2})^2 - c_2(\frac{n}{2})) + n = c_1n^2 - 2c_2n + n \leq c_1n^2 - c_2n
                        \]
                        zatem $c_1 = 1, c_2 = 1$ i $T(n) = O(n^2)$ \qed
                \end{itemize}
            \item Przykład
                \begin{itemize}
                    \item $T(n) = 2T(\sqrt{n}) + \log n$
                        \newline
                        załóżmy, że $n$ jest potęgą liczby $2$, czyli $n = 2^m$
                        \[
                            T(2^m) = 2T(2^{\frac{m}{2}}) + m
                        \]
                        Co implikuje
                        \[
                            T(2^\frac{m}{2}) \rightarrow S(m)
                        \]
                        wtedy
                        \[
                            S(m) = 2S(\frac{m}{2}) + m
                        \]
                        rozwiązując rekurencję otrzymujemy
                        \[
                            S(m) = m \log m
                        \]
                        zatem
                        \[
                            T(n) = \log n \log \log n
                        \]
                \end{itemize}
        \end{itemize}

\end{itemize}
\section{Wykład \date{2025-03-17}}
\subsection{Dzrzewo rekursji}
Przykład dzewa rekursji:
\begin{itemize}
    \item $T(n) = T(\frac{n}{2})+T(\frac{n}{4}) + n^2$
\end{itemize}
\begin{center}
\begin{tikzpicture}
    [level distance=1.5cm,
    level 1/.style={sibling distance=4cm},
    level 2/.style={sibling distance=2cm}]
    \node {$n^2$}
        child {node {$\frac{n^2}{4}$}
            child {node {$\frac{n^2}{16}$}}
            child {node {$\frac{n^2}{64}$}}
        }
        child {node {$\frac{n^2}{16}$}
            child {node {$\frac{n^2}{64}$}}
            child {node {$\frac{n^2}{256}$}}
        };
      \node[draw=none] at (-4.5,0) {$n^2$};
      \node[draw=none] at (-4.5,-1.5) {$\frac{5}{16}n^2$};
      \node[draw=none] at (-4.5,-3) {$\frac{25}{256}n^2$};
\end{tikzpicture}
\end{center}
\subsubsection*{Uwaga!}
Nie jest to formalne rozwiązanie problemu. Nie można używać drzewa rekursji do dowodzenia złożoności algorytmów. Jest to jedynie intuicyjne podejście do problemu. Trzeba policzyć to na piechote, aby było formalnie.\newline
Aby policzyć $T(n)$ musimy policzyć sumę wszystkich wierzchołków w drzewie rekursji.
\[
    T(n) = \sum^{\infty}_{k=0} \left(\frac{5}{16}\right)^k \cdot n^2 = n^2 \sum^{\infty}_{k=0} \left(\frac{5}{16}\right)^k = n^2 \frac{1}{1-\frac{5}{16}} = n^2 \frac{16}{11} = \frac{16}{11}n^2
\]
A wiec $T(n) = O(n^2)$
\newline
Możemy to policzyć dokładniej dostajac mniejsze wyrazy w sumie.
%TODO przepisac od szymiego

\subsecton{Metoda iteracyjna}
Weźmy na przykład taką rekurencję:
\[
    T(n) = 3T(\frac{n}{4}) + n
\]
Zobaczmy co się dzieje po podstawieniu rekurencji do samej siebie:
\begin{enumerate}
    \item $T(n) = 3T(\frac{n}{4}) + n$
    \item $T(n) = 3(3T(\frac{n}{16}) + \frac{n}{4}) + n = 3^2T(\frac{n}{16}) + \frac{3}{4}n + n$
    \item $T(n) = 3^2(3T(\frac{n}{64}) + \frac{n}{16}) + \frac{3}{4}n + n = 3^3T(\frac{n}{64}) + \frac{3}{16}n + \frac{3}{4}n + n$
    \item \dots \footnote{Warto zauważyć, że jest to analogicznie do liczenia sumy wszystkich nodów drzewa rekursji}
\end{enumerate}
A więc ogólnie wychodzi:
\[
    %T(n) = \sum^{\log_{2}n}_{k=0} \frac{{3}{4}
\]
\subsection{Master Theorem}
Niech $a \geq 1, b > 1, f(n), d \in \mathbb{N}$ oraz $f(n)$ będzie funkcją nieujemną. Rozważmy rekurencję:
\[
    T(n) = aT(\frac{n}{b}) + \Theta(n^d)
\]
gdzie $a$ i $b$ są stałymi, a $f(n)$ jest funkcją nieujemną. Wtedy:
\begin{enumerate}
    \item $\Theta(n^d)$ jeśli $d > \log_b a$
    \item $\Theta(n^d \log n)$ jeśli $d = \log_b a$
    \item $\Theta(n^{\log_b a})$ jeśli $d < \log_b a$
\end{enumerate}
\subsubsection*{Szkic D-d}
Do przedstawienia problemu użyjemy drzewa rekursji. Rozważmy rekurencję:
\[
    T(n) = aT(\frac{n}{b}) + \Theta(n^d)
\]
\begin{center}
\begin{tikzpicture}
    [level distance=1.5cm,
    level 1/.style={sibling distance=4cm},
    level 2/.style={sibling distance=2cm}]
    \node {$c \cdot n^d$}
        child {node {$c \cdot \left(\frac{n}{b}\right)^d$}
            child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}}
            child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}}
        }
        child {node {$c \cdot \left(\frac{n}{b}\right)^d$}
            child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}}
            child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}}
        };
      \node[draw=none] at (-4.5,0) {$n^d$};
      \node[draw=none] at (-4.5,-1.5) {$\frac{n^d}{b^d}$};
      \node[draw=none] at (-4.5,-3) {$\frac{n^d}{b^{2d}}$};
  \end{tikzpicture} %TODO zrobic lepsze drzewo...
\end{center}
\begin{enumerate}
    \item suma kosztoów w $k$--tym kroku
        \[
            a^k c (\frac{n}{b^k})^d = c (\frac{a}{b^d})^k n^d
        \]
        gdzie $c(\frac{n}{b^k})^d$ to koszt jednego podproblemu w $k$--tym kroku
    \item obliczenie wysokości drzewa:
        \[
            \frac{n}{b^h} = 1 \rightarrow h = \log_b n
        \]
    \item Obliczenie $T(n)$
        \begin{equation*}
            T(n) = \Theta(\sum^{\log_b n}_{k=0} c\frac{a}{b^k}n^d) \\
                 &= \Theta(c \cdot n^d \sum^{\log_b n}_{k=0} (\frac{a}{b^d})^k) \\
                 &= \Theta(c \cdot n^d \frac{1-(\frac{a}{b^d})^{\log_b n + 1}}{1-\frac{a}{b^d}}) \implies T(n) = \Theta(n^d)
        \end{equation*}
    \item rozważmy 3 przypadki:
        \begin{enumerate}
            \item $d > \log_b a$ \marginpar{root -- heavy}
                \[
                    T(n) = \Theta(n^d)
                \]
            \item $d = \log_b a$ \marginpar{równo}
                \[
                    T(n) = \Theta(n^d \log n)
                \]
            \item $d < \log_b a$ \marginpar{leaf -- heavy}
                \[
                    T(n) = \Theta(n^{\log_b a})
                \]
        \end{enumerate}
\end{enumerate}

\subsubsection*{Przykłady}
\begin{itemize}
    \item $T(n) = 4T(\frac{n}{2}) + 11n$ \newline
        Wtedy kożystając z \textbf{Master Theorem} mamy:
        \[
            a = 4, b = 2, d = 1
        \]
        Jak i również
        \[
            \log_b a = \log_2 4 = 2 > 1 = d \implies T(n) = \Theta(n^2)7
        \]
    \item $T(n) = 4T(\frac{n}{3}) + 3n^2$ \newline
        Wtedy
        \[
            a = 4, b = 3, d = 2
        \]
        Jak i również
        \[
            \log_b a = \log_3 4 > 2 = d \implies T(n) = \Theta(n^{\log_3 4})
        \]
    \item $T(n) = 27T(\frac{n}{3}) + \frac{n^2}{3}$ \newline
        Wtedy
        \[
            a = 27, b = 3, d = 2
        \]
        Jak i również
        \[
            \log_b a = \log_3 27 = 3 > 2 = d \implies T(n) = \Theta(n^3\log n)
        \]
\end{itemize}

\subsection{Metoda dziel i zwyciężaj (D\&C)}
Na czym ona polega?
\begin{enumerate}
    \item Podział problemu na mniejsze podproblemy \footnote{W zapisie rekurencyjnym $T(n) = cT(cn) + \underline{n^d}$}
    \item Rozwiazanie rekurencyjnie mniejsze podpoblemy
    \item połącz rozwiązania podproblemów w celu rozwiązania problemu wejściowego
\end{enumerate}
\subsubsection{Algorytm -- Binary Search}
\begin{itemize}
    \item \textbf{Input}: posortowania tablica \texttt{A[1..n]} oraz element \texttt{x}
    \item \textbf{Output}: indeks \texttt{i} taki, że \texttt{A[i] = x} lub \texttt{0} jeśli \texttt{x} nie występuje w \texttt{A}
    \item przebieg algorytmu: %pseudokod
        \begin{algorithm}[H]
            \caption{Binary Search}
            \begin{algorithmic}[1]
                \Procedure{BinarySearch}{A, x}
                \State $l = 1$
                \State $r = |A|$
                \While{$l \leq r$}
                \State $m = \lfloor \frac{l+r}{2} \rfloor$
                \If{$A[m] = x$}
                \State \Return{$m$}
                \ElsIf{$A[m] < x$}
                \State $l = m + 1$
                \Else
                \State $r = m - 1$
                \EndIf
                \EndWhile
                \State \Return{0}
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item \textbf{Asypmtotyka}
        Algorytm spełnia następująca rekurencje:
        \[
            T(n) = T(\frac{n}{2}) + \Theta(1)
        \]
        Rozwiązując za pomocą \textbf{Master Theorem} otrzymujemy:
        \[
            T(n) = \Theta(\log n)
        \]
\end{itemize}

\subsubsection{Algorytm -- potęgowanie liczby do naturalnej potęgi}
\begin{itemize}
    \item \textbf{Problem}: obliczanie $x^n$\\
        Można rozbić mnożenie $n$ $x$ na odpowiednie podproblemy:
        %podkresl n/2 xksów
        \[
            x^n = \underbrace{x \cdot x \cdot \dots \cdot x}_{\frac{n}{2}} \cdot \underbrace{x \cdot x \cdot \dots \cdot x}_{\frac{n}{2}}
        \]
        A więc mamy:
        \[
            x^n = \begin{cases}
                x^{\frac{n}{2}} \cdot x^{\frac{n}{2}} & \text{dla n parzystych} \\
                x^{\frac{n-1}{2}} \cdot x^{\frac{n-1}{2}} \cdot x & \text{dla n nieparzystych}
            \end{cases}
        \]
    \item \textbf{Asymptotyka}: \\
        Algorytm spełnia następującą rekurencję:
        \[
            T(n) = T(\frac{n}{2}) + \Theta(1)
        \]
        Rozwiązując za pomocą \textbf{Master Theorem} otrzymujemy:
        \[
            T(n) = \Theta(\log n)
        \]
\end{itemize}

\subsubsection{Obliczenie n-tej liczby Fibonacciego}
\begin{itemize}
    %TODO dorysowac drzewo jak u Szymiego
    \item \textbf{Problem}:
        \[
            F_n = \begin{cases}
                0 & \text{dla n = 0} \\
                1 & \text{dla n = 1} \\
                F_{n-1} + F_{n-2} & \text{dla n > 1}
            \end{cases}
        \]
    \item \textbf{Algorytmy}:
        \begin{enumerate}
            \item Naiwna rekurencja używająca definicji. Asymptotyka wynosi $\Theta(\phi^n)$
            \item \textit{bottom up} -- iteracyjne obliczanie kolejnych liczb Fibonacciego. Asymptotyka wynosi $\Theta(n)$
            \item Kożystanie z wzoru wynikającego z rozwiązanej rekurencji:
                \[
                    F_n = \frac{1}{\sqrt{5}} \left( \left(\frac{1+\sqrt{5}}{2}\right)^n - \left(\frac{1-\sqrt{5}}{2}\right)^n \right)
                \]
                Problem z tym podejsciem polega na niedokładnym przybilżeniu przez komputery wartości $\phi$
            \item Kożystając z lematu:
                \[
                    \begin{pmatrix}
                        1 & 1 \\
                        1 & 0
                    \end{pmatrix}^n
                    =
                    \begin{pmatrix}
                        F_{n+1} & F_n \\
                        F_n & F_{n-1}
                    \end{pmatrix},
                \]
                \begin{proof}
                    \begin{enumerate}
                        \item Krok indukcyjny:
                            \[
                                %TODO dowod
                            \]
                    \end{enumerate}
                \end{proof}
                Algorytm ten ma złożoność $\Theta(n\log n)$
        \end{enumerate}
\end{itemize}

\subsubsection{Mnożenie liczb}
\begin{itemize}
    \item \textbf{Input}: $x, y$ takie, że $\max\{|x|, |y|\}$
    \item \textbf{Output}: $x \cdot y$
    \item \textbf{Algorytmy}:
        \begin{enumerate}
            \item standardowe mnożenie szkolne -- mnożenia w słupku jego asyptotyka wynosi $\Theta(n^2)$
            \item Podejście metodą \textbf{D\&C}
                \begin{itemize}
                    \item \textbf{Podejście}: Rozbijamy liczby na dwie równe części, a następnie mnożymy je przez siebie \\
                        Możemy zapisać $x$ oraz $y$ jako:
                        \[
                            x = \underbrace{x_L \cdot 2^{\frac{n}{2}}}_{\frac{n}{2} \text{bitów}} +\underbrace{x_R}_{\frac{n}{2} \text{bitów}}
                        \]
                        \[
                            y = \underbrace{y_L \cdot 2^{\frac{n}{2}}}_{\frac{n}{2} \text{bitów}} +\underbrace{y_R}_{\frac{n}{2} \text{bitów}}
                        \]
                        Proces mnożenia wygląda następująco:
                        \[
                            x \cdot y = (x_L \cdot 2^n) \cdot  %TODO przepisac od Szymiego ten algorytm
                        \]
                        Generalnie wszytkie wykonywane powyżej operacje są giga tanie bo opreacje takie jak mnożenie przez $2^k$ wiąże się jedynie z przesunięciem bitowym.
                    \item \textbf{Asymptotyka}: Nasz algorytm spełnia następującą rekurencje na podstawie zapisanego wyżej równania
                        \[
                            T(n) = 4T(\frac{n}{2}) + \Theta(n)
                        \]
                        Kożystając ponownie z \textbf{Master Theorem} można wywnioskować, że algorytm ma złożoność $\Theta(n^2)$. Zatem nie ma żadnego znacznego przyśpieszenia, nawet prawdopodobnie stała ukryta w $\Theta(n^2)$ jest gorsza niż w standardowym podjesciu
                \end{itemize}
            \item Metoda Gaussa
                \begin{itemize}
                    \item Rozważmy mnożenie liczb zespolonych
                        \[
                            (a+ib)(c+id) = ac + i(ad+bc) + bd
                        \]
                        \[
                            bc + ad = (a+b)(c+d) - ac - bd
                        \]
                        zatem
                        \[
                            x \cdot y = x_Ly_L \cdot 2^n + ((x_L + x_R)(y_L + y_R) - x_Ly_L - x_Ry_R) \cdot 2^{\frac{n}{2}} + x_Ry_R
                        \]
                    \item \textbf{Asymptotyka}: algorytm ten spełnia rekurencje
                        \[
                            T(n) = 3T(\frac{n}{2}) + \Theta(n)
                        \]
                        Z \textbf{Master Theorem} otrzymujemy, że algorytm ma złożoność $\Theta(n^{\log_2 3})$, a $\log_2 3 \approx 1.58$
                \end{itemize}
            \item Istneją jeszcze szybsze, nowsze algorytmy mnożenia liczb, takie jak algorytm Schönhage'a-Strassena bazuje ono na szybkiej transformacie Fouriera \textit{Fast Fourier Transform}, który ma złożoność $\Theta(n \log n \log \log n)$. Jednakże, trzeba wziąść pod uwagę stałą ukrytą w $\Theta$. W praktyce, dla liczb o rozmiarze do $10^6$ lepiej jest użyć standardowego algorytmu mnożenia.
        \end{enumerate}
\end{itemize}
Trochę pseudo kodu dla mnożenia liczb:
\begin{algorithm}
    \caption{Mnożenie liczb}
    \begin{algorithmic}[1]
        \Procedure{Multiply}{x, y}
        \State $n = \max\{|x|, |y|\}$
        \If{$n = 1$}
        \State \Return{$x \cdot y$}
        \EndIf
        \State $x_L, x_R = \text{LetfMost}\left\lceil\frac{n}{2}\right\rceil, \text{RightMost}\left\lceil\frac{n}{2}\right\rceil \text{bits of $x$}$
        \State $y_L, y_R = \text{LetfMost}\left\lceil\frac{n}{2}\right\rceil, \text{RightMost}\left\lceil\frac{n}{2}\right\rceil \text{bits of $y$}$
        \State $p_1 = \text{Multiply}(x_L, y_L)$
        \State $p_2 = \text{Multiply}(x_R, y_R)$
        \State $p_3 = \text{Multiply}(x_L + x_R, y_L + y_R)$
        \State \Return{$p_1 \cdot 2^{2n} + (p_3 - p_1 - p_2) \cdot 2^n + p_2$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\subsubsection{Mnożenie macierzy}
\begin{itemize}
    \item \textbf{Input}: dwie macierze $A, B$ rozmiaru $n \times n$
    \item \textbf{Output}: macierz $C = A \cdot B$
    \item \textbf{Algorytmy}:
        \begin{enumerate}
            \item Naiwne mnożenie macierzy -- jego złożoność wynosi $\Theta(n^3)$ bo aby policzyć jedną komórkę macierzy $C$ musimy wykonać $n$ mnożeń (i $n-1$ dodawań \footnote{w sumie $n^2$ operacji}), a skoro macierz $C$ ma $n^2$ komórek to złożoność wynosi $\Theta(n^3)$
            \item Algorytm Strassena -- \textbf{D\&C}
                \begin{itemize}
                    \item \textbf{Podejście}: Rozbijamy macierze na 4 równe części
                        \[
                            A = \begin{pmatrix}
                                A_{11} & A_{12} \\
                                A_{21} & A_{22}
                            \end{pmatrix}
                        \]
                        \[
                            B = \begin{pmatrix}
                                B_{11} & B_{12} \\
                                B_{21} & B_{22}
                            \end{pmatrix}
                        \]
                        \[
                            C = \begin{pmatrix}
                                C_{11} & C_{12} \\
                                C_{21} & C_{22}
                            \end{pmatrix}
                        \]
                        gdzie
                        \[
                            C_{11} = A_{11}B_{11} + A_{12}B_{21}
                        \]
                        \[
                            C_{12} = A_{11}B_{12} + A_{12}B_{22}
                        \]
                        \[
                            C_{21} = A_{21}B_{11} + A_{22}B_{21}
                        \]
                        \[
                            C_{22} = A_{21}B_{12} + A_{22}B_{22}
                        \]
                    \item \textbf{Asymptotyka}: Algorytm ten spełnia rekurencje
                        \[
                            T(n) = 7T(\frac{n}{2}) + \Theta(n^2)
                        \]
                        Z \textbf{Master Theorem} otrzymujemy, że algorytm ma złożoność $\Theta(n^{\log_2 7})$, a $\log_2 7 \approx 2.81$
                        \footnote{Aby zejść do rekurencji $T(n) = 7T(\frac{n}{2}) + \Theta(n^2)$ trzeba wykonać pewne, bardziej wyrafinowane triki, które nie są dokładnie opisane tutaj. Z algorytmu zapisanego wyżej wynika że rekurencja to $T(n) = 8T(\frac{n}{2}) + \Theta(n^2)$, a wieć złożoność wynosi $\Theta(n^3)$}
                \end{itemize}
        \end{enumerate}
\end{itemize}

\subsubsection{Quick Sort}
Algortym \textbf{Merge Sort} ociera się o minimalną granicę złożoności sortowania, która wynosi $\Theta(n \log n)$, jednaże jest z nim problem związany z pamięcia: nie sortuje w miejscu, a więc wymaga dodatkowej pamięci.
\begin{itemize}
    \item \textbf{Input}: tablica $A[1..n]$
    \item \textbf{Output}: posortowana tablica $A$
    \item \textbf{Algorytm}: \texttt{QuickSort(A, p, q)}
        \begin{enumerate}
            \item Podziel tablicę \texttt{A[p...q]} na dwie podtablice \texttt{A[p...k-1]} oraz \texttt{A[k+1...q]}, gdzie \texttt{A[k]} jest elementem rozdzielającym -- \textit{pivotem}\footnote{o tym jak ten \textif{pivot} jest wybierany bedziemy mówić poźniej} tak, że:
                \[
                    \forall i \in [p...k-1]: A[i] \leq A[k]: \forall j \in [k+1...q]: A[j] \geq A[k]
                \]
            \item Odpalamy rekurencyjnie \texttt{QuickSort(A, p, k-1)} oraz \texttt{QuickSort(A, k+1, q)}
        \end{enumerate}
    \item \textbf{Przyklad}:
        \begin{enumerate}
            \item mamy dane $A = [6,1,4,3,5,7,2,8]$, wybieramy \textit{pivot} jako $6$
            \item po pierwszym kroku mamy $A = [1,4,3,5,2,6,7,8]$ odplamay rekurencyjnie \texttt{QuickSort(A, 1, 5)} oraz \texttt{QuickSort(A, 7, 8)}
            \item otrzymujemy %TODO kurwaaaaaaaaaaaa

        \end{enumerate}
\end{itemize}

\section{Wykład \date{2025-03-24}}
\subsection{Quick Sort}
\subsubsection{Lemuto Partition}
\begin{itemize}
    \item \textbf{Input}: tablica $A[1..n]$
    \item \textbf{Output}: posortowana tablica $A$
    \item \textbf{Algorytm}: \texttt{Lemuto(A, p, q)}
        \begin{algorithm}
            \caption{Lemuto Partition}
            \begin{algorithmic}[1]
                \Procedure{Lemuto}{A, p, q}
                \State $\text{pivot} = A[p]$
                \State $i = p$
                \For{$j = p+1$ to $q$}
                \If{$A[j] < \text{pivot}$}
                \State $i = i + 1$
                \State \text{swap} $A[i] \leftrightarrow A[j]$
                \EndIf
                \EndFor
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item \textbf{Przykład:}
        \begin{enumerate}
            \item zaczynamy z nieposotowaną tablicą $A = [6,10,13,5,8,3,2,11]$
                %TODO dokonczyć przykład? może od szymona
        \end{enumerate}
    \item \textbf{Asymptotyka}:
        Algorytm ten wykonuje w głownej pętli $n-1$ porównań, natomiast wersja Lemuto Partition wymaga dodatkowo $n-1$ zamian elementów.

\end{itemize}

\subsubsection{Hoare Partition}
\begin{itemize}
    \item \textbf{Input}: Tablica $A[1..n]$
    \item \textbf{Output}: Posortowana Tablica $A$
    \item \textbf{Algorytm}: \texttt{Hoare(A, p, q)}
        \begin{algorithm}
            \caption{Hoare Partition}
            \begin{algorithmic}[1]
                \Procedure{Hoare}{A, p, q}
                \State $\text{pivot} = A[\frac{p+q}{2}]$
                \State $i = p-1$
                \State $j = q+1$
                \While{True}
                \State $i = i + 1$
                \While{$A[j] > \text{pivot}$}
                \State $j = j - 1$
                \If{$\geq j$}
                \State \textbf{break}
                \EndIf
                \EndWhile
                \State $\text{swap} A[i] \leftrightarrow A[j]$
                \EndWhile
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item \textbf{Przykład:}
        Generalnie algorytm ten działa na zasadzie zamiany elementów w tablicy względem \textit{pivotu} tak, że jeżeli jest element mniejszy od \textit{pivotu} to zamieniamy go z elementem większym od \textit{pivotu} z drugiej strony tablicy. Algorytm kończy się gdy wszytkie elementy mniejsze od \textit{pivotu} są po lewej stronie, a większe po prawej.
        \begin{enumerate}
            \item zaczynamy z nieposotowaną tablicą $A = [6,10,13,5,8,3,2,11]$
                %TODO dokonczyć przykład? może od szymona
        \end{enumerate}
    \item \textbf{Asymptotyka}:
        \textit{Hoare Partition} wykonue $n\pm c$ porównań -- o stałą więcej niżeli \textit{Lemuto Partition}, ale za to wykonuje mniej zamian elementów. W praktyce \textit{Hoare Partition} jest szybszy. Całościowa Asymptotyka wynosi $\Theta(n)$
\end{itemize}

\subsubsection{Analiza Worst Case}
Algorytm sortowania Quick Sort zachowuje się najgorzej w przypadku, gdy dostaje tablicę odwrotnie posortowaną. Wszystkie elementy będą znajdowały się po złej stronie \textit{pivotu}. \\
Zostaje spełniana rekurencja:
\[
    T(n) = T(n-1) + \underbrace{T(0)}_{\text{pusta lewa tablica}} + \Theta(n)
\]
Można zauważyć, że nie zadziała tu \textbf{Master Theorem}, trzeba rozwiązać ja na przykład drzewem rekursji:
\begin{center} %TODO od szymona. napisać ze wysokośc to n -1 itp
\begin{tikzpicture}
    [level distance=1.5cm,
    level 1/.style={sibling distance=4cm},
    level 2/.style={sibling distance=2cm}]
    \node {$cn$}
        child {node {$c(n-1)$}
            child {node {$1$}} %tutaj
            child {node {$\Theta(1)$}}
        }
        child {node {$\Theta(1)$}};
  \end{tikzpicture}
\end{center}
Z drzewa rekursji wynika, że powyższa rekurencja to:
\[%TODO to też do przepisania
    T(n) \leq \sum^{n}_{i=0} i + \Theta(n) = O(n^2)
\]

\subsubsection{Best Case Analysis}
Algorytm sortowania Quick Sort zachowuje się najlepiej w przypadku, gdy dostaje tablicę posortowaną. Wszystkie elementy będą znajdowały się po dobrej stronie \textit{pivotu}. \\
Zostaje spełniana rekurencja:
\[
    T(n) = T(\frac{n}{2}) +T(\frac{n}{2}) +\Theta(n)
\]
Można zauważyć, z \textbf{Master Theorem}, że asymptotyka wynosi:
\[
    T(n) = \Theta(n \log n)
\]
%TODO drzewo

\subsubsection{Rozważenie przypadku mieszanego}
Rozważmy przypadek, w którym algorytm raz wykonuje się z best casem -- dzieli się tablica na pół, a raz z worst casem -- dzieli się tablica na 1 i $n-1$ elementów. \\
Zostaje spełniana rekurencja:
\[
    L(n) = 2U(\frac{n}{2}) + \Theta(n)
\]
\[
    U(n) = L(n-1) + \Theta(n)
\]
gdzie $L$ symbolizuje best case, natomiast $U$ worst case. Rozwiązując powyższą rekurencje otrzymujemy:
\begin{equation*} %TODO upewnic sie, ze to dobrze
    \begin{aligned}
        L(n)
        &= 2(L(\frac{n}{2} -1) +\Theta(n)) + \Theta(n) \\
        &= 2L(\frac{n}{2} -1) + \Theta(n) \\
        &= \Theta(n \log n)
    \end{aligned}
\end{equation*}

\subsubsection{Average Case Analysis}
Algorytm Quick Sort da się ``zabezpieczyć'' przed złym rozkładem danych poprzez losowym wybraniem pivota i następnie swapnięcie go z naszym deterministycznym miejscem. W ten sposób bedzięmy mieli zawsze jednostajnie losowy rozkład danych.\\
Wprowadźmy
\[
    T_n  \text{ -- zmienna losowa liczby porównań w Quick Sorcie sortowanej tablicy } A, |A|=n
\]
Do dziś nie jest znany rozkład zmiennej losowej $T_n$.
Niech
%TODO
%\begin{equation*}
%    X^{(n)}_{k} =
%    \begin{cases}
%        1 \text{ jeśli partition podzieli tablice n-elementową na (k,(n-k-1)) \\
%        0 \text{w przeciwnym przypadku}
%    \end{cases}
%\end{equation*}
%TODO Rownanie od szymonaaaaaa. w rownaniu z wartoscia oczekiwana zapisac ze X i T+T sa niezalezne




\section{Ćwiczenia}
tu beda pojawialy sie notatki z cwiczen do przedmiotu Algorytmy i struktury danych na Politechnice Wrocławskiej na kierunku Informatyka Algorytmiczna rok 2025 semestr letni.

\subsection{Lista 2}
robiona na zajęciach \date{2025-03-10}
\subsubsection{zadanie 1}
Wylicz ile linijek wypisze poniższy program (podaj wynik będacy funkcją od n w postaci asymptotycznej $\Theta(\cdot)$). Można założyć, że $n$ jest potęgą liczby $3$.
\begin{algorithm}
\begin{algorithmic}[1]
\State \textbf{function} f(n)
\If{$n > 1$}
    \State print\_line('still going')
    \State f(n/3)
    \State f(n/3)
\EndIf
\end{algorithmic}
\end{algorithm}
w pseudo kodzie pojawia sie nastepujaca rekurencja:
\[
    T(n) = 2T(\frac{n}{3}) + 1
\]
rozwiąże ją używając metody podstawienia. Niech $n=3^k, k = \log_3 n$, wtedy:
\[
    T(3^k) = 2T(3^{k-1}) + 1
\]
Zatem przyjmując $S(k) = T(3^k)$ mamy:
\[
    S(k) = 2S(k-1) + 1
\]
rozwiązując rekurencję otrzymujemy:
\[
    S(k) = 2^k - 1
\]
zatem
\[
    T(n) = 2^{\log_3 n} - 1 = n^{\log_3 2} - 1 = \Theta(n^{\log_3 2})
\]
analogicznie liczmy jaka jest wykonana ``praca'' wykonana przez program w drzweie rekursji.

\subsubsection{zadanie 2}
Niech $f(n)$ i $g(n)$ będą funkcjami asymptotycznie nieujemnymi (tzn. nieujemnymi dla dostatecznie dużego $n$). Korzystając z definicji notacji $\Theta$, udowodnij, że:
\[
\max\{f(n), g(n)\} = \Theta(f(n) + g(n)).
\]
\begin{proof}
    Z definicji notacji $\Theta$ mamy:
    \[
        f(n)=\Theta(g(n)) \iff \exists c_1, c_2 > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n)
    \]
    skoro $f(n)$ i $g(n)$ są asymptotycznie nieujemne to:
    \[
        \exists n_f: \forall n \geq n_f, f(n) \geq 0
    \]
    \[
        \exists n_g: \forall n \geq n_g, g(n) \geq 0
    \]
    zatem
    \[
        n_0=\max\{n_f, n_g\}
    \]
    a więc
    \[
        f(n) \leq \max\{f(n), g(n)\}
    \]
    \[
        g(n) \leq \max\{f(n), g(n)\}
    \]
    dodając obie nierówności otrzymujemy:
    \[
        f(n) + g(n) \leq 2 \cdot \max\{f(n), g(n)\}
    \]
    zatem
    \[
        \forall n \geq n_0: \max\{f(n), g(n)\} \leq f(n) + g(n) \leq 2 \cdot \max\{f(n), g(n)\}
    \]
    a więc z definicji mamy
    \[
        \max\{f(n), g(n)\} = \Theta(f(n) + g(n))
    \]
\end{proof}

\subsubsection{zadanie 3}
Wylicz asymptotyczną złożoność (używając notacji $\Theta$) poniższych fragmentów programów:

\begin{algorithm}
\caption{Pierwszy fragment kodu}
\begin{algorithmic}[1]
\For{$i = 1$ to $n$}
    \State $j = i$
    \While{$j < n$}
        \State $sum = P(i, j)$
        \State $j = j + 1$
    \EndWhile
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Drugi fragment kodu}
\begin{algorithmic}[1]
\For{$i = 1$ to $n$}
    \State $j = i$
    \While{$j < n$}
        \State $sum = R(i, j)$
        \State $j = j + j$
    \EndWhile
\EndFor
\end{algorithmic}
\end{algorithm}
Gdzie:
\begin{itemize}
    \item koszt wykonania procedury $P(i,j)$ wynosi $\Theta(1)$,
    \item koszt wykonania procedury $R(i,j)$ wynosi $\Theta(j)$.
\end{itemize}

\begin{proof}
    \begin{itemize}
        \item Pierwszy fragment kodu
            \begin{itemize}
                \item Wewnętrzna pętla wykonuje się $n-i$ razy
                \item Koszt wykonania procedury $P(i,j)$ wynosi $\Theta(1)$
                \item Zatem koszt wykonania wewnętrznej pętli wynosi $\Theta(n-i)$
                \item Zatem koszt wykonania całego fragmentu wynosi
                    \[
                        \sum_{i=1}^{n} \Theta(n-i) = \Theta(n^2)
                    \]
            \end{itemize}
        \item Drugi fragment kodu
            \begin{itemize}
                \item Wewnętrzna pętla wykonuje się $\log_2 n$ razy
                \item Koszt wykonania procedury $R(i,j)$ wynosi $\Theta(j)$
                \item Zatem koszt wykonania wewnętrznej pętli wynosi $\Theta(\log_2 n)$
                \item Zatem koszt wykonania całego fragmentu wynosi
                    \[
                        \sum_{i=1}^{n} \Theta(\log_2 n) = \Theta(n \log_2 n)
                    \]
            \end{itemize}
    \end{itemize}
\end{proof}
Dla pewnosci sprawdzone empirycznie:
%zalacz wykres wykresAlgoZad3.png
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{wykresAlgoZad3.png}
\end{figure}

\subsubsection{zadanie 4}
Wyznacz asymptotyczne oszacowanie górne dla następujących rekurencji:

\begin{itemize}
    \item $T(n) = 2T(n/2) + 1$
    \item $T(n) = 2T(n/2) + n$
    \item $T(n) = 3T(n/2) + n \log n$
\end{itemize}

\bigskip
\hrule
\bigskip

Kożystając z \textbf{Master Theorem} możemy wyznaczyć ograniczenie dla tych rekurencji.
\begin{itemize}
    \item $T(n) = 2T(n/2) + 1$
        \begin{proof}
            \[
                a = 2, b = 2, d = 0
            \]
            \[
                \log_b a = \log_2 2 = 1 > 0 = d
            \]
            \[
                T(n) = \Theta(n)
            \]
        \end{proof}
    \item $T(n) = 2T(n/2) + n$
        \begin{proof}
            \[
                a = 2, b = 2, d = 1
            \]
            \[
                \log_b a = \log_2 2 = 1 = d
            \]
            \[
                T(n) = \Theta(n \log n)
            \]
        \end{proof}
    \item $T(n) = 3T(n/2) + n \log n$
        \begin{proof}
            \text{Dolne ograniczenie}
            \[
                T(n) = 3T(n/2) + n \implies^{\text{Master Theorem}} T(n) = \Theta(n^{\log_2 3})
            \]
            \text{Górne ograniczenie}
            \[
                T(n) = 3T(n/2) + n^{1.1} \implies^{\text{Master Theorem}} T(n) = \Theta(n^{1.1})
            \]
        \end{proof}
\end{itemize}

\subsubsection{zadanie 5}
Zaprojektuj algorytm wczytujący z wejścia tablicę liczb $A[1], \ldots, A[N]$ i przygotowujący tablicę $B$ tak, że na jej podstawie będzie potrafił odpowiadać na pytania:
\begin{enumerate}
    \item ile wynosi suma elementów tablicy $A$ od miejsca $i$ do miejsca $j$ włącznie, dla $i < j$.
    \item Jaka jest złożoność czasowa Twojego algorytmu? Ile pamięci zajmuje tablica $B$?
    \item Ile zajmuje odpowiedź na jedno pytanie?
\end{enumerate}

\bigskip
\hrule
\bigskip

Przykładowy algorytm mógłby wyglądać następująco:
\begin{algorithm}
    \caption{Algorytm do zadania 5.}
    \begin{algorithmic}[1]
        \State $B[1] = A[1]$
        \For{$i = 2$ to $N$}
            \State $B[i] = B[i-1] + A[i]$
        \EndFor
        \Procedure{Sum}{i, j} %uwaga na edge case
        \If {$i=1$}
            \State \Return $B[j]$
        \Else
            \State \Return $B[j] - B[i-1]$
        \EndIf
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

Co tu się dzieje?
\begin{itemize}
    \item W pierwszej pętli obliczamy sumy prefiksowe tablicy $A$ i zapisujemy je w tablicy $B$.
    \item W procedurze \texttt{Sum} zwracamy różnicę między dwoma elementami tablicy $B$.
\end{itemize}

\begin{itemize}
    \item Złożoność czasowa algorytmu wynosi $\Theta(n)$.
    \item Tablica $B$ zajmuje $\Theta(n)$ pamięci.
    \item Odpowiedź na jedno pytanie zajmuje $\Theta(1)$ czasu.
\end{itemize}

\subsubsection{zadanie 6}
Pokaż, jak grać w grę w "10 pytań", w której wiadomo, że wybrana liczba jest dodatnia, ale nie jest na początku znane górne ograniczenie jej wartości. Ile pytań potrzebujesz, żeby zgadnąć dowolną liczbę (liczba pytań może zależeć od wielkości liczby)?

\bigskip
\hrule
\bigskip

W grze "10 pytań" możemy zadać pytania w stylu "czy liczba jest większa od $x$?". W ten sposób możemy zredukować przestrzeń poszukiwań. W pierwszym pytaniu zapytajmy, czy liczba jest większa od $1$. Jeśli tak, to zapytajmy, czy liczba jest większa od $2$. W ten sposób możemy zredukować przestrzeń poszukiwań do $2^k$ dla pewnego $k$. W ten sposób możemy znaleźć dowolną liczbę w $k$ pytaniach.
\begin{algorithm}
    \caption{Algorytm do zadania 6.}
    \begin{algorithmic}[1]
        \State{$k = 0$}
        \While{$2^k < x$}
            \State{$k = k + 1$}
        \EndWhile
        \State{$p=2^{k-1}$}
        \State{$q=2^k$}
        \Procedure{BinarySearch}{p, q}
    \end{algorithmic}
\end{algorithm}


\subsubsection{zadanie 7}
Używając algorytmu \textbf{divide-and-conquer} do mnożenia liczb wykonaj mnożenie dwóch liczb binarnych 11011, 1010.

\bigskip
\hrule
\bigskip

Algorytm \textbf{divide-and-conquer} do mnożenia liczb działa w następujący sposób:
\begin{enumerate}
    \item Podziel liczby na dwie równe części.
    \item Rekurencyjnie pomnóż te części.
    \item Połącz wyniki.
\end{enumerate}

Mnożenie dwóch liczb binarnych $11011$ i $1010$ możemy zrealizować w następujący sposób:
\begin{enumerate}
    \item Podziel liczby na dwie równe części: $1101$, $1$ oraz $10$, $10$.
    \item Rekurencyjnie pomnóż te części: $1101 \cdot 10 = 11010$.
    \item Połącz wyniki: $11010 + 110100 = 1000000$.
\end{enumerate}

\begin{algorithm}
\caption{Algorytm do zadania 7 (pokazany na wykładzie)}
    \begin{algorithmic}[1]
        \Procedure{Mul}{x, y}
        \State{$n = \max\{|x|, |y|\}$}
        \If{$n = 1$}
        \State \Return{$x \cdot y$}
        \EndIf
        \State{$x_L, x_R = \text{LetfMost}\left\lceil\frac{n}{2}\right\rceil, \text{RightMost}\left\lceil\frac{n}{2}\right\rceil \text{bits of $x$}$}
        \State{$y_L, y_R = \text{LetfMost}\left\lceil\frac{n}{2}\right\rceil, \text{RightMost}\left\lceil\frac{n}{2}\right\rceil \text{bits of $y$}$}
        \State{$p_1 = \text{Mul}(x_L, y_L)$}
        \State{$p_2 = \text{Mul}(x_R, y_R)$}
        \State{$p_3 = \text{Mul}(x_L + x_R, y_L + y_R)$}
        \State \Return{$p_1 \cdot 2^{2n} + (p_3 - p_1 - p_2) \cdot 2^n + p_2$}
    \end{algorithmic}
\end{algorithm}

\subsection{Lista 3}
robiona na zajęciach \date{2025-03-24}

\subsubsection{zadanie 1}
Podaj algorytm scalający $k$ posortowanych list tak aby powstała jedna posortowana lista $nb$ (liczba wszystkich elementów na listach to n) działający w czasie $O(n \log k)$.

\bigskip
\hrule
\bigskip

Algorytm ten można zrealizować w następujący sposób:
\begin{algorithm}
    \caption{Algorytm do zadania 1.}
    \begin{algorithmic}[1]
        \Procedure{MergeLists}{$L_1, L_2, \ldots, L_k$}
        \State{$n = \sum_{i=1}^{k} |L_i|$}
        \State{$B = \text{tablica} [1 \ldots n]$}
        \State{$\text{heap} = \text{MinHeap}$}
        \For{$i = 1$ to $k$}
            \State{$\text{heap}.\text{insert}(L_i.\text{pop}())$}
        \EndFor
        \For{$i = 1$ to $n$}
            \State{$B[i] = \text{heap}.\text{pop}()$}
            \State{$\text{heap}.\text{insert}(L_i.\text{pop}())$}
        \EndFor
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\subsubsection{zadanie 2}
Zdefiniujmy algorytm \textit{k-MergeSort} jako uogólnienie algorytmu sortowania przez scalanie. Różni się od omawianego na wykładzie algorytmu sortowania przez scalanie tym, że dzieli sortowana tablice rekurencyjnie na k równych części (zakładamy, że liczba elementów w tablicy jest potęgą k $(n = k^l)$). \\
Używając wyniku z zadania 1 proszę wykazać dla jakiego k algorytm ma najmniejsza asymptotyczną złożoność obliczeniową liczby porównań (górne ograniczenie $O(\cdot)$).

\bigskip
\hrule
\bigskip

Algorytm \textit{k-MergeSort} spełnia rekurencję:
\[
    T(n) = kT(\frac{n}{k}) + \Theta(n \log k)
\]
gdzie $\Theta(n \log k)$ to koszt scalania $k$ posortowanych list (zadanie 1). Z \textbf{Master Theorem} otrzymujemy, że algorytm ma złożoność:
\[
    T(n) = \Theta(n \log_k n)
\]

\end{document}
