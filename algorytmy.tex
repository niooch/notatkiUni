\documentclass[11pt,a4paper]{article}
% Kodowanie i obsługa języka polskiego
\usepackage[utf8]{inputenc}      % Kodowanie wejścia
\usepackage[T1]{fontenc}         % Kodowanie fontów
\usepackage[polish]{babel}       % Obsługa języka polskiego

% Pakiety matematyczne i inne przydatne
\usepackage{mathtools}
\usepackage{tabularx}
\usepackage{array} % for centering in p-columns
\usepackage{calc}
\usepackage[makeroom]{cancel}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{marginnote}
\usepackage{tikz}
\usetikzlibrary{trees}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{amsmath, amssymb, amsthm}  % Pakiety do matematyki
\usepackage{graphicx}                  % Obsługa grafiki
\usepackage{hyperref}                  % Linki i spis treści
\usepackage{geometry}                  % Ustawienie marginesów
\usepackage{algorithm}               % Algorytmy
\usepackage{algpseudocode}           % Pseudokod
\usetikzlibrary{positioning, shapes.geometric, arrows.meta}
\geometry{margin=2cm}                  % Ustawienie marginesów na 2 cm
\tikzset{
  rednode/.style={
    ellipse, draw=red!80!black, fill=red!50,
    text=white, font=\footnotesize\bfseries, minimum width=14mm, minimum height=8mm
  },
  blacknode/.style={
    rectangle, draw=black, fill=black!70, rounded corners=2mm,
    text=white, font=\footnotesize\bfseries, minimum width=16mm, minimum height=8mm
  },
  edgestyle/.style={-Stealth, thick, gray!70},
}

% Dodatkowe ustawienia
\newlength{\exprwidth}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\title{Notatki z Algorytmów i Struktur Danych}
\author{Jakub Kogut}

\begin{document}

\maketitle

\tableofcontents  % Spis treści (opcjonalnie)
\newpage

\section{Wstęp}
To będa notatki z przedmiotu Algorytmy i struktury danych na Politechnice Wrocławskiej na kierunku Informatyka Algorytmiczna rok 2025 semestr letni.
\subsection{Informacje}
Prowadzący Przedmiot: \textbf{Zbychu Gołębiewski}
\begin{itemize}
    \item Należy kontaktować się przez maila: \href{mailto:zbigniew.golebiewski@pwr.edu.pl}{mail}
    \item Konsultacje \textbf{216/D1}:
        \begin{itemize}
            \item Wtorek 13:00-15:00
            \item Środa 9:00-11:00
        \end{itemize}
    \item Wiecej info na stronie \href{https://cs.pwr.edu.pl/golebiewski/teaching/aisd.php}{przedmiotu}
    \item Literatura
        \begin{itemize}
            \item Algorithms, Dasgupta, Papadimitriou, Vazirani
            \item Algorithms, Sedgewick, Wayne (strona internetowa książki)
            \item Algorithms Designs, Jon Kleinberg and Eva Trados
            \item Wprowadzenie do algorytmów, Cormen, Leiserson, Rivest, Stein
            \item Sztuka programowania (wszystkie tomy), Donald E. Knuth
        \end{itemize}
\end{itemize}
\subsection{Ocenianie}
Ocena z kursu składa się z:
\begin{itemize}
    \item Oceny z egzaminu -- E
    \item Oceny z ćwiczeń -- C
    \item Oceny z laboratorium -- L
\end{itemize}
Wszystkie oceny są z zakresu $[0,100]$. Ocena końcowa jest wyliczana ze wzoru:
\[
    K = \frac{1}{2}E + \frac{1}{4}C + \frac{1}{4}L
\]

\section{Wykład \date{2025-03-03}}
\subsection{Przykładowy Problem}
Sortowanie:
\begin{itemize}
    \item Input: $n$ liczb $a_1, a_2, \ldots, a_n, |A|$, gdzie $|A|$ to długośc tablicy
    \item Output: permutacja $a_1', a_2', \ldots, a_n'$ taka, że $a_1' \leq a_2' \leq \ldots \leq a_n'$
\end{itemize}
Najważniejsze w algorytmach jest to, żeby były POPRAWNE: edge case, ...

\subsection{Jak mierzyć złożoność algorytmów}
\begin{enumerate}
    \item Worst Case Analysis T(n) $\leftarrow$ stosowane najcześciej
    \item Average Case Analysis
        \begin{itemize}
            \item zakładamy pewnien rozkład prawdopodobieństwa na danych wejściowych
            \item $T$ -- zmienna losowa liczby operacji wykonanych przez algorytm
                \[
                    T(n) = \max\{\# \text{operacji dla danego wejścia}\}
                \]
            \item $E[T]$ -- wartość oczekiwana $T$ $\rightarrow$ średnia liczba operacji, to co nas interesuje
        \end{itemize}
\end{enumerate}
\subsection{Przykład algorytmu}
W tej sekcji mamy pokazany przykład jak pisać pseudo kod:
\begin{algorithm}[H]
    \caption{Merge Sort}\label{alg:merge_sort}
    \begin{algorithmic}[1]
    \Procedure{MergeSort}{A, 1, n}
        \If{|A[1..n]| == 1}
            \State \Return{A[1..n]}
        \Else
            \State $B = \text{MergeSort}(A, 1, \lfloor n/2 \rfloor)$
            \State $C = \text{MergeSort}(A, \lfloor n/2 \rfloor, n)$
            \State \Return{Merge(B, C)}
        \EndIf
    \EndProcedure
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
    \caption{Merge}\label{alg:merge}
    \begin{algorithmic}[1]
    \Procedure{Merge}{X[1..k], Y[1..n]}
        \If{$X = \emptyset$}
            \State \Return{$Y$}
        \ElsIf{$Y = \emptyset$}
            \State \Return{$X$}
        \ElsIf{$X[1] \leq Y[1]$}
            \State \Return{$[X[1]] \times \text{Merge}(X[2..k], Y[1..n])$}
        \Else
            \State \Return{$[Y[1]] \times \text{Merge}(X[1..k], Y[2..n])$}
        \EndIf
    \EndProcedure
    \end{algorithmic}
\end{algorithm}
\subsection{Przykład działania Merge Sort}
\textbf{Example: Sorting the array \([10,\, 2,\, 5,\, 3,\, 7,\, 13,\, 1,\, 6]\) step by step}

\begin{enumerate}
  \item \textbf{Initial split:}
  \[
    [\,10,\, 2,\, 5,\, 3,\, 7,\, 13,\, 1,\, 6\,]
    \quad\longrightarrow\quad
    [\,10,\, 2,\, 5,\, 3\,] \quad\text{and}\quad [\,7,\, 13,\, 1,\, 6\,].
  \]

  \item \textbf{Sort the left half \([10,\, 2,\, 5,\, 3]\):}
  \begin{enumerate}
    \item Split into \([10,\, 2]\) and \([5,\, 3]\).
    \item \(\text{MergeSort}([10,\, 2])\):
    \begin{itemize}
      \item Split into \([10]\) and \([2]\).
      \item Each is already sorted (single element).
      \item Merge: \([2,\, 10]\).
    \end{itemize}
    \item \(\text{MergeSort}([5,\, 3])\):
    \begin{itemize}
      \item Split into \([5]\) and \([3]\).
      \item Each is already sorted.
      \item Merge: \([3,\, 5]\).
    \end{itemize}
    \item Merge \([2,\, 10]\) and \([3,\, 5]\) to get \([2,\, 3,\, 5,\, 10]\).
  \end{enumerate}

  \item \textbf{Sort the right half \([7,\, 13,\, 1,\, 6]\):}
  \begin{enumerate}
    \item Split into \([7,\, 13]\) and \([1,\, 6]\).
    \item \(\text{MergeSort}([7,\, 13])\):
    \begin{itemize}
      \item Split into \([7]\) and \([13]\).
      \item Each is already sorted.
      \item Merge: \([7,\, 13]\).
    \end{itemize}
    \item \(\text{MergeSort}([1,\, 6])\):
    \begin{itemize}
      \item Split into \([1]\) and \([6]\).
      \item Each is already sorted.
      \item Merge: \([1,\, 6]\).
    \end{itemize}
    \item Merge \([7,\, 13]\) and \([1,\, 6]\) to get \([1,\, 6,\, 7,\, 13]\).
  \end{enumerate}

  \item \textbf{Final merge:} Merge the two sorted halves:
  \[
    [\,2,\, 3,\, 5,\, 10\,] \quad\text{and}\quad [\,1,\, 6,\, 7,\, 13\,]
    \quad\longrightarrow\quad [\,1,\, 2,\, 3,\, 5,\, 6,\, 7,\, 10,\, 13\,].
  \]
\end{enumerate}

\noindent
Hence, after all the recursive splits and merges, the final sorted array is:
\[
[\,1,\, 2,\, 3,\, 5,\, 6,\, 7,\, 10,\, 13\,].
\]

\subsection{Złożoność Merge Sort}
\begin{itemize}
    \item Złożoność czasowa
        \begin{itemize}
            \item $T(n) = 2T(n/2) + \Theta(n)$
            \item $T(n) = \Theta(n \log n)$
        \end{itemize}
    \item Złożoność pamięciowa
        \begin{itemize}
            \item $M(n) = n + M(n/2)$
            \item $M(n) = \Theta(n)$
        \end{itemize}
\end{itemize}

\section{Wykład \date{2025-03-10}}
\subsection{Notacja Asypmtotyczna}
Na wykładzie będziemy omawiali:
\begin{itemize}
    \item Notację dużego O $O(n)$ //ograniczenie górne
        \begin{itemize}
                \item Definicja $O(n)$:
                \[
                    O(g(n)) = \{ f(n) \mid \exists c > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq f(n) \leq c \cdot g(n) \}
                \]
            \item Uwaga! \newline
                Jeśli
                \[
                    \limsup_{n \to \infty} \frac{f(n)}{g(n)} < \infty
                \]
                to
                \[
                    \limsup_{n \to \infty} \frac{f(n)}{g(n)} = \lim_{n \to \infty} \frac{f(n)}{g(n)}
                \]
            \item Przykład:
                \begin{itemize}
                    \item $2n^2=O(n^3)$
                        dla $n_0 = 2, c = 1$ Definicja jest spełniona
                    \item $f(n) = n^3 + O(n^2)$ jest to jeden z sposobów użycia $O(n)$
                        \[
                            \exists h(n) = O(n^2) \quad \text{takie, że} \quad f(n) = n^3 + h(n)
                        \]
                \end{itemize}
        \end{itemize}
    \item Notację omega //ograniczenie dolne
        \begin{itemize}
            \item Definicja
                \[
                    \Omega(g(n)) = \{ f(n) \mid \exists c > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq c \cdot g(n) \leq f(n) \}
                \]
            \item Przykład
                \begin{itemize}
                    \item $n^3 = \Omega(2n^2)$
                    \item $n = \Omega(\log n)$
                \end{itemize}
        \end{itemize}
    \item Notację theta $\theta(n)$ //ograniczenie z dwóch stron
        \begin{itemize}
            \item Definicja
                \[
                    \Theta(g(n)) = \{ f(n) \mid \exists c_1, c_2 > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n) \}
                \]
            \item Przykład
                \begin{itemize}
                    \item $n^3 = \Theta(n^3)$
                    \item $n^3 = \Theta(n^3 + 2n^2)$
                    \item $log n +8 + \frac{1}{12n} = \Theta(\log n)$
                \end{itemize}
            \item Uwaga!
                \[
                    f(n) = \Theta(g(n)) \iff f(n) = O(g(n)) \land f(n) = \Omega(g(n))
                \]
                Można to zapisać jako klasy funkcji:
                \[
                    \Theta(g(n)) = O(g(n)) \cap \Omega(g(n))
                \]
        \end{itemize}
    \item Patologiczny przykład:
        mamy funkcje $g(n) = n$ oraz $f(n) = n^{1+sin{\frac{\pi n}{2}}}$, a więc
        \[
            f(n) = \begin{cases}
                n^2 & \text{dla n parzystych} \\
                n & \text{dla n nieparzystych}
            \end{cases}
        \]
        wtedy
        \[
            \limsup_{n \to \infty} \frac{f(n)}{g(n)} = \infty
        \]
        \[
            \limsup_{n \to \infty} \frac{g(n)}{f(n)} = \infty
        \]
        zatem
        $f \neq O(g)$ oraz $g \neq O(f)$
    \item o małe
        \begin{itemize}
            \item Definicja
                \[
                    o(g(n)) = \{ f(n) \mid \forall c > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq f(n) < c \cdot g(n) \}
                \]
                Równoważnie
                \[
                    lim_{n \to \infty} \frac{f(n)}{g(n)} = 0
                \]
            \item Przykład
                \begin{itemize}
                    \item $n^2 = o(n^3)$ i $n^2 O(n^3)$ ale $n^2 \neq o(n^2)$
                    \item $n = o(n^2)$
                \end{itemize}
        \end{itemize}
\end{itemize}

\subsection{Rekurencja}
\begin{itemize}
    \item Metoda podstawienia (metoda dowodu indukcyjnego)
        \begin{enumerate}
        \item Zadnij Odpowiedź (bez stałych)
        \item Sprawdź przez indukcję czy odpowiedź jest poprawna
        \item Wylicz stałe
        \end{enumerate}
        \begin{itemize}
            \item Przykład
                \begin{itemize}
                    \item $T(n) = T(\frac{n}{2}) + n$
                    \item Pierwotny strzał: $T(n) = O(n^3)$
                    \item cel: Pokazać, że $\exists c>0: T(n) \leq c \cdot n^3$
                        \begin{itemize}
                            \item warunek początowy: $T(1) = 1 \leq c$
                            \item krok indukcyjny: załóżmy, że $\forall k \leq n: T(k) \leq ck^3$
                        \end{itemize}
                        \[
                            T(n) = 4T(\frac{n}{2}) + n \leq 4c(\frac{n}{2})^3 + n = \frac{1}{2}cn^3 + n \leq cn^3 \quad \text{dla} \quad c \geq 2
                        \]
                        jednakże ``Przestrzeliliśmy'' znacznie, spróbojmy wzmocnić założenie indukcyjne:
                        \[
                            T(n) \leq c_1k^2 -c_2k, k < n
                        \]
                        wtedy mamy:
                        \[
                            T(n) = 4T(\frac{n}{2}) +n \leq 4(c_1(\frac{n}{2})^2 - c_2(\frac{n}{2})) + n = c_1n^2 - 2c_2n + n \leq c_1n^2 - c_2n
                        \]
                        zatem $c_1 = 1, c_2 = 1$ i $T(n) = O(n^2)$ \qed
                \end{itemize}
            \item Przykład
                \begin{itemize}
                    \item $T(n) = 2T(\sqrt{n}) + \log n$
                        \newline
                        załóżmy, że $n$ jest potęgą liczby $2$, czyli $n = 2^m$
                        \[
                            T(2^m) = 2T(2^{\frac{m}{2}}) + m
                        \]
                        Co implikuje
                        \[
                            T(2^\frac{m}{2}) \rightarrow S(m)
                        \]
                        wtedy
                        \[
                            S(m) = 2S(\frac{m}{2}) + m
                        \]
                        rozwiązując rekurencję otrzymujemy
                        \[
                            S(m) = m \log m
                        \]
                        zatem
                        \[
                            T(n) = \log n \log \log n
                        \]
                \end{itemize}
        \end{itemize}

\end{itemize}
\section{Wykład \date{2025-03-17}}
\subsection{Drzewo rekursji}
Przykład dzewa rekursji:
\begin{itemize}
    \item $T(n) = T(\frac{n}{2})+T(\frac{n}{4}) + n^2$
\end{itemize}
\begin{center}
\begin{tikzpicture}
    [level distance=1.5cm,
    level 1/.style={sibling distance=4cm},
    level 2/.style={sibling distance=2cm}]
    \node {$n^2$}
        child {node {$\frac{n^2}{4}$}
            child {node {$\frac{n^2}{16}$}}
            child {node {$\frac{n^2}{64}$}}
        }
        child {node {$\frac{n^2}{16}$}
            child {node {$\frac{n^2}{64}$}}
            child {node {$\frac{n^2}{256}$}}
        };
      \node[draw=none] at (-4.5,0) {$n^2$};
      \node[draw=none] at (-4.5,-1.5) {$\frac{5}{16}n^2$};
      \node[draw=none] at (-4.5,-3) {$\frac{25}{256}n^2$};
\end{tikzpicture}
\end{center}
\subsubsection*{Uwaga!}
Nie jest to formalne rozwiązanie problemu. Nie można używać drzewa rekursji do dowodzenia złożoności algorytmów. Jest to jedynie intuicyjne podejście do problemu. Trzeba policzyć to na piechote, aby było formalnie.\newline
Aby policzyć $T(n)$ musimy policzyć sumę wszystkich wierzchołków w drzewie rekursji.
\[
    T(n) = \sum^{\infty}_{k=0} \left(\frac{5}{16}\right)^k \cdot n^2 = n^2 \sum^{\infty}_{k=0} \left(\frac{5}{16}\right)^k = n^2 \frac{1}{1-\frac{5}{16}} = n^2 \frac{16}{11} = \frac{16}{11}n^2
\]
A wiec $T(n) = O(n^2)$
\newline
Możemy to policzyć dokładniej dostajac mniejsze wyrazy w sumie.
\[
    T(n) = O(\hat{T}(n)) = O(\check{T}(n))
\]
\[
    T(n) = \Omega(\check{T}(n))
\]
\[
    T(n) = \Theta(n^2) = \frac{16}{11}n^2 + o(n^2)
\]

\subsection{Metoda iteracyjna}
Weźmy na przykład taką rekurencję:
\[
    T(n) = 3T(\frac{n}{4}) + n
\]
Zobaczmy co się dzieje po podstawieniu rekurencji do samej siebie:
\begin{enumerate}
    \item $T(n) = 3T(\frac{n}{4}) + n$
    \item $T(n) = 3(3T(\frac{n}{16}) + \frac{n}{4}) + n = 3^2T(\frac{n}{16}) + \frac{3}{4}n + n$
    \item $T(n) = 3^2(3T(\frac{n}{64}) + \frac{n}{16}) + \frac{3}{4}n + n = 3^3T(\frac{n}{64}) + \frac{3}{16}n + \frac{3}{4}n + n$
    \item \dots \footnote{Warto zauważyć, że jest to analogicznie do liczenia sumy wszystkich nodów drzewa rekursji}
\end{enumerate}
A więc ogólnie wychodzi:
\[
    %T(n) = \sum^{\log_{2}n}_{k=0} \frac{{3}{4}
\]
\subsection{Master Theorem}
Niech $a \geq 1, b > 1, f(n), d \in \mathbb{N}$ oraz $f(n)$ będzie funkcją nieujemną. Rozważmy rekurencję:
\[
    T(n) = aT(\frac{n}{b}) + \Theta(n^d)
\]
gdzie $a$ i $b$ są stałymi, a $f(n)$ jest funkcją nieujemną. Wtedy:
\begin{enumerate}
    \item $\Theta(n^d)$ jeśli $d > \log_b a$
    \item $\Theta(n^d \log n)$ jeśli $d = \log_b a$
    \item $\Theta(n^{\log_b a})$ jeśli $d < \log_b a$
\end{enumerate}
\subsubsection*{Szkic D-d}
Do przedstawienia problemu użyjemy drzewa rekursji. Rozważmy rekurencję:
\[
    T(n) = aT(\frac{n}{b}) + \Theta(n^d)
\]
\begin{center}
    \begin{tikzpicture}
        [level distance=1.5cm,
        level 1/.style={sibling distance=4cm},
        level 2/.style={sibling distance=2cm},
        level 3/.style={sibling distance=1cm}]
        \node {$c \cdot n^d$}
            child {node {$c \cdot \left(\frac{n}{b}\right)^d$}
                child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                }
                child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                }
            }
            child {node {$c \cdot \left(\frac{n}{b}\right)^d$}
                child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                }
                child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                    child {node {$\dots$}}
                }
            };
        %opis
        \node[draw=none] at (-4.5,1.5) {wielkość};
        \node[draw=none] at (0,1.5) {drzewo};
        \node[draw=none] at (4,1.5) {liczba problemów};
        %wielkosc
        \node[draw=none] at (-4.5,0) {$n^d$};
        \node[draw=none] at (-4.5,-1.5) {$\frac{n^d}{b^d}$};
        \node[draw=none] at (-4.5,-3) {$\frac{n^d}{b^{2d}}$};
        %liczba problemów
        \node[draw=none] at (4.5,0) {$1$};
        \node[draw=none] at (4.5,-1.5) {$a$};
        \node[draw=none] at (4.5,-3) {$a^2$};
        %dodaj linie oddzielajaca miedzy opisem a drzewem
        \draw[thin] (-6,1) -- (6,1);
        %dodaj linie między wielkością a drzewem
        \draw[thin] (-4,0.5) -- (-4,-3.5);
        %dodaj linie między drzewem a liczbą problemów
        \draw[thin] (4,0.5) -- (4,-3.5);
    \end{tikzpicture}
\end{center}
\begin{enumerate}
    \item suma kosztów w $k$--tym kroku
        \[
            a^k c (\frac{n}{b^k})^d = c (\frac{a}{b^d})^k n^d
        \]
        gdzie $c(\frac{n}{b^k})^d$ to koszt jednego podproblemu w $k$--tym kroku
    \item obliczenie wysokości drzewa:
        \[
            \frac{n}{b^h} = 1 \rightarrow h = \log_b n
        \]
    \item Obliczenie $T(n)$
        \begin{equation*}
            T(n) = \Theta(\sum^{\log_b n}_{k=0} c\frac{a}{b^k}n^d) \\
                 &= \Theta(c \cdot n^d \sum^{\log_b n}_{k=0} (\frac{a}{b^d})^k) \\
                 &= \Theta(c \cdot n^d \frac{1-(\frac{a}{b^d})^{\log_b n + 1}}{1-\frac{a}{b^d}}) \implies T(n) = \Theta(n^d)
        \end{equation*}
    \item rozważmy 3 przypadki:
        \begin{enumerate}
            \item $d > \log_b a$ \marginpar{root -- heavy}
                \[
                    T(n) = \Theta(n^d)
                \]
            \item $d = \log_b a$ \marginpar{równo}
                \[
                    T(n) = \Theta(n^d \log n)
                \]
            \item $d < \log_b a$ \marginpar{leaf -- heavy}
                \[
                    T(n) = \Theta(n^{\log_b a})
                \]
        \end{enumerate}
\end{enumerate}

\subsubsection*{Przykłady}
\begin{itemize}
    \item $T(n) = 4T(\frac{n}{2}) + 11n$ \newline
        Wtedy kożystając z \textbf{Master Theorem} mamy:
        \[
            a = 4, b = 2, d = 1
        \]
        Jak i również
        \[
            \log_b a = \log_2 4 = 2 > 1 = d \implies T(n) = \Theta(n^2)7
        \]
    \item $T(n) = 4T(\frac{n}{3}) + 3n^2$ \newline
        Wtedy
        \[
            a = 4, b = 3, d = 2
        \]
        Jak i również
        \[
            \log_b a = \log_3 4 > 2 = d \implies T(n) = \Theta(n^{\log_3 4})
        \]
    \item $T(n) = 27T(\frac{n}{3}) + \frac{n^2}{3}$ \newline
        Wtedy
        \[
            a = 27, b = 3, d = 2
        \]
        Jak i również
        \[
            \log_b a = \log_3 27 = 3 > 2 = d \implies T(n) = \Theta(n^3\log n)
        \]
\end{itemize}

\subsection{Metoda dziel i zwyciężaj (D\&C)}
Na czym ona polega?
\begin{enumerate}
    \item Podział problemu na mniejsze podproblemy \footnote{W zapisie rekurencyjnym $T(n) = cT(cn) + \underline{n^d}$}
    \item Rozwiazanie rekurencyjnie mniejsze podpoblemy
    \item połącz rozwiązania podproblemów w celu rozwiązania problemu wejściowego
\end{enumerate}
\subsubsection{Algorytm -- Binary Search}
\begin{itemize}
    \item \textbf{Input}: posortowania tablica \texttt{A[1..n]} oraz element \texttt{x}
    \item \textbf{Output}: indeks \texttt{i} taki, że \texttt{A[i] = x} lub \texttt{0} jeśli \texttt{x} nie występuje w \texttt{A}
    \item przebieg algorytmu: %pseudokod
        \begin{algorithm}[H]
            \caption{Binary Search}
            \begin{algorithmic}[1]
                \Procedure{BinarySearch}{A, x}
                \State $l = 1$
                \State $r = |A|$
                \While{$l \leq r$}
                \State $m = \lfloor \frac{l+r}{2} \rfloor$
                \If{$A[m] = x$}
                \State \Return{$m$}
                \ElsIf{$A[m] < x$}
                \State $l = m + 1$
                \Else
                \State $r = m - 1$
                \EndIf
                \EndWhile
                \State \Return{0}
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item \textbf{Asypmtotyka}
        Algorytm spełnia następująca rekurencje:
        \[
            T(n) = T(\frac{n}{2}) + \Theta(1)
        \]
        Rozwiązując za pomocą \textbf{Master Theorem} otrzymujemy:
        \[
            T(n) = \Theta(\log n)
        \]
\end{itemize}

\subsubsection{Algorytm -- potęgowanie liczby do naturalnej potęgi}
\begin{itemize}
    \item \textbf{Problem}: obliczanie $x^n$\\
        Można rozbić mnożenie $n$ $x$ na odpowiednie podproblemy:
        %podkresl n/2 xksów
        \[
            x^n = \underbrace{x \cdot x \cdot \dots \cdot x}_{\frac{n}{2}} \cdot \underbrace{x \cdot x \cdot \dots \cdot x}_{\frac{n}{2}}
        \]
        A więc mamy:
        \[
            x^n = \begin{cases}
                x^{\frac{n}{2}} \cdot x^{\frac{n}{2}} & \text{dla n parzystych} \\
                x^{\frac{n-1}{2}} \cdot x^{\frac{n-1}{2}} \cdot x & \text{dla n nieparzystych}
            \end{cases}
        \]
    \item \textbf{Asymptotyka}: \\
        Algorytm spełnia następującą rekurencję:
        \[
            T(n) = T(\frac{n}{2}) + \Theta(1)
        \]
        Rozwiązując za pomocą \textbf{Master Theorem} otrzymujemy:
        \[
            T(n) = \Theta(\log n)
        \]
\end{itemize}

\subsubsection{Obliczenie n-tej liczby Fibonacciego}
\begin{itemize}
    \item \textbf{Problem}:
        \[
            F_n = \begin{cases}
                0 & \text{dla n = 0} \\
                1 & \text{dla n = 1} \\
                F_{n-1} + F_{n-2} & \text{dla n > 1}
            \end{cases}
        \]
    \item \textbf{Algorytmy}:
        \begin{enumerate}
            \item Naiwna rekurencja używająca definicji.
                %drzewo rekursji dla n = 4
                \begin{center}
                    \begin{tikzpicture}
                        [level distance=1.5cm,
                        level 1/.style={sibling distance=4cm},
                        level 2/.style={sibling distance=2cm},
                        level 3/.style={sibling distance=1cm}]
                        \node {$F_4$}
                            child {node {$F_3$}
                                child {node {$F_2$}
                                    child {node {$F_1$}}
                                    child {node {$F_0$}}
                                }
                                child {node {$F_1$}}
                            }
                            child {node {$F_2$}
                                child {node {$F_1$}}
                                child {node {$F_0$}}
                            };
                        %caption
                        \node[draw=none] at (0,1) {Obliczanie $F_4$};
                    \end{tikzpicture}
                \end{center}
                Kontynułując dostajemy asymptotyke rzędu $\Theta(\phi^n)$
            \item \textit{bottom up} -- iteracyjne obliczanie kolejnych liczb Fibonacciego. Asymptotyka wynosi $\Theta(n)$
            \item Kożystanie z wzoru wynikającego z rozwiązanej rekurencji:
                \[
                    F_n = \frac{1}{\sqrt{5}} \left( \left(\frac{1+\sqrt{5}}{2}\right)^n - \left(\frac{1-\sqrt{5}}{2}\right)^n \right)
                \]
                Problem z tym podejsciem polega na niedokładnym przybilżeniu przez komputery wartości $\phi$
            \item Kożystając z lematu:
                \[
                    \begin{pmatrix}
                        1 & 1 \\
                        1 & 0
                    \end{pmatrix}^n
                    =
                    \begin{pmatrix}
                        F_{n+1} & F_n \\
                        F_n & F_{n-1}
                    \end{pmatrix},
                \]
                \begin{proof}
                    \begin{enumerate}
                        \item Warunek początkowy: dla $n = 0$
                            \[
                                \begin{pmatrix}
                                    1 & 1 \\
                                    1 & 0
                                \end{pmatrix}^0
                                =
                                \begin{pmatrix}
                                    1 & 0 \\
                                    0 & 1
                                \end{pmatrix}
                                =
                                \begin{pmatrix}
                                    F_1 & F_0 \\
                                    F_0 & F_{-1}
                                \end{pmatrix}
                            \]
                        \item Krok indukcyjny:\\
                            załóżmy, że dla pewnego $k$ zachodzi:
                            \[
                                \begin{pmatrix}
                                    1 & 1 \\
                                    1 & 0
                                \end{pmatrix}^k
                                =
                                \begin{pmatrix}
                                    F_{k+1} & F_k \\
                                    F_k & F_{k-1}
                                \end{pmatrix}
                            \]
                            wtedy dla $k+1$ mamy:
                            \[
                                \begin{pmatrix}
                                    1 & 1 \\
                                    1 & 0
                                \end{pmatrix}^{k+1}
                                =
                                \begin{pmatrix}
                                    F_{k+2} & F_{k+1} \\
                                    F_{k+1} & F_k
                                \end{pmatrix}
                            \]
                            \[
                                \begin{pmatrix}
                                    1 & 1 \\
                                    1 & 0
                                \end{pmatrix}^{k+1}
                                =
                                \begin{pmatrix}
                                    F_{k+1} & F_k \\
                                    F_k & F_{k-1}
                                \end{pmatrix}
                                \begin{pmatrix}
                                    1 & 1 \\
                                    1 & 0
                                \end{pmatrix}
                                =
                                \begin{pmatrix}
                                    F_{k+1} + F_k & F_{k+1} \\
                                    F_k + F_{k-1} & F_k
                                \end{pmatrix}
                                =
                                \begin{pmatrix}
                                    F_{k+2} & F_{k+1} \\
                                    F_{k+1} & F_k
                                \end{pmatrix}
                            \]
                    \end{enumerate}
                \end{proof}
                Algorytm ten ma złożoność $\Theta(n\log n)$
        \end{enumerate}
\end{itemize}

\subsubsection{Mnożenie liczb}
\begin{itemize}
    \item \textbf{Input}: $x, y$ takie, że $\max\{|x|, |y|\}$
    \item \textbf{Output}: $x \cdot y$
    \item \textbf{Algorytmy}:
        \begin{enumerate}
            \item standardowe mnożenie szkolne -- mnożenia w słupku jego asyptotyka wynosi $\Theta(n^2)$
            \item Podejście metodą \textbf{D\&C}
                \begin{itemize}
                    \item \textbf{Podejście}: Rozbijamy liczby na dwie równe części, a następnie mnożymy je przez siebie \\
                        Możemy zapisać $x$ oraz $y$ jako:
                        \[
                            x = \underbrace{x_L \cdot 2^{\frac{n}{2}}}_{\frac{n}{2} \text{bitów}} +\underbrace{x_R}_{\frac{n}{2} \text{bitów}}
                        \]
                        \[
                            y = \underbrace{y_L \cdot 2^{\frac{n}{2}}}_{\frac{n}{2} \text{bitów}} +\underbrace{y_R}_{\frac{n}{2} \text{bitów}}
                        \]
                        Proces mnożenia wygląda następująco:
                        \begin{equation*}\begin{split}
                            x \cdot y &= (x_L \cdot 2^n + x_R) \cdot (y_L \cdot 2^n + y_R) \\
                                      &= x_L y_L \cdot 2^{2n} + ((x_L + x_R)(y_L + y_R) - x_L y_L - x_R y_R) \cdot 2^n \\
                                      &\quad + x_R y_R
                        \end{split}\end{equation*}
                        Generalnie wszytkie wykonywane powyżej operacje są giga tanie bo opreacje takie jak mnożenie przez $2^k$ wiąże się jedynie z przesunięciem bitowym.
                    \item \textbf{Asymptotyka}: Nasz algorytm spełnia następującą rekurencje na podstawie zapisanego wyżej równania
                        \[
                            T(n) = 4T(\frac{n}{2}) + \Theta(n)
                        \]
                        Kożystając ponownie z \textbf{Master Theorem} można wywnioskować, że algorytm ma złożoność $\Theta(n^2)$. Zatem nie ma żadnego znacznego przyśpieszenia, nawet prawdopodobnie stała ukryta w $\Theta(n^2)$ jest gorsza niż w standardowym podjesciu
                \end{itemize}
            \item Metoda Gaussa
                \begin{itemize}
                    \item Rozważmy mnożenie liczb zespolonych
                        \[
                            (a+ib)(c+id) = ac + i(ad+bc) + bd
                        \]
                        \[
                            bc + ad = (a+b)(c+d) - ac - bd
                        \]
                        zatem
                        \[
                            x \cdot y = x_Ly_L \cdot 2^n + ((x_L + x_R)(y_L + y_R) - x_Ly_L - x_Ry_R) \cdot 2^{\frac{n}{2}} + x_Ry_R
                        \]
                    \item \textbf{Asymptotyka}: algorytm ten spełnia rekurencje
                        \[
                            T(n) = 3T(\frac{n}{2}) + \Theta(n)
                        \]
                        Z \textbf{Master Theorem} otrzymujemy, że algorytm ma złożoność $\Theta(n^{\log_2 3})$, a $\log_2 3 \approx 1.58$
                \end{itemize}
            \item Istneją jeszcze szybsze, nowsze algorytmy mnożenia liczb, takie jak algorytm Schönhage'a-Strassena bazuje ono na szybkiej transformacie Fouriera \textit{Fast Fourier Transform}, który ma złożoność $\Theta(n \log n \log \log n)$. Jednakże, trzeba wziąść pod uwagę stałą ukrytą w $\Theta$. W praktyce, dla liczb o rozmiarze do $10^6$ lepiej jest użyć standardowego algorytmu mnożenia.
        \end{enumerate}
\end{itemize}
Trochę pseudo kodu dla mnożenia liczb:
\begin{algorithm}
    \caption{Mnożenie liczb}
    \begin{algorithmic}[1]
        \Procedure{Multiply}{x, y}
        \State $n = \max\{|x|, |y|\}$
        \If{$n = 1$}
        \State \Return{$x \cdot y$}
        \EndIf
        \State $x_L, x_R = \text{LetfMost}\left\lceil\frac{n}{2}\right\rceil, \text{RightMost}\left\lceil\frac{n}{2}\right\rceil \text{bits of $x$}$
        \State $y_L, y_R = \text{LetfMost}\left\lceil\frac{n}{2}\right\rceil, \text{RightMost}\left\lceil\frac{n}{2}\right\rceil \text{bits of $y$}$
        \State $p_1 = \text{Multiply}(x_L, y_L)$
        \State $p_2 = \text{Multiply}(x_R, y_R)$
        \State $p_3 = \text{Multiply}(x_L + x_R, y_L + y_R)$
        \State \Return{$p_1 \cdot 2^{2n} + (p_3 - p_1 - p_2) \cdot 2^n + p_2$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\subsubsection{Mnożenie macierzy}
\begin{itemize}
    \item \textbf{Input}: dwie macierze $A, B$ rozmiaru $n \times n$
    \item \textbf{Output}: macierz $C = A \cdot B$
    \item \textbf{Algorytmy}:
        \begin{enumerate}
            \item Naiwne mnożenie macierzy -- jego złożoność wynosi $\Theta(n^3)$ bo aby policzyć jedną komórkę macierzy $C$ musimy wykonać $n$ mnożeń (i $n-1$ dodawań \footnote{w sumie $n^2$ operacji}), a skoro macierz $C$ ma $n^2$ komórek to złożoność wynosi $\Theta(n^3)$
            \item Algorytm Strassena -- \textbf{D\&C}
                \begin{itemize}
                    \item \textbf{Podejście}: Rozbijamy macierze na 4 równe części
                        \[
                            A = \begin{pmatrix}
                                A_{11} & A_{12} \\
                                A_{21} & A_{22}
                            \end{pmatrix}
                        \]
                        \[
                            B = \begin{pmatrix}
                                B_{11} & B_{12} \\
                                B_{21} & B_{22}
                            \end{pmatrix}
                        \]
                        \[
                            C = \begin{pmatrix}
                                C_{11} & C_{12} \\
                                C_{21} & C_{22}
                            \end{pmatrix}
                        \]
                        gdzie
                        \[
                            C_{11} = A_{11}B_{11} + A_{12}B_{21}
                        \]
                        \[
                            C_{12} = A_{11}B_{12} + A_{12}B_{22}
                        \]
                        \[
                            C_{21} = A_{21}B_{11} + A_{22}B_{21}
                        \]
                        \[
                            C_{22} = A_{21}B_{12} + A_{22}B_{22}
                        \]
                    \item \textbf{Asymptotyka}: Algorytm ten spełnia rekurencje
                        \[
                            T(n) = 7T(\frac{n}{2}) + \Theta(n^2)
                        \]
                        Z \textbf{Master Theorem} otrzymujemy, że algorytm ma złożoność $\Theta(n^{\log_2 7})$, a $\log_2 7 \approx 2.81$
                        \footnote{Aby zejść do rekurencji $T(n) = 7T(\frac{n}{2}) + \Theta(n^2)$ trzeba wykonać pewne, bardziej wyrafinowane triki, które nie są dokładnie opisane tutaj. Z algorytmu zapisanego wyżej wynika że rekurencja to $T(n) = 8T(\frac{n}{2}) + \Theta(n^2)$, a wieć złożoność wynosi $\Theta(n^3)$}
                \end{itemize}
        \end{enumerate}
\end{itemize}

\subsubsection{Quick Sort}
Algortym \textbf{Merge Sort} ociera się o minimalną granicę złożoności sortowania, która wynosi $\Theta(n \log n)$, jednaże jest z nim problem związany z pamięcia: nie sortuje w miejscu, a więc wymaga dodatkowej pamięci.
\begin{itemize}
    \item \textbf{Input}: tablica $A[1..n]$
    \item \textbf{Output}: posortowana tablica $A$
    \item \textbf{Algorytm}: \texttt{QuickSort(A, p, q)}
        \begin{enumerate}
            \item Podziel tablicę \texttt{A[p...q]} na dwie podtablice \texttt{A[p...k-1]} oraz \texttt{A[k+1...q]}, gdzie \texttt{A[k]} jest elementem rozdzielającym -- \textit{pivotem}\footnote{o tym jak ten \textif{pivot} jest wybierany będziemy mówić później} tak, że:
                \[
                    \forall i \in [p...k-1]: A[i] \leq A[k]: \forall j \in [k+1...q]: A[j] \geq A[k]
                \]
            \item Odpalamy rekurencyjnie \texttt{QuickSort(A, p, k-1)} oraz \texttt{QuickSort(A, k+1, q)}
        \end{enumerate}
    \item \textbf{Przyklad}:
        \begin{enumerate}
            \item mamy dane $A = [6,1,4,3,5,7,2,8]$, wybieramy \textit{pivot} jako $6$
            \item Przebieg partycjonowania:
            \[
                A = [1,4,3,5,2,6,7,8]
            \]
            Elementy mniejsze niż $6$ znalazły się po lewej stronie, większe po prawej. Pozycja pivota: $A[6] = 6$.
            \item Rekurencyjnie sortujemy dwie części:
            \begin{itemize}
                \item \texttt{QuickSort(A, 1, 5)} dla tablicy $[1,4,3,5,2]$, np. wybieramy pivot $1$:
                \[
                    A = [1,4,3,5,2]
                \]
                Po dalszym sortowaniu otrzymamy $[1,2,3,4,5]$
                \item \texttt{QuickSort(A, 7, 8)} dla $[7,8]$, który już jest posortowany.
            \end{itemize}
            \item Finalna posortowana tablica:
            \[
                A = [1,2,3,4,5,6,7,8]
            \]
        \end{enumerate}
        Na koniec przykład w drzewie rekursji:
        \begin{center}
            \begin{tikzpicture}[
                level distance=2.2cm,
                level 1/.style={sibling distance=8cm},
                level 2/.style={sibling distance=5cm},
                level 3/.style={sibling distance=3cm},
                level 4/.style={sibling distance=2cm},
                every node/.style={align=center}
                ]

                \node {QS([\textcolor{orange}{6}, \textcolor{blue}{1,4,3,5,2}, \textcolor{red}{7,8}])}
                    child {
                        node {QS([\textcolor{orange}{1}, \textcolor{red}{4,3,5,2}])}
                        child {
                            node {QS([])}
                        }
                        child {
                            node {QS([\textcolor{orange}{4}, \textcolor{blue}{3,2}, \textcolor{red}{5}])}
                            child {
                                node {QS([\textcolor{orange}{3}, \textcolor{blue}{2}])}
                                child {
                                    node {QS([2])}
                                }
                                child {
                                    node {QS([])}
                                }
                            }
                            child {
                                node {QS([5])}
                            }
                        }
                    }
                    child {
                        node {QS([\textcolor{orange}{7}, \textcolor{red}{8}])}
                        child {
                            node {QS([])}
                        }
                        child {
                            node {QS([8])}
                        }
                    };

            \end{tikzpicture}
        \end{center}

\end{itemize}

\section{Wykład \date{2025-03-24}}
\subsection{Quick Sort}
\subsubsection{Lemuto Partition}
\begin{itemize}
    \item \textbf{Input}: tablica $A[1..n]$
    \item \textbf{Output}: posortowana tablica $A$
    \item \textbf{Algorytm}: \texttt{Lemuto(A, p, q)}
        \begin{algorithm}
            \caption{Lemuto Partition}
            \begin{algorithmic}[1]
                \Procedure{Lemuto}{A, p, q}
                \State $\text{pivot} = A[p]$
                \State $i = p$
                \For{$j = p+1$ to $q$}
                \If{$A[j] < \text{pivot}$}
                \State $i = i + 1$
                \State \text{swap} $A[i] \leftrightarrow A[j]$
                \EndIf
                \EndFor
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item \textbf{Przykład:}
        \begin{enumerate}
            \item zaczynamy z nieposotowaną tablicą $A = [6,10,13,5,8,3,2,11]$
                \item wybieramy pivot $A[1] = 6$
            \item inicjalizujemy $i = 1$
            \item iterujemy przez tablicę od $j = 2$ do $j = 8$:
                \begin{itemize}
                    \item $j = 2$: $A[2] = 10$ (nie mniejsze od pivot)
                    \item $j = 3$: $A[3] = 13$ (nie mniejsze od pivot)
                    \item $j = 4$: $A[4] = 5$ (mniejsze od pivot)
                        \begin{itemize}
                            \item $i = i + 1 = 2$
                            \item zamiana $A[2] \leftrightarrow A[4] \Rightarrow A = [6, 5, 13, 10, 8, 3, 2, 11]$
                        \end{itemize}
                    \item $j = 5$: $A[5] = 8$ (nie mniejsze od pivot)
                    \item $j = 6$: $A[6] = 3$ (mniejsze od pivot)
                        \begin{itemize}
                            \item $i = i + 1 = 3$
                            \item zamiana $A[3] \leftrightarrow A[6] \Rightarrow A = [6, 5, 3, 10, 8, 13, 2, 11]$
                        \end{itemize}
                    \item $j = 7$: $A[7] = 2$ (mniejsze od pivot)
                        \begin{itemize}
                            \item $i = i + 1 = 4$
                            \item zamiana $A[4] \leftrightarrow A[7] \Rightarrow A = [6, 5, 3, 2, 8, 13, 10, 11]$
                        \end{itemize}
                    \item $j = 8$: $A[8] = 11$ (nie mniejsze od pivot)
                \end{itemize}
            \item zamiana pivot $A[1] \leftrightarrow A[4] \Rightarrow A = [2, 5, 3, 6, 8, 13, 10, 11]$
            \item pivot $6$ jest na pozycji $4$
        \end{enumerate}
    \item \textbf{Asymptotyka}:
        Algorytm ten wykonuje w głownej pętli $n-1$ porównań, natomiast wersja Lemuto Partition wymaga dodatkowo $n-1$ zamian elementów.

\end{itemize}

\subsubsection{Hoare Partition}
\begin{itemize}
    \item \textbf{Input}: Tablica $A[1..n]$
    \item \textbf{Output}: Posortowana Tablica $A$
    \item \textbf{Algorytm}: \texttt{Hoare(A, p, q)}
        \begin{algorithm}
            \caption{Hoare Partition}
            \begin{algorithmic}[1]
                \Procedure{Hoare}{A, p, q}
                \State $\text{pivot} = A[\frac{p+q}{2}]$
                \State $i = p-1$
                \State $j = q+1$
                \While{True}
                \State $i = i + 1$
                \While{$A[j] > \text{pivot}$}
                \State $j = j - 1$
                \If{$\geq j$}
                \State \textbf{break}
                \EndIf
                \EndWhile
                \State $\text{swap} A[i] \leftrightarrow A[j]$
                \EndWhile
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item \textbf{Przykład:}
        Generalnie algorytm ten działa na zasadzie zamiany elementów w tablicy względem \textit{pivotu} tak, że jeżeli jest element mniejszy od \textit{pivotu} to zamieniamy go z elementem większym od \textit{pivotu} z drugiej strony tablicy. Algorytm kończy się gdy wszytkie elementy mniejsze od \textit{pivotu} są po lewej stronie, a większe po prawej.
        \begin{enumerate}
            \item zaczynamy z nieposotowaną tablicą $A = [6,10,13,5,8,3,2,11]$
                 \item wybieramy pivot $A[\frac{1+8}{2}] = A[4] = \textcolor{blue}{5}$
            \item inicjalizujemy $i = 0$ i $j = 9$
            \item iterujemy aż $i \geq j$:
                \begin{itemize}
                    \item $i = 1$: $A[1] = 6$ (większe od pivot)
                    \item $j = 8$: $A[8] = 11$ (większe od pivot)
                        \begin{itemize}
                            \item $j = 7$: $A[7] = \textcolor{red}{2}$ (mniejsze od pivot)
                            \item zamiana $A[1] \leftrightarrow A[7] \Rightarrow \\
                            \begin{array}{|c|c|c|c|c|c|c|c|}
                            \hline
                            2 & \textcolor{red}{10} & 13 & \textcolor{blue}{5} & 8 & 3 & \textcolor{red}{6} & 11 \\
                            \hline
                            \end{array}$
                        \end{itemize}
                    \item $i = 2$: $A[2] = 10$ (większe od pivot)
                    \item $j = 6$: $A[6] = 6$ (większe od pivot)
                        \begin{itemize}
                            \item $j = 5$: $A[5] = \textcolor{red}{3}$ (mniejsze od pivot)
                            \item zamiana $A[2] \leftrightarrow A[5] \Rightarrow \\
                            \begin{array}{|c|c|c|c|c|c|c|c|}
                            \hline
                            2 & 3 & \textcolor{red}{13} & \textcolor{blue}{5} & 8 & \textcolor{red}{10} & 6 & 11 \\
                            \hline
                            \end{array}$
                        \end{itemize}
                    \item $i = 3$: $A[3] = 13$ (większe od pivot)
                    \item $j = 4$: $A[4] = 8$ (większe od pivot)
                        \begin{itemize}
                            \item $j = 3$: $A[3] = 13$ (większe od pivot)
                            \item zamiana $A[3] \leftrightarrow A[3] \Rightarrow \\
                            \begin{array}{|c|c|c|c|c|c|c|c|}
                            \hline
                            2 & 3 & \textcolor{blue}{5} & 13 & 8 & 10 & 6 & 11 \\
                            \hline
                            \end{array}$
                        \end{itemize}
                    \item $i = 4$: $A[4] = 8$ (większe od pivot)
                    \item $j = 3$: $A[3] = 5$ (pivot), kończymy algorytm
                \end{itemize}
            \item pivot $5$ jest na pozycji $3$ i wszystkie elementy są podzielone względem niego
        \end{enumerate}
    \item \textbf{Asymptotyka}:
        \textit{Hoare Partition} wykonue $n\pm c$ porównań -- o stałą więcej niżeli \textit{Lemuto Partition}, ale za to wykonuje mniej zamian elementów. W praktyce \textit{Hoare Partition} jest szybszy. Całościowa Asymptotyka wynosi $\Theta(n)$
\end{itemize}

\subsubsection{Analiza Worst Case}
Algorytm sortowania Quick Sort zachowuje się najgorzej w przypadku, gdy dostaje tablicę odwrotnie posortowaną. Wszystkie elementy będą znajdowały się po złej stronie \textit{pivotu}. \\
Zostaje spełniana rekurencja:
\[
    T(n) = T(n-1) + \underbrace{T(0)}_{\text{pusta lewa tablica}} + \Theta(n)
\]
Można zauważyć, że nie zadziała tu \textbf{Master Theorem}, trzeba rozwiązać ja na przykład drzewem rekursji:
\begin{center}
\begin{tikzpicture}
    \node {$cn$}
        child {node {$c(n-1)$}
            child {node {$c(n-2)$}
                child {node {$\vdots$}
                    child {node {$\Theta(1)$}}
                    child {node {$\Theta(1)$}}
                }
                child {node {$\Theta(1)$}}
            }
            child {node {$\Theta(1)$}}
        }
        child {node {$\Theta(1)$}};
    \draw [decorate,decoration={brace,mirror,raise=5pt}]
    (-1,0.5) -- (-4.5,-6.5) node [black,midway,xshift=-1.5cm] {$h = n-1$};
  \end{tikzpicture}
\end{center}
Z drzewa rekursji wynika, że powyższa rekurencja to:
\begin{equation*}\begin{aligned}
    T(n) &= T(n-1 + \Theta(n) \\
         &\leq \sum^{n}_{k=0} \left(c(n-k) + \Theta(1)\right) \\
         &= c \sum^{n}_{k=0} (n-k) + \Theta(n) \\
         &= c \sum^{n}_{k=0} k + \Theta(n) \\
         &= \Theta(n^2)
\end{aligned}\end{equation*}
Ograniczenie dolne analogicznie...
\subsubsection{Best Case Analysis}
Algorytm sortowania Quick Sort zachowuje się najlepiej w przypadku, gdy dostaje tablicę posortowaną. Wszystkie elementy będą znajdowały się po dobrej stronie \textit{pivotu}. \\
Zostaje spełniana rekurencja:
\[
    T(n) = T(\frac{n}{2}) +T(\frac{n}{2}) +\Theta(n)
\]
Można zauważyć, z \textbf{Master Theorem}, że asymptotyka wynosi:
\[
    T(n) = \Theta(n \log n)
\]
Rozważmy przypadek, w którym algorytm wykonuje się nie koniecznie optymalną ilość razy. Powiedzmy, że spełnia on taką rekurencje:
\[
    T(n) =T(\frac{n}{10}) +T(\frac{9n}{10}) +\Theta(n)
\]
Rozważając drzewo rekursji możemy zauważyć, że

\begin{center}\begin{tikzpicture}[
    level distance=2.2cm,
    level 1/.style={sibling distance=8cm},
    level 2/.style={sibling distance=5cm},
    level 3/.style={sibling distance=3cm},
    level 4/.style={sibling distance=2cm},
    every node/.style={align=center}
    ]
    \node {$cn$}
        child {node {$c\frac{n}{10}$}
            child {node {$c\frac{n}{100}$}
                child {node {$\vdots$}}
                child {node {$\vdots$}}
            }
            child {node {$c\frac{9n}{100}$}
                child {node {$\vdots$}}
                child {node {$\vdots$}}
            }
        }
        child {node {$c\frac{9n}{10}$}
            child {node {$c\frac{9n}{100}$}
                child {node {$\vdots$}}
                child {node {$\vdots$}}
            }
            child {node {$c\frac{81n}{100}$}
                child {node {$\vdots$}}
                child {node {$\vdots$}}
            }
        };
\end{tikzpicture}\end{center}
Każdy wiersz tego drzewa sumuje się do $cn$. Wysokość drzewa wynosi $\log_{10/9} n$, zatem złożoność wynosi $\Theta(n \log_{10/9} n)$, co jest tak naprawdę równe $\Theta(n \log n)$.
\subsubsection{Rozważenie przypadku mieszanego}
Rozważmy przypadek, w którym algorytm raz wykonuje się z best casem -- dzieli się tablica na pół, a raz z worst casem -- dzieli się tablica na 1 i $n-1$ elementów. \\
Zostaje spełniana rekurencja:
\[
    L(n) = 2U(\frac{n}{2}) + \Theta(n)
\]
\[
    U(n) = L(n-1) + \Theta(n)
\]
gdzie $L$ symbolizuje best case, natomiast $U$ worst case. Rozwiązując powyższą rekurencje otrzymujemy:
\begin{equation*}
    \begin{aligned}
        L(n)
        &= 2(L(\frac{n}{2} -1) +\Theta(n)) + \Theta(n) \\
        &= 2L(\frac{n}{2} -1) + \Theta(n) \\
        &= \Theta(n \log n)
    \end{aligned}
\end{equation*}

\subsubsection{Average Case Analysis} \label{sec:average_case_analysis}
Algorytm Quick Sort da się ``zabezpieczyć'' przed złym rozkładem danych poprzez losowym wybraniem pivota i następnie swapnięcie go z naszym deterministycznym miejscem. W ten sposób bedzięmy mieli zawsze jednostajnie losowy rozkład danych.\\
Wprowadźmy
\[
    T_n  \text{ -- zmienna losowa liczby porównań w Quick Sorcie sortowanej tablicy } A, |A|=n
\]
Do dziś nie jest znany rozkład zmiennej losowej $T_n$. \\
Niech $X$ będzie zmienną indykatorową:
\begin{equation*}
    X^{(n)}_{k} =
    \begin{cases}
        1 \text{ jeśli partition podzieli tablice n-elementową na }(k,(n-k-1)) \\
        0 \text{ w przeciwnym przypadku}
    \end{cases}
\end{equation*}
Teraz rozważmy zachowanie zmiennej losowej $T_n$:
\begin{equation*}
    T_n \stackrel{\mathclap{\normalfont\mbox{d}}}{=}
    \begin{cases}
        T_0+T_{n-1}+n-1 \text{ jeśli } (0,n-1) \text{ jest partitionem} \\
        T_1+T_{n-2}+n-1 \text{ jeśli } (1,n-2) \text{ jest partitionem} \\
        \vdots \\
        T_k+T_{n-k-1}+n-1 \text{ jeśli } (k,n-k-1) \text{ jest partitionem} \\
        \vdots \\
        T_{n-1}+T_0+n-1 \text{ jeśli } (n-1,0) \text{ jest partitionem}
    \end{cases}
\end{equation*}
Stosując zmienną indykatową $X$ otrzymujemy
\[
    T_n = \sum^{n-1}_{k=0} X^{(n)}_{k} (T_k + T_{n-k-1} + n-1)
\]
Rozważmy niezależność zmiennych $X^{(n}_k}$ i $T_k$. Są one niezależne, ponieważ ilość porównań nie jest zależna od tego jak poźniej bedzie dzielić się tablica. Zatem można zapisać
\[
    \mathbb{E}[X^{(n)}_{k} T_k] = \mathbb{E}[X^{(n)}_{k}] \mathbb{E}[T_k]
\]
Skoro przyjmujemy jednostajny rozkład danych wejścowych to wartość oczekiwana $X^{(n)}_{k}$ wynosi:
\[
    \mathbb{E}[X^{(n)}_{k}] = 1 \cdot P(X^{(n)}_{k} = 1) + 0 \cdot P(X^{(n)}_{k} = 0) = P(X^{(n)}_{k} = 1) = \frac{(n-1)!}{n!} = \frac{1}{n}
\]
Teraz policzmy wartość oczekiwaną $T_n$
\begin{equation*}\begin{aligned}
    \mathbb{E}[T_n]
    &= \mathbb{E}\left[\sum^{n-1}_{k=0} X^{(n)}_{k} (T_k + T_{n-k-1} + n-1)\right] \\
    &= \sum^{n-1}_{k=0} \mathbb{E}[X^{(n)}_{k} (T_k + T_{n-k-1} + n-1)] \\
    &= \sum^{n-1}_{k=0} \mathbb{E}[X^{(n)}_{k}] \mathbb{E}[T_k + T_{n-k-1} + n-1] \\
    &= \sum^{n-1}_{k=0} \frac{1}{n} \mathbb{E}[T_k + T_{n-k-1} + n-1] \\
    &= \frac{1}{n} \sum^{n-1}_{k=0} \mathbb{E}[T_k] + \mathbb{E}[T_{n-k-1}] + \mathbb{E}[n-1] \\
    &= \frac{1}{n} \sum^{n-1}_{k=0} \mathbb{E}[T_k] + \frac{1}{n} \sum^{n-1}_{k=0} \mathbb{E}[T_{n-k-1}] + \frac{1}{n} \sum^{n-1}_{k=0} n-1 \\
\end{aligned}\end{equation*}
Można zauważyć, że $\sum^{n-1}_{k=0} \mathbb{E}[T_k] = \sum^{n-1}_{k=0} \mathbb{E}[T_{n-k-1}]$ ponieważ jest to doawanie tych samych rzeczy w innej kolejności (od przodu i od tyłu). Zatem
\begin{equation*}\begin{aligned}
    \mathbb{E}[T_n]
    &= \frac{2}{n} \sum^{n-1}_{k=0} \mathbb{E}[T_k] + \frac{1}{n} \sum^{n-1}_{k=0} n-1 \\
    &= \frac{2}{n} \sum^{n-1}_{k=0} \mathbb{E}[T_k] + n-1
\end{aligned}\end{equation*}
Przyjmijmy oznaczenie $\mathbb{E}[T_n] = t_n$, wtedy
\[
    t_n = \frac{2}{n} \sum^{n-1}_{k=0} t_k + n-1
\]
Jest to rekurencja %TODO sprawdzic jaka to rekurencja
Można ją rozwiązać w następujący sposób:
\[
    t_n = \frac{2}{n} \sum^{n-1}_{k=0} t_k + n-1 \quad | \cdot n
\]
\[
    nt_n = 2 \sum^{n-1}_{k=0} t_k + n(n-1)
\]
Podstawmy za $n \rightarrow n-1$
\[
    (n-1)t_{n-1} = 2 \sum^{n-2}_{k=0} t_k + (n-1)(n-2)
\]
Odejmując stronami równanie otrzymujemy:
\[
    nt_n - (n-1)t_{n-1} = 2 \sum^{n-1}_{k=0} t_k + n(n-1) - 2 \sum^{n-2}_{k=0} t_k - (n-1)(n-2)
\]
\[
    nt_n - (n-1)t_{n-1} = 2t_{n-1} + 2(n-1)
\]
Przekształcając otrzymujemy:
\[
    nt_n = (n+1)t_{n-1} + 2(n-1) \quad | : n(n+1)
\]
\[
    \frac{t_n}{n+1} = \frac{t_{n-1}}{n} + \frac{2(n-1)}{n(n+1)}
\]
Przyjmijmy kolejne oznaczenie $s_n = \frac{t_n}{n+1}$, wtedy
\[
    s_n = s_{n-1} + 2 \frac{n-1}{n(n+1)}
\]
jest już prostą rekurencją, którą można łatwo rozwiązać iteracyjnie.
\begin{equation*}\begin{aligned}
    s_n
    &= 2 \sum^n_{k=1} \frac{k-1}{k(k+1)} \\
    &= 2 \sum^n_{k=1} \left( \frac{2}{k+1} - \frac{1}{k} \right) \\
    &= 4 \sum^n_{k=1} \frac{1}{k+1} - 2 \sum^n_{k=1} \frac{1}{k} \\
    &= 4 \left( H_n + \frac{1}{n+1} - 1 \right) - 2 H_n \\
    &= 4H_n + 4 \frac{1}{n+1} - 4 - 2H_n \\
    &= 2H_n + \frac{4}{n+1} - 4
\end{aligned}\end{equation*}
gdzie $H_n$ to $n$-ty element ciągu harmonicznego. Podstawiając z powrotem $t_n \leftarrow s_n(n+1)$ otrzymujemy
\[
    t_n = 2(n+1)H_n + 4(n+1) -4 = 2nH_n +2H_n +4n
\]
Kożystając z faktu, że $H_n = \ln n + \gamma + O(\frac{1}{n})$ otrzymujemy
\[
    \mathbb{E}[T_n] = 2n \log n + 2 \gamma n + \Theta(n)
\]

\section{Wykład \date{2025-03-25}}
\subsection{Dual Pivot Quick Sort}
W roku 1975 Sedgewick pokazał, że
\[
    \mathbb{E}\left[\text{\#porównań w dual pivot partition}\right] \approx \frac{16}{9}n \implies
\]
\[
    \implies \mathbb{E}\left[\text{\#porównań w dual pivot Quick Sortcie Sedgwick}\right] \approx \frac{32}{15}n \log n
\]

Jak się to liczy? \\
Niech $T_n$ - \#porównań w dual pivot Quick Sortcie Sedgwick oraz $P_n$ - \#porównań w dual pivot partition. Rekurencja spełnia takie równanie:
\[
    \mathbb{E}\left[T_n\right] = \mathbb{E}\left[P_n\right] + \frac{1}{\binom{n}{2}} \sum_{1 \leq p \leq q \leq n} \left(\mathbb{E}[T_{p-1}] + \mathbb{E}[T_{q-p-1}] + \mathbb[T_{n-q}]\left)
\]
Poźniej rozwiązuje się to analogicznie jak na poprzednim wykładzie rozwiązywana była rekurencja dla $T_n$ \ref{sec:average_case_analysis}

Natomiast w roku 2009 pan Yaroslavsky, Bentley, Blach opracował poprzez testowanie w Javie (nie miał backgroundu matematycznego) lepszy algortytm Quick Sort.
Poźniej w 2012 Sebastian Wild, Nedel pokazali, że
\[
    \mathbb{E}\left[\text{\#porównań w Dual Pivot Quick Sorcie Yaroslavskiego}\right] \approx 1.9n \log n
\]
2015, Aufmüller, Dietzfelbinger zaprojektowali \textit{Strategie Count} oraz pokazali jej optymalność
\[
    \mathbb{E}\left[\text{\#porównań w Count Partition}\right] \approx \frac{3}{2}n \implies
\]
\[
    \implies \mathbb{E}\left[\text{\#porównań w Dual Pivot z Count Partition}\right] \approx 1.8n \log n
\]
Wartość oczekiwana pojawia się w tych asymptotykach ponieważ jest element losowści zwiazany z porównaniami elementu z pivotami. Nie zawsze trzeba bedzię go porównywać z jednym i drugim pivotem.
\subsubsection{Strategia Count Partition}
\begin{itemize}
    \item Zakładamy $p<q$
        \[
            \left[ \,
                a \,\middle|\,
                \underbrace{\cdots}_{S_{i-1}} \, p \,\middle|\,
                \cdots \, q \,\middle|\,
                \underbrace{\cdots}_{L_{i-1}} \, \boxed{i} \,\middle|\,
                b
            \,\right]
        \]
    \item rozpatrzmy $i$-ty element tablicy
    \item jeśli $s_{i-1} \geq l_{i-1}$ to porównujemy $A[i]$ najpierw z $p$ jeżeli jest potrzeba to z $q$
    \item jesli $s_{i-1} \leq l_{i-1}$ to porównujemy $A[i]$ najpierw z $q$ jeżeli jest potrzeba to z $p$\footnote{jest to swoiste przewidywanie przeszłości na podstawie ilości elementów, które są już posortowane}
\end{itemize}

\subsection{Comparison Model}
\subsubsection{Drzewo decyzyjne}
Jak wygląda to na przykładzie? Mamy daną tablice $[a_1, a_2, a_3]$ i chcemy ją posortować. W drzewie decyzyjnym każdy węzeł to porównanie dwóch elementów. W liściu znajduje się permutacja elementów.
\begin{center}
    \begin{tikzpicture}[
        level 1/.style={sibling distance=70mm, level distance=40mm},
        level 2/.style={sibling distance=40mm, level distance=40mm},
        level 3/.style={sibling distance=20mm, level distance=40mm},
        root/.style={fill=orange!30},
        internal/.style={fill=blue!20},
        leaf/.style={fill=green!20, draw, ellipse}
        ]
        \node[root] {$a_1 < a_3$}
            child {node[internal] {$a_1 < a_2$}
                child {node[internal] {$a_2 < a_3$}
                    child {node[leaf] {$[a_1,a_2,a_3]$}
                    edge from parent node[engine, sloped] {$T$}}
                    child {node[leaf] {$[a_1,a_3,a_2]$}
                    edge from parent node[engine, sloped] {$F$}}
                    edge from parent node[engine, sloped] {$T$}
                }
                child {node[leaf] {$[a_3,a_1,a_2]$}
                edge from parent node[engine, sloped] {$F$}}
                edge from parent node[engine, sloped] {$T$}
            }
            child {node[internal] {$a_2 < a_3$}
                child {node[internal] {$a_1 < a_3$}
                    child {node[leaf] {$[a_2,a_1,a_3]$}
                    edge from parent node[engine, sloped] {$T$}}
                    child {node[leaf] {$[a_2,a_3,a_1]$}
                    edge from parent node[engine, sloped] {$F$}}
                    edge from parent node[engine, sloped] {$T$}
                }
                child {node[leaf] {$[a_3,a_2,a_1]$}
                edge from parent node[engine, sloped] {$F$}}
                edge from parent node[engine, sloped] {$F$}
            };
    \end{tikzpicture}
\end{center}
\begin{itemize}
    \item dla dowolnego algorytmu sortowania w \textit{Comparison Model} można znaleść drogę na drzewie decyzyjnym
    \item W tym drzewie występuje $n!$ liści będacych permutacjami tablicy
    \item \textit{Worst Case} odpowiada najdłuższej ścieżce w drzewie decyzyjnym
    \item drzewo binarne pełne odpowiada najlepszemu algorytmowi sortowania
    \item drzewo binarne pełne o wysokości $h$ ma conajwyżej $2^h$ liści, ale liści w drzewie decyzyjnym powinno być przynajmniej $n!$ zatem
        \[
            2^h \geq n! \implies h \geq \log_2 n!
        \]
\end{itemize}
\subsubsection{Twierdzenie}
Dolne ograniczenie na liczbę porównań w problemie sortowania w \textit{Comparison Model} wynosi $\Omega(n \log n)$
\begin{proof}
    Rozważmy drzewo decyzyjne dla sortowania $n$ elementów. Każda ścieżka w drzewie decyzyjnym odpowiada jednej permutacji. Zatem drzewo decyzyjne ma przynajmniej $n!$ liści. Trzeba pokazać, że wysokość drzewa decyzyjnego jest przynajmniej $\log_2 n!$.
    \[
        2^h \geq n! \implies h \geq \log n!
    \]
    Używając wzoru Stirlinga
    \begin{equation*}\begin{aligned}
        h \geq \log n! &= \log \left( \sqrt{2\pi n} \left( \frac{n}{e} \right)^n \left(1+ O(1)\right) \right)\\
                       &= \log \left(\frac{n}{e}\right)^n + \log \left(\sqrt{2\pi n}(1+O(1))\right) \\
                       &= \underbrace{n \log n}_{\Theta(n \log n)} - \underbrace{n \log e}_{\Theta(n)} + \underbrace{\log \sqrt{2\pi n}}_{\Theta(\log n)} + \underbrace{\log (1+O(1))}_{\Theta(1)} \\
                       &= \Omega(n \log n)
    \end{aligned}\end{equation*}
    Zatem dolne ograniczenie na liczbę porównań w problemie sortowania w \textit{Comparison Model} wynosi $\Omega(n \log n)$ \\
\end{proof}

\subsubsection{Counting Sort}
\begin{itemize}
    \item \textbf{Input}: Tablica $A, |A|=n, \forall i A[i] \in \{0,1,2,...,k\}$, $k$ - stała
    \item \textbf{Output}: Posortowana tablica $A$
    \item \textbf{Algorytm}:
        \begin{algorithm}
            \caption{Counting Sort}
            \begin{algorithmic}[1]
                \Procedure{CountingSort}{A, n, k}
                \For{$i = 1$ to $k$} \Comment{$\Theta(k)$}
                    \State $C[i] = 0$
                \EndFor
                \For{$j = 1$ to $n$} \Comment{$\Theta(n)$}
                    \State $C[A[j]] = C[A[j]] + 1$
                \EndFor
                \For{$i = 2$ to $k$} \Comment{$\Theta(k)$}
                    \State $C[i] = C[i] + C[i-1]$
                \State $C[i] = C[i] + C[i-1]$ \Comment{liczba elementów mniejszych lub równych $i$}
                \EndFor
                \For{$j = n$ downto $1$} \Comment{$\Theta(n)$}
                    \State $B[C[A[j]]] = A[j]$
                    \State $C[A[j]] = C[A[j]] - 1$
                \EndFor
                \State \textbf{return} $B$ \Comment{$\Theta(1)$}
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item \textbf{Przykład}:
        \begin{enumerate}
            \item $A = [4,1,3,4,3]$, $C=[0,0,0,0]$, $B=[0,0,0,0,0]$ (tablice indeksowane od $1$)
            \item $A = [4,1,3,4,3]$, $C=[1,0,2,2]$, $B=[0,0,0,0,0]$
            \item $A = [4,1,3,4,3]$, $C=[1,1,3,5]$, $B=[0,0,0,0,0]$
            \item Ostatnia pętla, skupmy się na tablicy $B$:
                \begin{enumerate}
                    \item $j = 5$: $B[5] = A[5] = 3$, $C[3] = 4$
                    \item $j = 4$: $B[4] = A[4] = 4$, $C[4] = 3$
                    \item $j = 3$: $B[3] = A[3] = 3$, $C[3] = 3$
                    \item $j = 2$: $B[3] = A[2] = 1$, $C[1] = 1$
                    \item $j = 1$: $B[1] = A[1] = 4$, $C[4] = 2$
                \end{enumerate}
                $B = [1,3,3,4,4]$ - posortowana tablica
        \end{enumerate}
    \item \textbf{Asymptotyka}: $\Theta(n+k)$, gdzie $k = O(n)$
\end{itemize}

\subsection{Stable Sorting Property}
Algorytm zachowuje kolejność równych sobie elementów z tablicy wejściowej

\subsubsection{Radix Sort}
\begin{itemize}
    \item \textbf{Input}: Tablica $A, |A|=n, \forall i A[i] \in \{0,1,2,...,k\}$, $k$ - stała
    \item \textbf{Output}: Posortowana tablica $A$
    \item \textbf{Algorytm}: Zastosuj \textif{Counting Sort} dla każdej cyfry liczby
    \item \textbf{Przykład}:
        \[
            %Zapisz A jako macierz 7x1
            A = \begin{bmatrix}
                32\textcolor{red}{9} \\
                45\textcolor{red}{7} \\
                65\textcolor{red}{7} \\
                83\textcolor{red}{9} \\
                43\textcolor{red}{6} \\
                72\textcolor{red}{0} \\
                35\textcolor{red}{5}
                \end{bmatrix} \xrightarrow{\text{Counting Sort}} \begin{bmatrix}
                7\textcolor{red}{2}0 \\
                3\textcolor{red}{5}5 \\
                4\textcolor{red}{3}6 \\
                4\textcolor{red}{5}7 \\
                6\textcolor{red}{5}7 \\
                3\textcolor{red}{2}9 \\
                8\textcolor{red}{3}9
                \end{bmatrix} \xrightarrow{\text{Counting Sort}} \begin{bmatrix}
                \textcolor{red}{3}29 \\
                \textcolor{red}{3}55 \\
                \textcolor{red}{4}36 \\
                \textcolor{red}{4}57 \\
                \textcolor{red}{6}57 \\
                \textcolor{red}{7}20 \\
                \textcolor{red}{8}39
            \end{bmatrix}
        \]
        Algorytm zachowuje kolejność równych sobie elementów z tablicy wejściowej -- \textit{Stable Sorting Property}, co umożliwia sortowanie po kolejnych cyfrach.
    \item \textbf{Poprawność}: Indukcja po $t$ -- numer cyfry
        \begin{enumerate}
            \item jeśli $t=1$ to poprawność \textit{Counting Sort} jest trywialna
            \item załóżmy, że \textif{Counting Sort} jest poprawny dla $t-1$ -- cyfrowej liczby
            \item Krok indukcyjny:
                \begin{enumerate}
                    \item $t$-ta cyfra ... liczby jest ...:
                        to z założenia indukcyjnego oraz \textit{stable counting property Counting Sorta} liczby do $t$--tej cyfty dajej poprawnie posortowane
                    \item $t$-ta cyfra różna: z poprawności \textit{Counting Sort} ok.
                \end{enumerate}
        \end{enumerate}
    \item \textbf{Złożoność obliczeniowa}:
        \begin{itemize}
            \item $n$ $b$-bitowych liczb
            \item liczba $b$-bitowa dzielę na $r$-bitowych cyfr ($\frac{b}{r}$ takich liczb):
            %daj reprezentacje w postaci tablicy
                \[
                    \underbrace{\underline{\mid \quad \quad \mid} \underline{\quad \quad \mid} \underline{ \quad \dots \quad \mid} \overbrace{\underline{\quad \quad \mid}}^{r\text{-bitów}}}_{b\text{-bitów}}
                \]
            \item cyfry są z $\mid \left\{ 0,\dots,2^r -1\right\} \mid = 2^r$
            \item \textit{Counting Sort} sortujemy $\underline{n}$ liczb względem jednej liczby. A więc wykonujemy
                \[
                    \Theta(n+2^r)
                \]
                operacji.
        \end{itemize}
        Zatem \textit{Radix Sort} będzie miał złożoność obliczeniową
        \[
            \Theta\left(\frac{b}{r}(n+2^r)\right)
        \]
        machając trochę rękoma wybieramy $r$ jako $r=\log n$, wtedy
        \[
            \Theta\left(\frac{b}{\log n} \left( n +2^{\log n}\right)\right) = \Theta\left(\frac{bn}{\log n}\right)
        \]
        Jeśli $\{0, \dots, n^d-1\}$ - zakres sortowanych liczb, wtedy $b=\log n^d = d \log n$, a więc
        \[
            \Theta\left(\frac{d\log n \space n}{ \log n} \right) = \Theta\left(\frac{\cancel{\log n} n}{\cancel{\log n}}\right) = \Theta(d \cdot n)
        \]
\end{itemize}
%TO KONIEC PROBLEMU SORTOWANIA!!!!!!!!!!!!!!111

\section{Wykład \date{2025-03-31}}
Problem którym się teraz zajmiemy to tak zwany \textbf{Statystyka Pozycyjna}
\subsection{Definicja Statystyki Pozycyjnej}
\subsubsection{Definicja}
$k$-tą statystyką pozycyjną nazywamy $k$-tą najmniejszą wartością z zadnaego zbioru.
\subsubsection{Przykład}
\begin{itemize}
    \item $k=1 \rightarrow O(n)$
    \item $k=n \rightarrow O(n)$
    \item $k=\left\lfloor\frac{n-1}{2}\right\rfloor \or k=\left\lfloor\frac{n+1}{2}\right\rfloor \rightarrow \text{ sortowanie } O(n \log n)$
\end{itemize}
\subsection{Algorytm Random Select}
\begin{itemize}
    \item \textbf{Input}: Tablica $A, |A|=n$, liczba $k$
    \item \textbf{Output}: $k$-ta statystyka pozycyjna
    \item \textbf{Algorytm}:
        \begin{algorithm}
            \caption{Random Select}
            \begin{algorithmic}[1]
                \Procedure{RandomSelect}{A, p, q, i}
                \If{$p=q$}
                    \State \textbf{return} $A[p]$
                \EndIf
                \State $r = \text{RandomPartition}(A,p,q)$ \Comment{losowa partycja}
                \State $k = r-p+1$
                \If{$i=k$}
                \State \textbf{return} $A[r]$ \Comment{$A[r]$ jest $i$-tą statystyką pozycyjną}
                \ElsIf{$i<k$}
                \State \textbf{return} $\text{RandomSelect}(A,p,r-1,i)$
                \Else
                \State \textbf{return} $\text{RandomSelect}(A,r+1,q,i-k)$
                \EndIf
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item \textbf{Przykład}: $A = [6,10,13,5,8,3,2,11]$, RandomSelect($A,1,8,4$)
        Dla tablicy
        \[
            A = \begin{array}{|c|c|c|c|c|c|c|c|}
                \hline
                6 & 10 & 13 & 5 & 8 & 3 & 2 & 11 \\ \hline
            \end{array}
        \]
        wywołujemy procedurę \texttt{RandomSelect(A, 1, 8, 4)} aby znaleźć 4-ty najmniejszy element.

        \bigskip

        \textbf{Krok 1.}\\
        Zakładamy, że procedura \texttt{RandomPartition} wybiera pivot $8$.\\
        Po partycjonowaniu tablica wygląda następująco:
        \[
            A = \begin{array}{cccccccc}
                6 \quad & 5 \quad & 3 \quad & 2 \quad & \underline{8} \quad & 13 \quad & 10 \quad & 11
            \end{array}
        \]
        Pivot znajduje się na pozycji $r=5$, a $k = r - p + 1 = 5 - 1 + 1 = 5$. Ponieważ poszukiwany rząd $i=4$ jest mniejszy od $k=5$, następuje wywołanie rekurencyjne:
        \[
            \texttt{RandomSelect}(A,1,4,4)
        \]
        dla lewego podzbioru
        \[
            \begin{array}{|c|c|c|c|}
                \hline
                6 & 5 & 3 & 2 \\ \hline
            \end{array}.
        \]

        \bigskip

        \textbf{Krok 2.} \\
        Dla podtablicy
        \[
            B = \begin{array}{|c|c|c|c|}
                \hline
                6 & 5 & 3 & 2 \\ \hline
            \end{array},
        \]
        zakładamy, że pivot został wybrany jako $2$.\\
        Po partycjonowaniu otrzymujemy:
        \[
            B = \begin{array}{|c|c|c|c|}
                \hline
                \underline{2} \quad & 5 \quad & 3 \quad & 6 \\ \hline
            \end{array}
        \]
        gdzie pivot $2$ znajduje się teraz na pozycji $r=1$. Obliczamy $k = r - p + 1 = 1 - 1 + 1 = 1$.\\
        Ponieważ $i=4 > 1$, odejmujemy $k$ od $i$, czyli nowa wartość to $i = 4 - 1 = 3$, i wywołujemy:
        \[
            \texttt{RandomSelect}(B,2,4,3)
        \]
        dla podtablicy
        \[
            B' = \begin{array}{|c|c|c|}
                \hline
                5 & 3 & 6 \\ \hline
            \end{array}.
        \]

        \bigskip

        \textbf{Krok 3.} \\
        Dla podtablicy
        \[
            C = \begin{array}{|c|c|c|}
                \hline
                5 & 3 & 6 \\ \hline
            \end{array},
        \]
        zakładamy, że pivotem jest $6$.\\
        Po partycjonowaniu otrzymujemy:
        \[
            C = \begin{array}{|c|c|c|}
                \hline
                5 & 3 & \underline{6} \\ \hline
            \end{array}
        \]
        gdzie pivot $6$ znajduje się na pozycji $r=3$ (przyjmując indeksowanie od 1). Wówczas $k = r - p + 1 = 3 - 1 + 1 = 3$.\\
        Skoro $i=3$ jest równe $k$, zwracamy pivot:
        \[
            \texttt{RandomSelect}(C,1,3,3)=6.
        \]

        \bigskip

        \textbf{Wniosek:}\\
        Procedura zwraca wartość $\boxed{6}$, czyli 4-ty najmniejszy element w tablicy $A$.
    \item \textbf{Złożoność obliczeniowa}:
        \begin{itemize}
            \item \textit{Best Case}:
                W najlepszym przypadku dzielimy tablicę na dwie równe części, zatem
                \[
                    T(n) = T\left(\frac{n}{2}\right) + \Theta(n)
                \]
                Używając \textit{Master Theorem} otrzymujemy
                \[
                    T(n) = \Theta(n)
                \]
            \item \textit{Worst Case}:
                W najgorszym przypadku dzielimy tablicę na $n-1$ i $1$ elementów (-- pivot wybrany przez partition), a więc
                \[
                    T(n) = T(n-1) + \Theta(n)
                \]
                Używając \textit{Master Theorem} otrzymujemy
                \[
                    T(n) = \Theta(n^2)
                \]
            \item \textit{Average Case}:
                W średnim przypadku musimy policzyć wartość oczekiwaną takiej zmiennej losowej:
                \[
                    T_n = \begin{cases}
                        T_{n-1} + n-1: (0, n-1) \\
                        T_{n-2} + n-1: (1, n-2) \\
                        \vdots \\
                        T_1 + n-1: (n-2, 1) \\
                    \end{cases}
                \]
                Aby formalizować analizę, wprowadzamy zmienną indykatorową:
                \[
                    X^{(n)}_k =
                    \begin{cases}
                        1, & \text{jeśli przy partycjonowaniu tablicy $n$-elementowej nastąpiło rozbicie na } \\
                           & \quad (k, n-k-1) \text{ oraz dalsze wywołanie rekurencyjne odbywa się na} \\
                           & \quad \text{podtablicy zawierającej szukany element,} \\
                        0, & \text{w przeciwnym przypadku.}
                    \end{cases}
                \]
                Wówczas możemy zapisać:
                \[
                    T_n = \sum_{k=0}^{n-1} X^{(n)}_k \left( T\Bigl(\max(k,\, n-k-1)\Bigr) + n-1 \right).
                \]

                Przyjmując, że wybór pivotu jest jednostajny, mamy:
                \[
                    \mathbb{E}\Bigl[X^{(n)}_k\Bigr] = \frac{1}{n} \quad \text{dla } k=0,1,\dots,n-1.
                \]
                Zatem, stosując wartość oczekiwaną i liniowość tej wartości, otrzymujemy:
                \[
                    \mathbb{E}[T_n] = \frac{1}{n} \sum_{k=0}^{n-1} \left( \mathbb{E}\Bigl[T\bigl(\max(k,\, n-k-1)\bigr)\Bigr] + n-1 \right).
                \]

                Ponieważ gdy pivot jest szukanym elementem (dla $k=0$ lub $k=n-1$, w sensie odpowiedniego ustawienia) nie następuje rekurencja, przyjmujemy $T_0 = 0$. Dla pozostałych przypadków (czyli dla $1 \le k \le n-2$) dalsze wywołanie odbywa się na jednej z podtablic o rozmiarze nie większym niż $\lceil n/2 \rceil$ (ze względu na symetrię rozbicia). Możemy więc oszacować:
                \[
                    t_n = \mathbb{E}[T_n] \le n-1 + \frac{n-2}{n}\, t_{\lceil n/2 \rceil}.
                \]

                Łatwo (przez indukcję) pokazać, że taka rekurencja implikuje, iż
                \[
                    t_n = O(n).
                \]

                \bigskip

                \textbf{Wniosek:} Średnia liczba porównań w algorytmie RandomSelect wynosi $\Theta(n)$, czyli algorytm działa w czasie liniowym w średnim przypadku.

        \end{itemize}
\end{itemize}

\subsection{Algorytm Select}
Algorytm zwany jest także algorytmem \textit{Magicznych Piątek}
\begin{itemize}
    \item \textbf{Input}: Tablica $A, |A|=n$, liczba $k$
    \item \textbf{Output}: $k$-ta statystyka pozycyjna
    \item \textbf{Algorytm}:
        \begin{enumerate}
            \item dzielimy $A[p \dots q]$ na $\left\lfloor\frac{n}{5}\right\rfloor$ pięcio elementowe częsci oraz ostatnią część z resztą \marginnote{$\Theta(n)$}
        \item Sortujemy każdą pięcio elementową część i wybieramy z nich mediany: \marginnote{$T(\frac{\left\lceil n \right\rceil}{5}\right)$}
                \[
                    M = \left\{ m_1, m_2, \dots, m_{\left\lfloor\frac{n}{5}\right\rfloor} \right\}
                \]
            \item Znaleść medianę z tablicy $M$: wywołujemy Select($M, 1, \left\lceil\frac{n}{5}\right\rceil, \left\lfloor\frac{\left\lceil\frac{n}{10}\right\rceil}{2}\right\rfloor$) i oznaczmy ją jako $X$\footnote{nazywana też mediana median}
            \item Ustaw $X$ jako pivot w Partition \marginnote{$\Theta(n)$}
            \item idz do lewej albo prawej podtablicy w zależności od indeksu pivota i szukaj statystykju pozycyjnej \marginnote{$T(?)$}
        \end{enumerate}
    \item \textbf{Asymptotyka}:
        Algorytm spełnia rekurencję:
        \[
            T(n) = T\left(\frac{n}{5}\right) + T\left(?\right) + \Theta(n)
        \]
        Gdzie $T(?)$ oznacza czas rekurynceyjnego wywołania algorytmu Select dla reszty tablicy (pozotałych elementów niżeli napewno mniejsze/większe od $X$). Rozmiar \textit{?} jest zależny od dystrybucji elementów w tablicy. W najgorszym przypadku ograniczenie dolne na \textit{pewną część} jest rzędu:
        \[
            \geq \left( \frac{1}{2} \frac{\left\lceil n \right\rceil}{5} \right) -1 -1\right) \cdot 3 \geq \frac{3n}{10} - 6
        \]
        Zatem górne ograniczenie na \textit{?} jest rzędu:
        \[
            n - \left(\frac{3n}{10} - 6\right)-1 \leq \frac{7n}{10} + 5
        \]
        A więc Algorytm \textit{Select} spełnia rekurencję:
        \[
            T(n) = T\left(\frac{n}{5}\right) + T\left(\frac{7n}{10} + 5\right) + \Theta(n)
        \]
        Dla uproszczenia możemy przyjąć $T\left(\frac{7n}{10} + 5\right) = T\left(\frac{3n}{4}\right)$, ponieważ $\frac{3}{4}n \geq \frac{7}{10}n + 5$ dla $n \geq 20$. Przyjmujemy, że dla n<20 algorytm działa w czasie stałym. Reukrencja algorytmu \textit{Select} jest teraz następująca:
        \[
            T(n) = T\left(\frac{n}{5}\right) + T\left(\frac{3n}{4}\right) + \Theta(n)
        \]
        Rozwiążemy ją stosując indukcję:
        \begin{itemize}
            \item Base case: $T(n) = \Theta(1)$ dla $n < 20$
            \item Założenie indukcyjne:
                \[
                    \forall k \leq n: T(k) \leq c k
                \]
            \item Krok indukcyjny:
                \begin{equation*}\begin{aligned}
                    T(n) &\leq T\left(\frac{n}{5}\right) + T\left(\frac{3n}{4}\right) + \Theta(n) \\
                         &\leq c \cdot \frac{n}{5} + c \cdot \frac{3n}{4} + \Theta(n) \\
                         &= \frac{c}{5}n + \frac{3c}{4}n + \Theta(n) \\
                         &\leq \left(\frac{c}{5} + \frac{3c}{4} + \Theta(1)\right)n \\
                         &= \left(\frac{4c + 15c}{20} + \Theta(1)\right)n \\
                         &= \left(\frac{19c}{20} + \Theta(1)\right)n \\
                \end{aligned}\end{equation*}
                Dla naszego wybranego, dużego $c$ możemy przyjąć, że $\frac{19c}{20} + \Theta(1) \leq c$, zatem
                \[
                    T(n) \leq c n
                \]
                \qed
        \end{itemize}
\end{itemize}

\section{Wykład \date{2025-04-07}}
\subsection{Set Interface}
Zakładamy, że każda struktura ma pole nazwane kluczem $a \in A \rightarrow \exists 1 \geq a.key \geq \text{length()}$ , które jest unikalne dla każdego elementu. Klucz jest liczbą całkowitą, a jego wartość jest niezmienna. Wartości kluczy są różne dla różnych elementów, a więc $a.key \neq b.key$ dla $a \neq b$.\\
Do budowy zbiorów używamy \textit{Set Interface}, który definiuje następujące operacje:
\begin{itemize}
    \item build($A$) - buduje ``set'' z danych zawartych w tablicy $A$
    \item find($k$) - zwraca element o kluczu $k$ z ``setu''
    \item length() - zwraca liczbę elementów w ``secie''
    \item insert($a$) - dodaje element $a$ do ``setu''
    \item delete($a$) - usuwa element $a$ z ``setu''
    \item find\_min() - zwraca element o najmniejszym kluczu
    \item find\_max() - zwraca element o największym kluczu
    \item find\_next($k$) - zwraca element o kluczu większym od $k$
    \item find\_prev($k$) - zwraca element o kluczu mniejszym od $k$
    \item order() - zwraca elementy w ``secie'' w kolejności rosnącej kluczy
\end{itemize}
Zobaczmy sobie kilka przykładow asymptotyki operacji dla różnych struktur danych:

\begin{center}
\begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}X|%
                                >{\centering\arraybackslash}X|%
                                >{\centering\arraybackslash}X|%
                                >{\centering\arraybackslash}X|%
                                >{\centering\arraybackslash}X|%
                                >{\centering\arraybackslash}X|%
                                >{\centering\arraybackslash}X|}
    \hline
    \textbf{Struktura} & \textbf{build} & \textbf{find} & \textbf{input, delete} & \textbf{find\_max find\_min} & \textbf{find\_next find\_prev} & \textbf{order()} \\ \hline
    unsorted array & $\Theta(n)$ & $\Theta(n)$ & $\Theta(n)\footnote{nie $\Theta(1)$ ponieważ potrzeba czasu na realokacje pamięci itp.}$ & $\Theta(n)$ & $\Theta(n)$ & $\Theta(n \log n)$ \\ \hline
    sorted array & $\Theta(n \log n)$ & $\Theta(\log n)$ & $\Theta(n)$ & $\Theta(1)$ & $\Theta(\log n)$ & $\Theta(n)$ \\ \hline
    unsorted linked list & $\Theta(n)$ & $\Theta(n)$ & $\Theta(1), \Theta(n)\footnote{należy znaleść element do usunięcia, samo usuwanie $\Theta(1)$}$ & $\Theta(n)$ & $\Theta(n)$ & $\Theta(n \log n)$ \\ \hline
    BST & $\Theta(n)$ & -- & -- & -- & -- & -- \\ \hline
    Direct Access Array & $\Theta(K)$ & $\Theta(1)$ & $\Theta(1)$ & $\Theta(K)$ & $\Theta(K)$ & $\Theta(K)$ \\ \hline
\end{tabularx}
\end{center}
\subsubsection{Binary Search Tree -- BST}
Po polsku drzewo przeszukiwań binarnych. Struktura ta zawiera następujące pola:
\begin{itemize}
    \item parent -- wskaźnik na rodzica
    \item left -- wskaźnik na lewe dziecko
    \item right -- wskaźnik na prawe dziecko
    \item key -- klucz
    \item ... -- inne pola
\end{itemize}
Drzewa BST mają następujące właściwości, niech $T$ będzie drzewem BST, $x \in T$ -- $x$ jest węzłem drzewa T
\begin{itemize}
    \item każdy $y \in x.left$ ma klucz mniejszy od klucza $x$
    \item każdy $y \in x.right$ ma klucz większy od klucza $x$
\end{itemize}

Przykład drzewa BST:
\begin{center}
    \begin{tikzpicture}[
        level distance=1.5cm,
        every node/.style={circle, draw},
        level 1/.style={sibling distance=2cm},
        level 2/.style={sibling distance=1cm},
        level 3/.style={sibling distance=0.5cm}
        ]
        \node {10}
            child {node {5}
                child {node {3}}
            child {node {7}}}
            child {node {15}
                child {node {12}}
            child {node {20}}};
    \end{tikzpicture}
\end{center}
\subsubsection{Operacje na BST}
\begin{itemize}
    \item \textbf{InorderTreeWalk}: operacja do wypisania wszystkich elementów w drzewie w kolejności rosnącej kluczy. Algorytm działa rekurencyjnie, odwiedzając najpierw lewe poddrzewo, następnie węzeł, a na końcu prawe poddrzewo. \\
        \begin{algorithm}
            \caption{InorderTreeWalk}
            \begin{algorithmic}[1]
                \Procedure{InorderTreeWalk}{x}
                \If{$x \neq NIL$}
                    \State InorderTreeWalk($x.left$)
                    \State print($x.key$)
                    \State InorderTreeWalk($x.right$)
                \EndIf
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
        Na powyższym przykładzie wywołanie \texttt{InorderTreeWalk($T$)} wypisze 3, 5, 7, 10, 12, 15, 20. \\
        \textbf{Asymptotyka}: w zależności od rozmiaru lewego poddrzewa -- $k$, algorytm spełnia rekurencję:
        \[
            T(n) = T(k) + \Theta(1) + T(n-1-k)
        \]
        Rozwiążemy to rekurencyjnie:
        \begin{itemize}
            \item założenie indukcyjne: $\forall k < n: T(k) \leq ck$
            \item krok indykcyjny:
                \begin{equation*}\begin{aligned}
                    T(n) &= T(j) + \Theta(1) + T(n-1-j)\\
                         &\leq cj + \Theta(1) +c(n-1-j) \\
                         &= cn -c +\Theta(1) \\
                         &\leq cn
                \end{aligned}\end{equation*}
                Zatem asymptotyka algorytmu to $T(n) = \Theta(n)$
        \end{itemize}

    \item \textbf{TreeSearch}: operacja do wyszukiwania elementu w drzewie. Algorytm działa rekurencyjnie, porównując klucz szukanego elementu z kluczem węzła. Jeśli klucz jest mniejszy, algorytm przeszukuje lewe poddrzewo, jeśli większy -- prawe poddrzewo. \\
        \begin{algorithm}
            \caption{TreeSearch}
            \begin{algorithmic}[1]
                \Procedure{TreeSearch}{x, k}
                \If{$x = null$ or $k = x.key$}
                    \State \textbf{return} $x$
                \ElsIf{$k < x.key$}
                    \State \textbf{return} TreeSearch($x.left$, $k$)
                \Else
                    \State \textbf{return} TreeSearch($x.right$, $k$)
                \EndIf
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
        Na powyższym przykładzie wywołanie \texttt{TreeSearch($T$, 12)} zwróci węzeł o kluczu 12.\\
        \textbf{Asymptotyka}: złożoność algorytmu jest zależna jedynie od wysokości drzewa, a więc:
        \[
            T(n) = O(h)
        \]
    \item \textbf{TreeMin, TreeMax} operacje proste idące jedynie w lewo lub w prawo, odpowiednio. Na przykład
        \begin{algorithm}
            \caption{TreeMin}
            \begin{algorithmic}[1]
                \Procedure{TreeMin}{x}
                \While{$x.left \neq null$}
                    \State $x = x.left$
                \EndWhile
                \State \textbf{return} $x$
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
        Algorytm działa iteracyjnie, przeszukując lewe poddrzewo.\\
        Złożoność algorytmu to $\Theta(h)$, analogicznie jak w przypadku \texttt{TreeMax}.
    \item \textbf{TreeSuccessor} zwraca następnika węzła $x$ w drzewie BST. Algorytm działa rekurencyjnie, porównując klucz węzła z kluczem $x$. Jeśli klucz jest mniejszy, algorytm przeszukuje lewe poddrzewo, jeśli większy -- prawe poddrzewo. \\
        \begin{algorithm}
           \caption{TreeSuccessor}
           \begin{algorithmic}[1]
               \Procedure{TreeSuccessor}{x}
               \If{$x.right \neq null$}
               \State \textbf{return} TreeMin($x.right$)
               \Else
               \State $y = x.parent$
               \While{$y \neq null$ and $x = y.right$}
               \State $x = y$
               \State $y = y.parent$
               \EndWhile
               \State \textbf{return} $y$
               \EndIf
               \EndProcedure
           \end{algorithmic}
        \end{algorithm}
        Złożoność algorytmu to $\Theta(h)$.
    \item \textbf{BST-Inset}: operacja do dodawania elementu do drzewa BST. Algorytm działa rekurencyjnie, porównując klucz nowego elementu z kluczem węzła. Jeśli klucz jest mniejszy, algorytm przeszukuje lewe poddrzewo, jeśli większy -- prawe poddrzewo.
        \begin{algorithm}
            \caption{BST-Insert}
            \begin{algorithmic}[1]
                \Procedure{BST-Insert}{T, z}
                \State $y = null$
                \State $x = T.root$
                \While{$x \neq null$}
                    \State $y = x$
                    \If{$z.key < x.key$}
                        \State $x = x.left$
                    \Else
                        \State $x = x.right$
                    \EndIf
                \EndWhile
                $z.parent = y$
                \If{$y = null$}
                    $T.root = z$
                \ElsIf{$z.key < y.key$}
                    $y.left = z$
                \Else
                    $y.right = z$
                \EndIf
                $z.left = null$
                $z.right = null$
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item \textbf{BST-Delete}: operacja do usuwania elementu z drzewa BST. Algorytm działa rekurencyjnie, porównując klucz usuwanego elementu z kluczem węzła. Jeśli klucz jest mniejszy, algorytm przeszukuje lewe poddrzewo, jeśli większy -- prawe poddrzewo.
        \begin{enumerate}
            \item $x$ jest liściem $\rightarrow$ zwolnij pamięć zajmowaną przez $x$, ustaw wskaźnik jego ojca na null
            \item $x$ ma jedno poddrzewo $\rightarrow$, czyli ma jednego syna $v$, to zamienić pamięć z $x$ na $v$, ustawić wskaźnik ojca $x$ na $v$, a wskaźnik ojca $v.p$ na $x.p$
            \item $x$ ma dwa poddrzewa $\rightarrow$ znajdź następnika $x$ -- $y$ za pomocą \texttt{TreeSuccessor}, zamień pamięć z $x$ na $y$, a następnie usuń $y$ z drzewa.
        \end{enumerate}
        \begin{algorithm}
            \caption{BST-Delete}
            \begin{algorithmic}[1]
                \Procedure{BST-Delete}{T, z}
                \If{$z.left = null$}
                    $y = z.right$
                \ElsIf{$z.right = null$}
                    $y = z.left$
                \Else
                    $y = TreeSuccessor(z)$
                    $z.key = y.key$
                    $y = y.right$
                \EndIf
                $y.parent = z.parent$
                \If{$z.parent = null$}
                    $T.root = y$
                \ElsIf{$z = z.parent.left$}
                    $z.parent.left = y$
                \Else
                    $z.parent.right = y$
                    \EndIf
                    \EndProcedure
                \end{algorithmic}
            \end{algorithm}
            Złożoność algorytmu to $\Theta(h)$.
        \item \textbf{BST-Sort}: operacja do sortowania elementów w drzewie BST. Algorytm działa rekurencyjnie, odwiedzając najpierw lewe poddrzewo, następnie węzeł, a na końcu prawe poddrzewo.

            \begin{algorithm}
                \caption{BST-Sort}
                \begin{algorithmic}[1]
                    \Procedure{BST-Sort}{T}
                    \State InorderTreeWalk($T.root$)
                    \EndProcedure
                \end{algorithmic}
            \end{algorithm}

            Złożoność algorytmu to $\Theta(n)$.

            Na przykładzie wygląda to następująco:

            Teraz przedstawiamy drzewo BST krok po kroku przy wstawianiu elementów z tablicy:
        \begin{center}
            \textbf{Krok 1: Wstawienie 3}\\[0.5ex]
            \begin{tikzpicture}[every node/.style={circle, draw}, level distance=1.5cm, sibling distance=2cm]
                \node {3};
            \end{tikzpicture}
            \bigskip

            \textbf{Krok 2: Wstawienie 7}\\[0.5ex]
            \begin{tikzpicture}[every node/.style={circle, draw}, level distance=1.5cm, sibling distance=2cm]
                \node {3}
                    child[missing] % brak lewego potomka
                    child { node {7} };
            \end{tikzpicture}
            \bigskip

            \textbf{Krok 3: Wstawienie 5}\\[0.5ex]
            \begin{tikzpicture}[every node/.style={circle, draw}, level distance=1.5cm, sibling distance=2cm]
                \node {3}
                    child[missing]
                    child { node {7}
                        child { node {5} }
                        child[missing]
                    };
            \end{tikzpicture}
            \bigskip

            \textbf{Krok 4: Wstawienie 6}\\[0.5ex]
            \begin{tikzpicture}[every node/.style={circle, draw}, level distance=1.5cm, sibling distance=2cm]
                \node {3}
                    child[missing]
                    child { node {7}
                        child {
                            node {5}
                            child[missing]
                            child { node {6} }
                        }
                        child[missing]
                    };
            \end{tikzpicture}
            \bigskip

            \textbf{Krok 5: Wstawienie 8}\\[0.5ex]
            \begin{tikzpicture}[every node/.style={circle, draw}, level distance=1.5cm, sibling distance=2cm]
                \node {3}
                    child[missing]
                    child { node {7}
                        child {
                            node {5}
                            child[missing]
                            child { node {6} }
                        }
                        child { node {8} }
                    };
            \end{tikzpicture}
            \bigskip

            \textbf{Krok 6: Wstawienie 1}\\[0.5ex]
            \begin{tikzpicture}[every node/.style={circle, draw}, level distance=1.5cm, sibling distance=2cm]
                \node {3}
                    child { node {1} }
                    child { node {7}
                        child {
                            node {5}
                            child[missing]
                            child { node {6} }
                        }
                        child { node {8} }
                    };
            \end{tikzpicture}
            \bigskip

            \textbf{Krok 7: Wstawienie 2}\\[0.5ex]
            \begin{tikzpicture}[every node/.style={circle, draw}, level distance=1.5cm, sibling distance=2cm]
                \node {3}
                    child { node {1}
                        child[missing]
                        child { node {2} }
                    }
                    child { node {7}
                        child {
                            node {5}
                            child[missing]
                            child { node {6} }
                        }
                        child { node {8} }
                    };
            \end{tikzpicture}
        \end{center}

        W powyższych diagramach:
        \begin{itemize}
            \item \textbf{Krok 1:} Początkowy węzeł, w którym wstawiono 3.
            \item \textbf{Krok 2:} 7 zostaje wstawione jako prawy syn węzła 3.
            \item \textbf{Krok 3:} 5 wstawiono jako lewy syn węzła 7.
            \item \textbf{Krok 4:} 6 wstawiono jako prawy syn węzła 5.
            \item \textbf{Krok 5:} 8 zostaje dodane jako prawy syn węzła 7.
            \item \textbf{Krok 6:} 1 wstawione jako lewy syn węzła 3.
            \item \textbf{Krok 7:} 2 wstawiono jako prawy syn węzła 1.
        \end{itemize}

        Tak powstaje ostatecznie drzewo BST:
        \[
            \begin{array}{c}
                \begin{tikzpicture}[every node/.style={circle, draw}, level distance=1.5cm, sibling distance=2cm]
                    \node {3}
                    child { node {1}
                        child[missing]
                        child { node {2} }
                    }
                    child { node {7}
                        child {
                            node {5}
                            child[missing]
                            child { node {6} }
                        }
                        child { node {8} }
                    };
                \end{tikzpicture}
            \end{array}
        \]
        Porównując algorytm \texttt{BST-Sort} z \texttt{QuickSort} można zauważyć, że
        \[
            \mathbb{E}\left[\text{Time}(\text{BST-Sort})\right] = \mathbb{E}\left[\text{Time}(\text{QuickSort})\right] = \Theta(n \log n)
        \]
    Gdzie:
    \[
        \text{Time}(\text{BST-Sort}) = \sum_{x \in T} \text{depth}(x)
    \]
    A więc:
    \[
        \mathbb{E}\left[\sum_{x \in T} \text{depth}(x)\right] = \sum_{x \in T} \mathbb{E}\left[\text{depth}(x)\right] = \Theta(n \log n)
    \]
    Dzieląc obustronnie przez $n$ otrzymujemy:
    \settowidth{\exprwidth}{$\left[\frac{1}{n} \sum_{x \in T} \text{depth}(x)\right]$}

    \[
        \mathbb{E}\underbrace{\left[\frac{1}{n} \sum_{x \in T} \text{depth}(x)\right]}
        _{\parbox{\exprwidth}{\centering \scriptsize średnia głębokość\\ węzła w losowym drzewie BST}} = \Theta(\log n)
    \]
\end{itemize}

\subsubsection{Wysokość drzewa BST}
\begin{itemize}
    \item \textbf{Worst Case}: W najgorszym przypadku każdy węzeł drzewa ma tylko jedno dziecko, co prowadzi do powstania struktury liniowej (podobnej do listy). W takim przypadku wysokość drzewa wynosi $h = O(n)$. Przykład:
        \begin{center}
            \begin{tikzpicture}[
                level distance=1.5cm,
                every node/.style={circle, draw},
                level 1/.style={sibling distance=2cm},
                level 2/.style={sibling distance=1cm},
                level 3/.style={sibling distance=0.5cm}
                ]
                \node {10}
                    child {node {5}
                        child {node {3}
                            child {node {2}}
                        }
                    };
            \end{tikzpicture}
        \end{center}
    \item \textbf{Best Case}: Mówimy, że drzewo jest zbalansowane jeśli jego wysokość jest rzędu $O(\log n)$. W takim przypadku każdy węzeł ma co najwyżej dwóch potomków, a drzewo jest zbalansowane. Przykład:
        \begin{center}
            \begin{tikzpicture}[
                level distance=1.5cm,
                every node/.style={circle, draw},
                level 1/.style={sibling distance=2cm},
                level 2/.style={sibling distance=1cm},
                level 3/.style={sibling distance=0.5cm}
                ]
                \node {10}
                    child {node {5}
                        child {node {3}}
                        child {node {7}}}
                    child {node {15}
                        child {node {12}}
                        child {node {20}}};
            \end{tikzpicture}
        \end{center}
\end{itemize}

\subsubsection*{Twierdzenie}
Niech $T$ będzie losowym drzewem BST o $n$ węzłach. Wtedy:
\[
    \mathbb{E}\left[h(T)\right] \leq 3\log_2 n + o(\log n)
\]
\begin{proof}
    \begin{enumerate}
        \item \textbf{Nierówność Jensena}: Niech $f$-wypukła funkcja, wtedy
            \[
                f\left(\mathbb{E}\left[X\right]\right) \leq \mathbb{E}\left[f(X)\right]
            \]
        \item Zamiast analizować zmienną losową $h(T)$ (oznaczmy $H_n$ jako wysokość drzewa BST o n węzłach) będziemy się zajmować zmienną $Y_n = 2^{H_n}$.
        \item Pokażemy, że $\mathbb{E}[Y_n] = O(n^3)$

            \begin{center}
                \begin{tikzpicture}[scale=1.2]
   % Draw x and y axes
                    \draw[->] (-0.5, 0) -- (2.7, 0) node[right] {$x$};
                    \draw[->] (0, -0.5) -- (0,5) node[above] {$f(x)$};

   % Plot the convex function f(x)=x^2 in blue
                    \draw[domain=-0.5:2.5, smooth, variable=\x, blue, thick] plot (\x, {\x*\x});

   % Draw the chord connecting the points f(0)=0 and f(2)=4 in red
                    \draw[red, thick] (0,0) -- (2,4);

   % Mark points on the function:
                    \filldraw [black] (0,0) circle (2pt) node[below left] {$(x_1, f(x_1))$};
                    \filldraw [black] (2,4) circle (2pt) node[above right] {$(x_2, f(x_2))$};

   % Mark the midpoint corresponding to ( (x1+x2)/2, f((x1+x2)/2) ):
                    \filldraw [black] (1,1) circle (2pt) node[below right] {$f\Big(\frac{x_1+x_2}{2}\Big)$};

   % Draw a dashed vertical line from the graph point at (1,1) upward to the chord:
                    \draw[dashed] (1,1) -- (1,2);

   % Mark the corresponding point on the chord, which is (1, (f(x1)+f(x2))/2):
                    \filldraw [black] (1,2) circle (2pt) node[right] {$(\frac{x_1+x_2}{2},\,\frac{f(x_1)+f(x_2)}{2})$};

   % Add a label for Jensen's inequality on the right side
                    \node at (2.8,2.5) {$f\Big(\frac{x_1+x_2}{2}\Big) \leq \frac{f(x_1)+f(x_2)}{2}$};
                \end{tikzpicture}
            \end{center}
            Warunkująć, że korzeń $r$ tworzy $(k-1, n-k)$-split mamy
            \[
                H_n = 1 + \max\{H_{k-1}, H_{n-k}\}
            \]
            Zatem
            \[
                Y_n = 2\max\{Y_{k-1}, Y_{n-k}\}
            \]
            Niech $Z$ będzie zmienną indykatorową, zdefiniowaną w następujący sposób:
            \[
                Z_{n,k} = \begin{cases}
                    1, & \text{jeżeli $r$ wykonuje } (k-1, n-k)\text{-split} \\
                    0, & \text{w przeciwnym przypadku}
                \end{cases}
            \]
            Wtedy
            \[
                Y_n = \sum_{k=1}^n Z_{n,k} \cdot 2 \max\{Y_{k-1}, Y_{n-k}\}
            \]
            Nakładając wartość oczekiwaną na obie strony, mamy:
            \[
                \mathbb{E}[Y_n] = \mathbb{E}\left[ \sum_{k=1}^n Z_{n,k} \cdot 2 \max\{Y_{k-1}, Y_{n-k}\} \right]
            \]
            Zmienne $Z_{n,k}$ oraz $max\{Y_{k-1}, Y_{n-k}\}$ są niezależne na podstawie podobnego argumentu, jak przy QuickSorcie, a więc
            \[
                \mathbb{E}[Y_n] = 2 \sum_{k=1}^n \mathbb{E}[Z_{n,k}] \cdot \mathbb{E}\left[\max\{Y_{k-1}, Y_{n-k}\}\right]
            \]
            Rozważmy teraz wartość oczekiwaną $Z_{n,k}$:
            \[
                \mathbb{E}[Z_{n,k}] = 1 \cdot P(Z_{n,k} = 1) + 0 \cdot P(Z_{n,k} = 0) = P(Z_{n,k} = 1) = \frac{(n-1)!}{n!} = \frac{1}{n}
            \]
            A więc
            \[
                \mathbb{E}[Y_n] = \frac{2}{n} \sum_{k=1}^n \mathbb{E}\left[\max\{Y_{k-1}, Y_{n-k}\}\right]
            \]
            Można to ograniczyć z góry przez (tracimy zdecydowanie mniej, rzędu stałej, jeżeli robimy to ograniczenie na $Y_n$, nie na $H_n$):
            \[
                \mathbb{E}[Y_n] \leq \frac{2}{n} \sum_{k=1}^n \left(\mathbb{E}[Y_{k-1}] + \mathbb{E}[Y_{n-k}]\right) =
            \]
            \[
                = \frac{2}{n} \sum_{k=1}^n \mathbb{E}[Y_{k-1}] + \frac{2}{n} \sum_{k=1}^n \mathbb{E}[Y_{n-k}]
            \]
            Zauwazmy, że są to te same sumy liczone w przeciwnej kolejności:
            \[
                \mathbb{E}[Y_n] \leq \frac{4}{n} \sum_{k=0}^{n-1} \mathbb{E}[Y_k]
            \]
            Przyjmijmy oznaczenie $y_n = \mathbb{E}[Y_n]$.
            \[
                y_n \leq \frac{4}{n} \sum_{k=0}^{n-1} y_k
            \]
            Udowodnimy powyższe przy użyciu indukcji:
            \begin{itemize}
                \item Base Case: $y_0=y_1=0$
                \item założenie indukcyjne:
                    \[
                        \forall k < n: y_k \leq cn^3
                    \]
                \item krok indukcyjny:
                    \[
                        y_n \leq \frac{4}{n} \sum_{k=0}^{n-1} y_k \leq \frac{4}{n} \sum_{k=0}^{n-1} c k^3 = \frac{4c}{n} \sum_{k=0}^{n-1} k^3
                    \]
                    Ograniczmy to z góry przez całkę:
                    \[
                        y_n \leq \frac{4c}{n} \int_0^{n} x^3 dx = \frac{4c}{n} \cdot \frac{n^4}{4} = cn^3
                    \]
            \end{itemize}
            Zatem z definicji mamy:
            \[
                y_n = O(n^3)
            \]
            A więc cofając nasze oznaczenie:
            \[
                \mathbb{E}[Y_n] = O(n^3)
            \]
        \item Zauważmy, że
            \[
                2^{\mathbb{E}[H_n]} \leq \mathbb{E}[2^{H_n}] = \mathbb{E}[Y_n] = O(n^3)
            \]
            Jeżeli weźmiemy logarytm z obu stron to:
            \[
                E[H_n] = 3 \log_2 n + o(\log n)
            \]
\end{enumerate}
\end{proof}
Dokładny wynik pokazany przez Luc Devroye w 1986 roku:
\[
    \mathbb{E}[H_n] = 2.9882 \log_2 n
\]
Czyli ograniczając maksimum z góry przy użyciu sumy tracimy nie wiele.


\section{Red-Black Trees (RB)}
Na dzisiejszym wykładzie zaczynamy temat drzew czerwono-czarnych. Drzewa czerwono-czarne są to zbalansowane drzewa BST, które zapewniają, że wysokość drzewa jest ograniczona przez $2 \cdot \log_2 n$. Dzięki temu operacje na drzewach czerwono-czarnych mają złożoność $\Theta(\log n)$.
\subsection{Własności}
\begin{itemize}
    \item BST
    \item Każdy węzeł ma kolor czerwony lub czarny
    \item czerwony węzeł nie może mieć czerwonego rodzica
    \item $\forall x$ każda prosta ścieżka od węzła $x$ ma tyle samo czarnych węzłów: (blackHeight($x$), redHeight($x$))
\end{itemize}
Przykład:
\begin{center}
    \begin{tikzpicture}[
    level distance=1.5cm,                % Vertical distance between levels
    level 1/.style={sibling distance=2cm}, % Distance between children of the root
    level 2/.style={sibling distance=3cm}, % Increased distance for level 2 nodes
    level 3/.style={sibling distance=4cm},% Even greater distance for level 3 nodes
    every node/.style={circle, draw, minimum size=1cm, inner sep=0pt}
]

% Constructing the red-black tree with the desired nodes:
% Color assignment:
% 7 (root) is black
% 3 is black
% 18 is red
% 10 and 21 are black (children of red node 18)
% 8, 11, and 25 are red
\node [fill=black!70, text=white] {7}
    child { node [fill=black!70, text=white] {3} }
    child { node [fill=red!70, text=white] {18}
        child { node [fill=black!70, text=white] {10}
            child { node [fill=red!70, text=white] {8} }
            child { node [fill=red!70, text=white] {11} }
        }
        child { node [fill=black!70, text=white] {21}
            child [missing] {}  % A dummy child to keep symmetry (21 has only a right child)
            child { node [fill=red!70, text=white] {25} }
        }
    };
\end{tikzpicture}
\end{center}
\subsubsection*{Lemat}
Niech $T$ będzie drzewem czerwono-czarnym o $n$ węzłach. Wtedy:
\[
    \text{hight}(T) = 2\log_2(n+1)
\]
\begin{proof}
    Przeprowadzimy dowód w ten sposób. Niech $T$--dowolne drzewo czerwono-czarne. Czarne węzły ``wchłaniają'' czerwone węzły, rozważmy to na przykładzie drzewa:
    \begin{center}
    \begin{tikzpicture}[
    level distance=2cm,                % Vertical distance between levels
    level 1/.style={sibling distance=5cm},  % Horizontal distance between the children
    every node/.style={
        draw,
        rectangle,
        minimum height=1cm,
        minimum width=2.5cm,
        align=center,
        font=\sffamily
    }
]

% Root node (a 3-node with two keys: 7 and 18)
\node {7 \quad | \quad 18}
    % First child: a 2-node leaf with key 3
    child { node {3} }
    % Second child: a 4-node leaf with keys 8, 10, 11
    child { node {8 \quad | \quad 10 \quad | \quad 11} }
    % Third child: a 3-node leaf with keys 21 and 25
    child { node {21 \quad | \quad 25} };

\end{tikzpicture}
\end{center}

    Można zaobserwować, że nie zmienia się liczba liści drzewa. Takie drzewo nazywamy \textit{2-3-4 Tree} jest ona równa $n+1$
    Rozważmy wyskość tych drzew. Niech $h$--wysokość RBT, $h'$--wysokość 2-3-4 Tree. Wtedy
    \[
        h'=2h
    \]
    \[
        2^{h'} \leq n+1 \leq 4^{h'}
    \]
    biorąc logarytm:
    \[
        h' \leq \log_2(n+1) \leq 2h'
    \]
    Zbierając wszystko razem:
    \[
        h\leq 2\log_2(n+1)
    \]
\end{proof}
\subsection{Operacje}
\subsubsection{RB-Insert}
\begin{enumerate}
    \item Wstaw element $z$ do drzewa jako do BST
    \item ustaw kolor wstawianego noda na \textcolor{red}{czerwony}
    \item FixUp
        \begin{enumerate}
            \item $z$--\textcolor{red}{czerwony}, $z.parent$--czarny:
                \begin{enumerate}
                    \item recolor
                        %TODO nie wiem pojawia sie tutaj narysowany przyklad i dziala proponuje to ponownie przenalizowac
                \end{enumerate}
            \item $z$--\textcolor{red}{czerwony}, $z.parent$--\textcolor{red}{czerwony}, wujek--czarny, czyli tak zwany \textit{zig-zag}
                \begin{enumerate}
                    \item rotate
                    \item recolor
                \end{enumerate}
            \item $z$--\textcolor{red}{czerwony}, $z.parent$--\textcolor{red}{czerwony}, wujek--czarny, ale prosta linia

                \begin{enumerate}
                    \item rotate na $x$ i dziadku
                    \item recolor
                \end{enumerate}
        \end{enumerate}
\end{enumerate}
Każdy przypadek naprawy wykonuje się w $\Theta(1)$. Kiedy naprawiamy drzewo przypadki mogą się wykonać następująco
%Draw a use case as a flowchart where case 1 loops to itself and to case 2, case 3, case 2 goes to case 3 and case 3 terminates
\begin{center}
    \begin{tikzpicture}[node distance=2.5cm, auto, thick, >=Stealth]
    % Define nodes
    \node[draw, rectangle, rounded corners, minimum width=2cm, minimum height=1cm] (case1) {Case 1};
    \node[draw, rectangle, rounded corners, minimum width=2cm, minimum height=1cm, below left=of case1] (case2) {Case 2};
    \node[draw, rectangle, rounded corners, minimum width=2cm, minimum height=1cm, below right=of case1] (case3) {Case 3};
    \node[draw, ellipse, minimum width=2cm, minimum height=1cm, below=of case3] (term) {Terminate};

    % Draw arrows from Case 1
    % Self-loop on Case 1
    \draw[->] (case1.north) .. controls +(up:1cm) and +(left:1cm) .. (case1.west) node[midway, above, sloped] {Loop};

    % Arrow from Case 1 to Case 2
    \draw[->] (case1.south west) -- (case2.north);

    % Arrow from Case 1 to Case 3
    \draw[->] (case1.south east) -- (case3.north);

    % Arrow from Case 2 to Case 3
    \draw[->] (case2.east) -- (case3.west);

    % Arrow from Case 3 to Terminate (showing termination)
    \draw[->] (case3.south) -- (term.north);
\end{tikzpicture}
\end{center}
\subsubsection{Operacje używane w FixUp}
\begin{enumerate}
    \item recolor $\rightarrow$ zmień kolor węzła $\Theta(1)$
    \item rotate $\rightarrow$ zobaczmy na przykładzie:
        \bigskip
        \\
        \begin{minipage}{0.45\linewidth}
            \centering
            \textbf{Before Right Rotation}\\[5mm]
            \begin{tikzpicture}[
                level distance=1.5cm,
                sibling distance=2.5cm,
                every node/.style={draw, circle, inner sep=2pt, minimum size=10mm}
                ]
    % Original Tree:
    %        A
    %       / \
    %      B   γ
    %     / \
    %    α   β
                \node {A}
                    child {node {B}
                        child {node {$\alpha$}}
                        child {node {$\beta$}}
                    }
                    child {node {$\gamma$}};
            \end{tikzpicture}
        \end{minipage}
        \hfill
        \begin{minipage}{0.45\linewidth}
            \centering
            \textbf{After Right Rotation}\\[5mm]
            \begin{tikzpicture}[
                level distance=1.5cm,
                sibling distance=2.5cm,
                every node/.style={draw, circle, inner sep=2pt, minimum size=10mm}
                ]
    % Rotated Tree:
    %         B
    %        / \
    %      α     A
    %           / \
    %         β    γ
                \node {B}
                    child {node {$\alpha$}}
                    child {node {A}
                        child {node {$\beta$}}
                        child {node {$\gamma$}}
                    };
            \end{tikzpicture}
        \end{minipage}
        \bigskip
        Złożoność operacji to $\Theta(1)$
\end{enumerate}

\subsubsection{RB-Delete}

\section{Wykład \date{2025-04-28}}
\subsection{Direct Access Array}
\begin{itemize}
    \item klucze należą do zbioru $\left{0,\dots,k-1\right}$, gdzie $k$--rozmiar tablicy
    \item klucze będziemy utożsamiać z adresami w pamięci komputera
    \item operujemy w ramach pamięci WORD-Ram
\end{itemize}
Jednakże występują pewne ograniczenia:
\begin{itemize}
    \item adresy są 64-bitowe $\leftrightarrow$ $K\leq 2^{64}$
    \item każdy obiekt musi mieć unikatowy klucz. Weźmy na przykład baze danych z numerami PESEL.
\end{itemize}
\subsection{Hash Tables}
Jest to pewne ulepszenie DAA. Metoda ta polega na przypisaniu klucza do adresu w tablicy. Wykorzystuje się do tego funkcję haszującą, która przekształca klucz na adres w tablicy. Funkcja haszująca powinna być:
    \[
        h: \{0,\dots,k-1\} \rightarrow \{0,\dots,m-1\}
    \]
gdzie $m$--rozmiar tablicy oraz $m \ll k$. \\
Ponieważ $k \gg m$, to $h$ nie może być różnowartościowa:
\[
    \exists i \neq j: h(i) = h(j)
\]
Chcemy, aby $h$ zwracała wyniki z rozkładu jednostajnego na zbiorze $\{0,\dots,m-1\}$.
Zobaczmy sobie kilka przykładów funkcji haszujących:
\begin{itemize}
    \item Division hash function\footnote{jednym z problemów tej funkcji jest to, że nie jest ona jednostajna.}
        \[
            h(k) = k \mod m
        \]
    \item Counter, Wegmann 1977 Universal hash function:
        \[
            h_{a,b}(k) = ((ak + b) \mod p) \mod m
        \]
        gdzie $p$--liczba pierwsza, $a,b$--liczby losowe z przedziału $[1,p-1]$, $p > k$.\\
        Zdefinujmy sobie następujący zbiór funkcji haszujacych
        \[
            \mathcal{H}(p,m) = \left\{h_{a,b}: a,b \in \{0,\dots,p-1\}\right\}
        \]
\end{itemize}

\subsubsection{Universal Hash Property}
\subsubsection*{Twierdzenie}
\[
    \forall k_i \neq k_j \in \{0,\dots, k-1\}:P_{h\in\mathcal{H}}\left(h(k_i) = h(k_j)\right) \leq \frac{1}{m}
\]
\begin{proof}
    Zakładamy, że $h_{a,b}$ ma universal hashing property, wtedy
    \[
        h_{a,b}(x) = h_{a,b}(y), x \neq y
    \]
    a więc
    \[
        ax + b \equiv ay + b \mod p
    \]
    Skoro $p > k, x \neq y \implies x-y\neq 0$ oraz ma odwrotność $\mod p$, zatem
    \[
        a \equiv i\cdot m (x-y)^{-1} \mod p
    \]
    z czego wynika, że występuje $\left\lfloor \frac{p-1}{m}\right\rfloor$ możliwości.
    \[
        P_{h\in\mathcal{H}}\left(h(k_i) = h(k_j)\right) \leq \frac{1}{p-1} \cdot \left\lfloor \frac{p-1}{m}\right\rfloor \leq \frac{1}{m}
    \]
\end{proof}

\subsubsection{Wartość oczekiwana kolizji}
Niech $n$-liczba haszowanych elementów, $n=\Theta(m)$, zefinujmy zmienną losową
\[
    L = \text{\#kolizji}
\]
Do tego dodajmy zmienną losową indykatorową:
\[
    X_{i,j} = \begin{cases}
        1, & \text{jeżeli } h(k_i) = h(k_j) \\
        0, & \text{w przeciwnym przypadku}
    \end{cases}
\]
wtedy
\[
    L=\sum_{j\geq 1, i \neq j}^{n} X_{i,j}
\]
nakładając wartość oczekiwaną na obie strony:
\[
    \mathbb{E}[L] = \mathbb{E}\left[\sum_{j\geq 1, i \neq j}^{n} X_{i,j}\right] = \sum_{j\geq 1, i \neq j}^{n} \mathbb{E}[X_{i,j}]= \sum_{j\geq 1, i \neq j}^{n} P_{h\in\mathcal{H}}\left(h(k_i) = h(k_j)\right)
\]
Kożystając tutaj z universal hashing property:
\[
    \mathbb{E}[L] \leq \sum_{j\geq 1, i \neq j}^{n} \frac{1}{m} = \frac{n-1}{m} = \Theta(1)
\]

\subsection{Wzbogacona struktura danych}
Rozważmy przykład Dynamicznej statystyki pozycyjnej z zadanymi funkcjami
\begin{itemize}
    \item dynamizcne struktury danych
    \item OS-Select(i) -- zwraca i-tą statystykę pozycyjną
    \item OS-Rank(x) -- zwraca statystykę pozycyjną elementu $x$
\end{itemize}
Zobrazujmy to na przykładzie RB-Tree:
%jebnać tutaj drzewo czerwono-czarne
Wzbogacamy taką strukturę danych o pole \textbf{size}. Zachowuje on kilka własności:
\begin{itemize}
    \item $size(x)$--rozmiar poddrzewa $x$
    \item $size(x) = size(x.left) + size(x.right) + 1$
    \item $size(null)=0$
    \item $OS-Select(i)$--wykonuje się w $\Theta(\log n)$
    \item $OS-Rank(x)$--wykonuje się w $\Theta(\log n)$
\end{itemize}

\subsubsection{Algorytm OS-Select}
\begin{algorithm}
    \caption{OS-Select}
    \begin{algorithmic}[1]
        \Procedure{OS-Select}{x, i}
        \State $k=x.left.size+1$
        \If{$i=k$}
        \State \textbf{return} $x$
        \ElsIf{$i<k$}
        \State \textbf{return} OS-Select($x.left, i$)
        \Else
        \State \textbf{return} OS-Select($x.right, i-k$)
        \EndIf
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\subsubsection{OS-Rank}
\begin{algorithm}
    \caption{OS-Rank}
    \begin{algorithmic}[1]
        \Procedure{OS-Rank}{x}
            \State $r=x.left.size+1$
            \State $y=x$
            \While{$y \neq root$}
            \If{$y=(y.parent).right$}
            \State $r=r+(y.parent).left.size+1$
            \EndIf
            \EndWhile
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
Należy zauważyć, że dynamiczne operacje przeprowadzane na strukturze danych takiej jak RB-Tree da się również zmodyfikować w taki sposób, aby jednocześnie zmieniać pole size. Na Przykładzie RB-Insert:
\begin{enumerate}
    \item BSTinsert -- podczas przechodzenia po drzewie zwiększamy size
    \item fixup -- dalej nie zmieniamy złożonośći poszczególnych funkcji, jedynie zwiększamy stałą w ich złożoności
        \begin{itemize}
            \item recolor -- nie wpływa na size
            \item rotate -- znowu nie wpływa na size w poddrzewach, jednynie rotowanych nodów, ale da się je zmienić w $\Theta(1)$
        \end{itemize}
\end{enumerate}

\subsubsection{Metodologia wzbogacania struktur danych}
\begin{enumerate}
    \item Wybierz strukturę danych, która ma być wzbogacona
    \item Wybrać dodatkową informację, która ma być przechowywana w strukturze
    \item \textbf{Upewnić się}, że dodatkowa informacja nie \textif{``pogorszy asymptotycznej złożoności''} operacji
    \item Zaprojektować dodatkowe operacje na strukturze danych wykorzystując dodatkową informację
\end{enumerate}
\subsubsection{Drzewa Przedziałowe (Interval Trees)}
Teraz na innym przykładzie pokażemy metode wzbogacania danych do drzew przedziałowych. Drzewa przedziałowe są to struktury danych, które przechowują zbiory przedziałów. \\
Rozważmy najpierw przykład. Mamy dane takie odcinki:
%4, 8; 5, 11; 7, 10; 15, 18; 17, 19; 21, 23
\begin{center}
\begin{tikzpicture}[xscale=0.4, yscale=1]
  % Oś liczbowa
    \draw[->] (3,0) -- (24,0) node[right] {$x$};
    \foreach \x in {4,5,7,8,10,11,15,17,18,19,21,23}{
        \draw (\x,0.1) -- (\x,-0.1) node[below=3pt] {\small $\x$};
    }

  % Odcinki (y = poziom linii)
    \draw[thick, blue]  (4,1)  -- (8,1)  node[right] {$(4,8)$};
    \draw[thick, red]   (5,2)  -- (11,2) node[right] {$(5,11)$};
    \draw[thick, green] (7,3)  -- (10,3) node[right] {$(7,10)$};
    \draw[thick, orange](15,4) -- (18,4) node[right] {$(15,18)$};
    \draw[thick, purple] (17,5) -- (19,5) node[right] {$(17,19)$};
    \draw[thick, teal]  (21,6) -- (23,6) node[right] {$(21,23)$};

  % Etykiety poziomów
    \foreach \y/\lbl in {1/{1.},2/{2.},3/{3.},4/{4.},5/{5.},6/{6.}} {
        \node[left] at (3,\y) {\lbl};
    }
\end{tikzpicture}\end{center}
\begin{enumerate}
    \item Wybieramy strukturę danych do przechowywania informacji: \textbf{RB-Tree} $\rightarrow$ kluczem będzie \textif{low} każdego odcinka
    \item dodatkową informacja $m$
        \[
            x.m = \max \begin{cases}
                x.high \\
                x.left.m \\
                x.right.m
            \end{cases}
        \]
        \begin{center}
            \begin{tikzpicture}[node distance=1.5cm and 2cm]
  % Węzły
                \node[blacknode]                    (n5)  {$(5,11;23)$};
                \node[blacknode, below left=of n5] (n4)  {$(4,8;8)$};
                \node[rednode,   below right=of n5](n15) {$(15,18;23)$};
                \node[blacknode, below left=of n15](n7)  {$(7,10;10)$};
                \node[blacknode, below right=of n15](n17){$(17,19;23)$};
                \node[rednode,   below=of n17]      (n21) {$(21,23;23)$};

  % Krawędzie
                \draw[edgestyle] (n5)  -- (n4);
                \draw[edgestyle] (n5)  -- (n15);
                \draw[edgestyle] (n15) -- (n7);
                \draw[edgestyle] (n15) -- (n17);
                \draw[edgestyle] (n17) -- (n21);
            \end{tikzpicture}
        \end{center}
    \item Dodatkowe operacje na drzewie przedziałowym:
        \begin{itemize}
            \item \textbf{Interval-Search} -- zwraca wszystkie odcinki, które przecinają się z danym przedziałem $[a,b]$\\
                \begin{algorithm}
                    \caption{Interval-Search}
                    \begin{algorithmic}[1]
                        \Procedure{Interval-Search}{i, T}
                        \State $x=root(T)$
                        \While {$x=\text{null}\quad \& \quad(i.low > x.high | x.low> i.high)$} \Comment{if TRUE to $i$ nie przecina się z $x$}
                        \If{$x.left \neq \text{null}\quad \&\quad i.low \leq x.left.m$}
                            \State $x=x.left$
                            \Else
                            \State $x=x.right$
                        \EndIf
                        \EndWhile
                        \State \textbf{return} $x$
                    \end{algorithmic}
                \end{algorithm}
                \textbf{Poprawność algorytmu}:\\
                Dla $x \in T$ niech $L=\{j \in x.left\}$ oraz $R=\{j \in x.right\}$.\\
                Jeśli algorytm ``idzie w prawo'', wtedy
                \[
                    \{j \in L: j \text{ przecina } i\}=\emptyset.
                \]
                Jeśli algorytm ``idzie w lewo'', wtedy:
                \[
                    \{j \in L: i \text{ przecina } j\}\neq \emptyset
                \]
                \textbf{lub}
                \[
                    \{j \in R: i \text{ przecina } j\}= \emptyset
                \]
                \begin{proof}
                    załóżmy, że algorytm ``idzie w prawo'' z $x$, wtedy
                    \begin{itemize}
                        \item[$\rightarrow$] $x.left=null \rightarrow L=\emptyset$
                        \item[$\rightarrow$] $x.left \neq null, \quad i.low > x.left.m$
                            \begin{itemize}
                                \item[$\rightarrow$] $\exists j \in L: i.low > j.high (= x.left.m) \implies \{j \in L: i \text{ przecina } j\}=\emptyset$
                            \end{itemize}
                    \end{itemize}
                    Załóżmy teraz, że algorytm ``idzie w lewo'' z $x$, wtedy
                    \begin{itemize}
                        \item[$\rightarrow$] $x.right=null \rightarrow R=\emptyset$
                        \item[$\rightarrow$] $x.right \neq null, \quad i.low \leq x.right.m$
                            \begin{itemize}
                                \item[$\rightarrow$] $\exists j \in R: i.low \leq j.high (= x.right.m) \implies \{j \in R: i \text{ przecina } j\}=\emptyset$
                            \end{itemize}
                    \end{itemize}
                \end{proof}
        \end{itemize}
\end{enumerate}

\section{Wykład \date{2025-05-05}}
Dzisiaj zastępstwo z MG.
\section*{Programowanie Dynaniczne}
Do tej pory poznane metody programowania to:
\begin{itemize}
    \item rekurencyjne
    \item D\&C
\end{itemize}
Do nich dołączy dzisaj programowanie dynamiczne. Na pierwszy rzut oka wygląda ona podobnie do rekurencji. Zobaczmy to na przykładzie
\subsection{Problem znalezienia najdłuższego rosnącego podciągu}
\begin{itemize}
    \item \textbf{Input}: $a_1, a_2 \dots, a_n$--tablica liczb całkowitych
    \item \textbf{Output}: $L$ -- ciąg rosnący
    \item \textbf{Problem}: Znaleść $L$--najdłuższy rosnący podciąg
\end{itemize}
\subsubsection*{Rozwiązanie rekurencyjne}
Zastnaówmy się jak wyglądałoby rozwiązanie tego problemu w sposób dynamiczny. Niech
\[
    L(i) = 1 + \max_{1\leq j\leq i}\left\{\left\{L(j): a_i > a_j\right\}\cup\left\{0\right\}\right\}
\]
$L(i)$--najdłuższy rosnący podciąg kończący się na $a_i$. Teraz należało by przejść od $i=1$ do $n$ i policzyć $L(i)$ dla każdego $i$, zapisać w tablicy i zwrócić $\max_{1\leq i \leq n}\{L(i)\}$.\\
Złożoność czasowa tego algorytmu to $O(n^2)$, ponieważ dla każdego $i$ musimy przejść przez wszystkie $j$ i policzyć $L(j)$, natomiast złożoność pamięciowa to $O(n)$.
\subsection{Problem wydawania reszty}
\begin{itemize}
    \item \textbf{Input}: $c_1 < c_2 < \dots < c_n$ -- zbiór nominałów $\in \mathbb{N}$, $R$ -- reszta do wydania
    \item \textbf{Output}: $k \in \mathbb{N}$ -- liczba monet do wydania
    \item \textbf{Problem}: Znaleść minimalne $k$ takie, że $k$ monet wystarczy do wydania reszty $R$
\end{itemize}
Niech $L(i)$ oznacza minimalną liczbę monet do wydania reszty $i$.
\[
    L(i)=1+\min_{1\leq j\leq n}\left\{L(i-c_j): c_j\leq i\right\}
\]
Z warunkami początkowymi $L(0)=0$. Zobaczmy to na przykładzie, zbiór nominałów to $\{1,4,5\}$:
%stworz dwu wierszowa tabelke z i i L(i)
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
        \hline
        $i$  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
        \hline
        $L(i)$  & 0 & 1 & 2 & 3 & 1 & 1 & 2 & 3 & 2 \\
        \hline
    \end{tabular}
\end{center}
Jeżeli trafimy na $L(i)$, którego nie możemy policzyć bo się nie da wydać reszty z danymi nominałami, to zwracamy \textit{inf}.\\
Złożoność takiego algorytmu to $O(nR)$, natomiast z analizy długości danych wynika, że $m$--długość w bitach danych to:
\[
    m=n\cdot \log c_n + \log R \impliedby n \cdot \log c_n \leq \log R
\]
Wtedy Złożoność naszego algorytmu jest wykładnicza $O(2^m)$.
\subsubsection*{Fakt}
Jeśli zbiór nominałów zawiera $1$, to rozwiązanie istnieje dla każdego $R \in \mathbb{N}$.

\subsection{Problem plecakowy -- Knapsack}
\begin{itemize}
    \item \textbf{Input}: $n$ par waga, wartość $(w_i, v_i)$, ograniczone górnie pezez pojemność plecaka $W$
    \item \textbf{Output}: $I \subset \{1,\dots,n\}$ -- zbiór przedmiotów zabieranych do plecaka, takie że
        \begin{enumerate}
            \item $\sum_{i\in I} w_i \leq W$
            \item $\sum_{i\in I} v_i$ -- maksymalne
        \end{enumerate}
\end{itemize}
Naiwnym algorytmem byłoby przeszukiwane wszystkich podzbiorów zbioru przedmiotów, co daje nam złożoność $O(2^n)$.\\
Przyjrzyjmy się jednak temu problemowi rekurencyjnie. Niech $V(n, W)$ -- maksymalna wartość rozwiązania na $n$ przedmiotach
\[
    V(i, W) = \max\left\{ \underbrace{V(i-1, W),}_{\text{discard $i^\text{th}$ item}} \quad \underbrace{V(i-1, W-w_i)+v_i}_{\text{bierzemy $i$-ty przedmiot}} \right\}
\]
Z pewnymi warunkami początkowymi $V(\cdot, 0) = V(0, \cdot ) = 0$. Teraz możemy zacząć budować rozwiązanie rekurencyjne, odrazu rozwiązując rekurencje.\\
\begin{algorithm}
    \caption{Knapsack}
    \begin{algorithmic}[1]
        \Procedure{Knapsack}{n, W}
        \If{$n=0 \lor W=0$}
            \State \textbf{return} 0
        \EndIf
        \For{$i=1$ to $n$}
            \For{$w=1$ to $W$}
                \If{$w_i \leq w$}
                    \State $V(i, w) = \max\left\{V(i-1, w), V(i-1, w-w_i)+v_i\right\}$
                \Else
                    \State $V(i, w) = V(i-1, w)$
                \EndIf
            \EndFor
        \EndFor
        \State \textbf{return} $V(n, W)$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
Złożoność tego algorytmu to $O(nW)$, natomiast z podobnego powodu jak poprzednio (analiza długości danych) złożoność algorytmu jest wykładnicza $O(2^m)$, gdzie $m$ to długość danych.

\subsection{Optymalne mnożenie macierzy}
\begin{itemize}
    \item \textbf{Input}: $A_1, A_2, \dots, A_n$, gdzie $A_i:m_{i-1}\times m_i$
    \item \textbf{Output}: $P = \prod_{i=1}^n A_i$
    \item \textbf{Problem}: Znaleść optymalny sposób mnożenia macierzy
\end{itemize}
Niech $c(i, j)$ -- optymalny koszt przemnożenia macierzy $A_i, A_{i+1}, \dots, A_j$. Wtedy
\[
    c(i, j) = \min_{i\leq k < j}\left\{c(i, k)+c(k+1, j)+m_{i-1}m_k m_j\right\}
\]
Z warunkami początkowymi $c(i, i) = 0$.

\section{Wykład \date{2025-05-12}}
Dzisiaj zaczcynamy używać grafów. Grafy są to struktury danych, które składają się z węzłów i krawędzi. Węzły są to obiekty, które mogą być połączone ze sobą krawędziami. Krawędzie mogą być skierowane lub nieskierowane.
\begin{enumerate}
    \item Najkrótsza scieżka w DAG
        \begin{center}\begin{tikzpicture}[
            every node/.style={circle,draw,inner sep=2pt,minimum size=6mm},
            every edge/.style={draw,thick},
            node distance=2cm
            ]
  % Węzły
            \node (S)                                    {S};
            \node (A) [below right=of S]                {A};
            \node (C) [above right=of A]                {C};
            \node (B) [below right=of C]                {B};
            \node (D) [below right=of B]                {D};
            \node (E) [below left=of D]                 {E};

  % Krawędzie z wagami nad liniami
            \path
                (S) edge node[above,fill=white,pos=0.5] {2} (C)
                edge node[above,fill=white,pos=0.5] {1} (A)
                (C) edge node[above,fill=white,pos=0.5] {4} (A)
                edge node[above,fill=white,pos=0.5] {3} (D)
                (A) edge node[above,fill=white,pos=0.5] {6} (B)
                (B) edge node[above,fill=white,pos=0.5] {1} (D)
                edge node[above,fill=white,pos=0.5] {2} (E)
                (D) edge node[above,fill=white,pos=0.5] {1} (E);
        \end{tikzpicture}\end{center}
        W grafach możemy wyróżnić pewne węzły na podstawie ich krawędzi. Jeżeli do węzła $u$ nie wchodzą żadne krawędzie, to nazywamy go \textbf{źródłem}. Jeżeli z węzła $u$ nie wychodzą żadne krawędzie, to nazywamy go \textbf{ujściem}.\\
        Liczenie najkrótszej scieżki w DAG można wykonać w czasie $O(V+E)$, gdzie $V$ to liczba węzłów, a $E$ to liczba krawędzi. Wygląda to następująco na przykładzie
        \[
            L(A) = \max \begin{cases}
                L(S)+w(S, A) \\
                L(C)+w(C, A)
            \end{cases}
        \]
        Natomiast w pseudokodzie:
        \begin{center}
        \begin{algorithm}
            \caption{Najkrótsza ścieżka w DAG}
            \begin{algorithmic}[1]
                \Procedure{ShortestPath}{G, E}

    % --- oznaczenie sekcji inicjalizacji ---
                \Statex \Comment{\textbf{Inicjalizacja:} \(\Theta(|V|)\)}
                \For {$v \in V$}
                \State $L(v) \gets \infty$
                \EndFor
                \State $L(S) \gets 0$
    % --- dalsza część algorytmu ---
                \Statex \Comment{\textbf{Meat algorytmu:} \(\Theta(|E|)\)}
                \For {$v \in V\setminus\{S\}$}
                \State $L(v) \gets \min_{(u,v) \in E}\{L(u)+w(u,v)\}$
                \EndFor
                \Statex
                \State \textbf{return} $L$
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \end{center}
        Całkowita złożoność algorytmu to $\Theta(|V|+|E|)$, ponieważ w najgorszym przypadku musimy przejść przez wszystkie węzły i krawędzie.
\end{enumerate}

Rozważmy problem znajdowania nadłuższego rosnącego podciągu. Weźmy na przykład ciag:
\[
    5,2,8,6,3,6,9,7
\]
W reprezentacji graficznej wygląda to następująco:
\begin{center}
\begin{tikzpicture}[
  every node/.style={circle,draw,inner sep=1.5pt,minimum size=7mm},
  node distance=1cm
]
  % Węzły w rzędzie
  \node (1)                        {5};
  \node (2) [right=of 1]          {2};
  \node (3) [right=of 2]          {8};
  \node (4) [right=of 3]          {6};
  \node (5) [right=of 4]          {3};
  \node (6) [right=of 5]          {6};
  \node (7) [right=of 6]          {9};
  \node (8) [right=of 7]          {7};

  % Krawędzie: i -> j gdy i<j i a_i < a_j
  \path
    (1) edge (3) edge (4) edge (6) edge (7) edge (8)
    (2) edge (3) edge (4) edge (5) edge (6) edge (7) edge (8)
    (3) edge (7)
    (4) edge (7) edge (8)
    (5) edge (6) edge (7) edge (8)
    (6) edge (7) edge (8)
  ;
\end{tikzpicture}
\end{center}

\subsection{Edit Distance}
Przykładem wykożystania tego problemu jest użycie go w spellcheckerach, które sugerują poprawne słowa na podstawie podobieństwa do błędnie napisanego słowa.
\begin{itemize}
    \item \textbf{Input}: $w_1, w_2$ -- słowa, $\Sigma$ -- alfabet
    \item \textbf{Output}: $EditDistance(w_1, w_2)$ -- minimalna liczba operacji potrzebnych do przekształcenia $w_1$ w $w_2$
    \item \textbf{Problem}: Znaleść minimalną liczbę operacji potrzebnych do przekształcenia $w_1$ w $w_2$
\end{itemize}
Zobaczmy to najpierw na \textbf{przykładzie}:
%w_1=SNOWY
%w_2=SNOW
\[
w_1=\text{SNOWY},\quad w_2=\text{SUNNY}.
\]
\[
\begin{array}{c|cccccc}
d_{i,j} &   & S & U & N & N & Y \\\hline
        & 0 & 1 & 2 & 3 & 4 & 5 \\
S       & 1 & 0 & 1 & 2 & 3 & 4 \\
N       & 2 & 1 & 1 & 1 & 2 & 3 \\
O       & 3 & 2 & 2 & 2 & 2 & 3 \\
W       & 4 & 3 & 3 & 3 & 3 & 3 \\
Y       & 5 & 4 & 4 & 4 & 4 & 3 \\
\end{array}
\]
Stąd
\[
\mathrm{EditDistance}(\text{SNOWY},\text{SUNNY}) = d_{5,5} = 3,
\]
czyli potrzebne są trzy podstawienia (np. \(N\to U\), \(O\to N\), \(W\to N\)).

Niech $E(i,j)$ -- edit distance $w_1[1\dots i], w_2[1\dots j]$. Mamy następujące możliwości
\begin{itemize}
    \item dodanie litery do $w_1 \gets E(i, j-1)+1$
    \item usuniecie litery z $w_2 \gets E(i-1, j)+1$
    \item podmienienie litery w $w_2 \gets E(i-1, j-1)+1$
    \item bez zmian $w_1 \gets E(i-1, j-1)$
\end{itemize}
Przy wykonywaniu tego algorytmu musimy brać minimum z tych czterech możliwości, a więc
\[
    E(i,j) = \min \begin{cases}
        E(i, j-1)+1 \\
        E(i-1, j)+1 \\
        E(i-1, j-1)+1 \\
        E(i-1, j-1)
    \end{cases}
\]
A jak wygląda graf? %TODO od szymiego
\[
    \begin{array}{c|ccccccc}
        d_{i,j} & & 0 & 1 & 2 & 3 & 4 & \dots \\ \hline
                & 0 & 1 & 2 & 3 & 4 & 5 & \dots \\
        1 & 1 & 0 & 1 & 2 & 3 & 4 & \dots \\
    \end{array}
\]
Pseudokod:
\begin{center}
\begin{algorithm}
    \caption{Edit Distance}
    \begin{algorithmic}[1]
        \Procedure{EditDistance}{$w_1, w_2$}
            \State $n \gets |w_1|$
            \State $m \gets |w_2|$
            \For{$i=0$ to $n$}
                \For{$j=0$ to $m$}
                    \If{$i=0$}
                        \State $d(i,j) = j$
                    \ElsIf{$j=0$}
                        \State $d(i,j) = i$
                    \Else
                        \State $d(i,j) = \min\begin{cases}
                            d(i-1,j)+1\\
                            d(i,j-1)+1\\
                            d(i-1,j-1)+1\\
                            d(i-1,j-1)
                        \end{cases}$
                    \EndIf
                \EndFor
            \EndFor
            \State \textbf{return} $d(n,m)$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\end{center}

\section{Wykład \date{26-05-2025}}
Rozważmy graf skierowany
\begin{tikzpicture}[
    >=Stealth,                     % arrow tip
    every node/.style={           % node style
        circle, draw, minimum size=6mm, inner sep=1pt
    }
    ]
  %--- nodes in a square + midpoints ---
    \node (A) at (0,2) {A};
    \node (C) at (2,2) {C};
    \node (G) at (2,0) {G};
    \node (E) at (0,0) {E};
    \node (B) at (0,1) {B};
    \node (H) at (1,2) {H};
    \node (D) at (2,1) {D};
    \node (F) at (1,0) {F};

  %--- directed edges ---
    \draw[->] (A) -- (F);
    \draw[->] (A) -- (B);
    \draw[->] (A) -- (C);

    \draw[->] (B) -- (E);

    \draw[->] (C) -- (D);

    \draw[->] (E) -- (F);
    \draw[->] (E) -- (G);
    \draw[->] (E) -- (H);

    \draw[->] (F) -- (G);

    \draw[->] (H) -- (G);
\end{tikzpicture}
\subsubsection{Własność 1}
Dla każdego $\forall v,u \in V$:
\begin{enumerate}
    \item $pre(v)<pre(u)<post(u)<post(v)$
    \item $pre(v)<post(v)<pre(u)<post(u)$
\end{enumerate}
\subsubsection{Własność 2}
Dla każdej krawędzi $(u,v) \in V$ mamy nazewnictwo:
\begin{enumerate}
    \item Tree/forward edge: $pre(u)<pre(v)<post(v)<post(u)$
    \item Back edge: $pre(v)<pre(u)<post(u)<post(v)$
    \item Cross edge: $pre(v)<post(v)<pre(u)<post(u)$
\end{enumerate}
\subsubsection{Własność 3}
W grafie skierowanym istnieje cykl wtedy i tylko wtedy, gdy DFS wykryje back edge
\begin{proof}
    $\implies$\\
    Rozważmy następującą sytłację:
    \[
        v_0 \to v_1 \to v_2 \to \dots \to v_k \to \dots \to v_0
    \]
    powiedzmy, że DFS odwiedzi jako pierwszy w tym cyklu wieżchołek $v_i$. Dalej eksploruje, aż napotka się na $v_{i-1}$, wtedy krawędź $v_{i-1} \to v_i$ musi być back edge.
    \bigskip
    $\impliedby$\\
    Do własnego zastanowienia się
\end{proof}

\subsection{Sortowanie topologiczne DAG'ów}
Rozważmy następujący problem:
\begin{itemize}
    \item \textbf{Input}: $G=(V,E)$ -- acykliczny graf skierowany
    \item \textbf{Output}: $(V, \prec)$ -- porządek topologiczny
\end{itemize}
Algorytm musi wykonywać następujące operacje:
\begin{enumerate}
    \item wykonujemy DFS, zapisuje pre i post
    \item zwrócić wierzchołki w malejącej kolejności po post
\end{enumerate}
\subsubsection*{Przykład}
\begin{tikzpicture}[
    >=Stealth,                     % typ strzałki
    every node/.style={           % styl węzłów
        circle, draw, minimum size=6mm, inner sep=1pt
    }
    ]
  %--- węzły w siatce 3x2 ---
    \node (A) at (0,1) {A};
    \node (C) at (1,1) {C};
    \node (E) at (2,1) {E};
    \node (B) at (0,0) {B};
    \node (D) at (1,0) {D};
    \node (F) at (2,0) {F};

  %--- skierowane krawędzie ---
    \draw[->] (A) -- (C);    % A → C
    \draw[->] (C) -- (E);    % C → E
    \draw[->] (C) -- (F);    % C → F
    \draw[->] (B) -- (A);    % B → A
    \draw[->] (B) -- (D);    % B → D
    \draw[->] (D) -- (C);    % D → C
\end{tikzpicture}
Algorytm DFS bedzię odwiedzał odpowiednio wierzchołki w kolejności, gdzie $\sideset{^{pre}}{^{post}}X$
\begin{itemize}
    \item $\sideset{^1}{^2}F$
    \item $\sideset{^3}{^6}C \to \sideset{^4}{^5}E$
    \item $\sideset{^7}{^8}D$
    \item $\sideset{^9}{^{12}}B \to \sideset{^{10}}{^{11}}A$
\end{itemize}
co wraca następujący porządek topologiczny po post:\\
\begin{tikzpicture}[
    ->,                    % make all paths directed by default
    >=stealth,             % nice arrow tip
    node distance=2cm,     % default distance between nodes
    on grid,               % align nodes on grid
    auto,                  % automatic placement of labels (not needed here)
    state/.style={         % redefine “state” style for our nodes
      circle, draw, minimum size=6mm, inner sep=1pt
    }
  ]
  %--- place the states in one row ---
  \node[state] (B) {B};
  \node[state] (A) [right=of B] {A};
  \node[state] (D) [right=of A] {D};
  \node[state] (C) [right=of D] {C};
  \node[state] (E) [right=of C] {E};
  \node[state] (F) [right=of E] {F};

  %--- draw the edges (automata style snaps to borders) ---
  \path
    (B) edge              (A)
        edge[bend left]   (D)
        (A) edge[bend right]              (C)
    (D) edge              (C)
    (C) edge[bend left]   (E)
        edge[bend right]  (F);
\end{tikzpicture}
\subsection{costam}
%TODO nazwac to jakos
\subsubsection*{Definicja}
W grafie skierowanym $G=(V,E)$ powiemy, że wierzchołki $u,v \in V$ są połączone jeśli isnieje ścieżka z $u$ do $v$ oraz z $v$ do $u$.
\subsubsection*{Definicja}
Będziemy mówić, że $v_i, v_2 \in V$ tworzą silnie spójną komponentę w grafie $G$, jeśli:
\begin{itemize}
    \item $\forall 1 \leq j \leq k: v_i \text{ jest połączony z } v_j$
    \item nie istnije wieżchołek $u\in V$ taki, że $u$ jest połączony z $v_i \forall 1 \leq i \leq k$
\end{itemize}
\subsubsection{Własność 4}
wieżchołek z największą wartością post będzie znajdował się w źródłowym silnie spójnym komponencie grafu skierowanego.
\subsubsection{Własność 5}
Niech $C$ i $C'$ będą silnie spójnymi składowymi grafu skierowanego, $G$ oraz $\exists (u,v) \in G: u\in C, c \in C'$. Wtedy
maksymalna wartość post wierzchołka z $C$ będzie większa niż maksymalna wartość z $C'$
\begin{proof}
    Rozważmy możliwe przypadki
    \begin{enumerate}
        \item DFS najpiwer odwiedzi wierzchołek $u \in C$, przed wierzchołkami z $C'$ (DFS wróci i ustawi wiekszego posta, prosta idea)
        \item DFS najpierw odwiedzi wierzchołek $v \in C'$ przed wierzchołkami z $C$ (eksploracja $C$ odbędzie się ostatnia, bo musi do niej wrócić w poźniejszych eksplorach)
    \end{enumerate}
\end{proof}

\subsubsection{Własnośc 6}
Niech $G^R=(V,E^R)$, gdzie
\[
    E^R=\{(v,u): (u,v) \in E\}
\]
wtedy źródło metagrafu $G^R$ jest ujściem w metagrafie $G$
\subsection{Strongly Connected Componets algorytm}
\begin{itemize}
    \item \textbf{Input}: $G$ -- graf skierowany
    \item \textbf{Output}: metagraf silnie spójnych składowych $G$
\end{itemize}
Algorytm musi wyglądać następująco:
\pagebreak
\section{Ćwiczenia}
tu beda pojawialy sie notatki z cwiczen do przedmiotu Algorytmy i struktury danych na Politechnice Wrocławskiej na kierunku Informatyka Algorytmiczna rok 2025 semestr letni.

\subsection{Lista 2}
robiona na zajęciach \date{2025-03-10}
\subsubsection{zadanie 1}
Wylicz ile linijek wypisze poniższy program (podaj wynik będacy funkcją od n w postaci asymptotycznej $\Theta(\cdot)$). Można założyć, że $n$ jest potęgą liczby $3$.
\begin{algorithm}
\begin{algorithmic}[1]
\State \textbf{function} f(n)
\If{$n > 1$}
    \State print\_line('still going')
    \State f(n/3)
    \State f(n/3)
\EndIf
\end{algorithmic}
\end{algorithm}
w pseudo kodzie pojawia sie nastepujaca rekurencja:
\[
    T(n) = 2T(\frac{n}{3}) + 1
\]
rozwiąże ją używając metody podstawienia. Niech $n=3^k, k = \log_3 n$, wtedy:
\[
    T(3^k) = 2T(3^{k-1}) + 1
\]
Zatem przyjmując $S(k) = T(3^k)$ mamy:
\[
    S(k) = 2S(k-1) + 1
\]
rozwiązując rekurencję otrzymujemy:
\[
    S(k) = 2^k - 1
\]
zatem
\[
    T(n) = 2^{\log_3 n} - 1 = n^{\log_3 2} - 1 = \Theta(n^{\log_3 2})
\]
analogicznie liczmy jaka jest wykonana ``praca'' wykonana przez program w drzweie rekursji.

\subsubsection{zadanie 2}
Niech $f(n)$ i $g(n)$ będą funkcjami asymptotycznie nieujemnymi (tzn. nieujemnymi dla dostatecznie dużego $n$). Korzystając z definicji notacji $\Theta$, udowodnij, że:
\[
\max\{f(n), g(n)\} = \Theta(f(n) + g(n)).
\]
\begin{proof}
    Z definicji notacji $\Theta$ mamy:
    \[
        f(n)=\Theta(g(n)) \iff \exists c_1, c_2 > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, 0 \leq c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n)
    \]
    skoro $f(n)$ i $g(n)$ są asymptotycznie nieujemne to:
    \[
        \exists n_f: \forall n \geq n_f, f(n) \geq 0
    \]
    \[
        \exists n_g: \forall n \geq n_g, g(n) \geq 0
    \]
    zatem
    \[
        n_0=\max\{n_f, n_g\}
    \]
    a więc
    \[
        f(n) \leq \max\{f(n), g(n)\}
    \]
    \[
        g(n) \leq \max\{f(n), g(n)\}
    \]
    dodając obie nierówności otrzymujemy:
    \[
        f(n) + g(n) \leq 2 \cdot \max\{f(n), g(n)\}
    \]
    zatem
    \[
        \forall n \geq n_0: \max\{f(n), g(n)\} \leq f(n) + g(n) \leq 2 \cdot \max\{f(n), g(n)\}
    \]
    a więc z definicji mamy
    \[
        \max\{f(n), g(n)\} = \Theta(f(n) + g(n))
    \]
\end{proof}

\subsubsection{zadanie 3}
Wylicz asymptotyczną złożoność (używając notacji $\Theta$) poniższych fragmentów programów:

\begin{algorithm}
\caption{Pierwszy fragment kodu}
\begin{algorithmic}[1]
\For{$i = 1$ to $n$}
    \State $j = i$
    \While{$j < n$}
        \State $sum = P(i, j)$
        \State $j = j + 1$
    \EndWhile
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Drugi fragment kodu}
\begin{algorithmic}[1]
\For{$i = 1$ to $n$}
    \State $j = i$
    \While{$j < n$}
        \State $sum = R(i, j)$
        \State $j = j + j$
    \EndWhile
\EndFor
\end{algorithmic}
\end{algorithm}
Gdzie:
\begin{itemize}
    \item koszt wykonania procedury $P(i,j)$ wynosi $\Theta(1)$,
    \item koszt wykonania procedury $R(i,j)$ wynosi $\Theta(j)$.
\end{itemize}

\begin{proof}
    \begin{itemize}
        \item Pierwszy fragment kodu
            \begin{itemize}
                \item Wewnętrzna pętla wykonuje się $n-i$ razy
                \item Koszt wykonania procedury $P(i,j)$ wynosi $\Theta(1)$
                \item Zatem koszt wykonania wewnętrznej pętli wynosi $\Theta(n-i)$
                \item Zatem koszt wykonania całego fragmentu wynosi
                    \[
                        \sum_{i=1}^{n} \Theta(n-i) = \Theta(n^2)
                    \]
            \end{itemize}
        \item Drugi fragment kodu
            \begin{itemize}
                \item Wewnętrzna pętla wykonuje się $\log_2 n$ razy
                \item Koszt wykonania procedury $R(i,j)$ wynosi $\Theta(j)$
                \item Zatem koszt wykonania wewnętrznej pętli wynosi $\Theta(\log_2 n)$
                \item Zatem koszt wykonania całego fragmentu wynosi
                    \[
                        \sum_{i=1}^{n} \Theta(\log_2 n) = \Theta(n \log_2 n)
                    \]
            \end{itemize}
    \end{itemize}
\end{proof}
Dla pewnosci sprawdzone empirycznie:
%zalacz wykres wykresAlgoZad3.png
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{wykresAlgoZad3.png}
\end{figure}

\subsubsection{zadanie 4}
Wyznacz asymptotyczne oszacowanie górne dla następujących rekurencji:

\begin{itemize}
    \item $T(n) = 2T(n/2) + 1$
    \item $T(n) = 2T(n/2) + n$
    \item $T(n) = 3T(n/2) + n \log n$
\end{itemize}

\bigskip
\hrule
\bigskip

Kożystając z \textbf{Master Theorem} możemy wyznaczyć ograniczenie dla tych rekurencji.
\begin{itemize}
    \item $T(n) = 2T(n/2) + 1$
        \begin{proof}
            \[
                a = 2, b = 2, d = 0
            \]
            \[
                \log_b a = \log_2 2 = 1 > 0 = d
            \]
            \[
                T(n) = \Theta(n)
            \]
        \end{proof}
    \item $T(n) = 2T(n/2) + n$
        \begin{proof}
            \[
                a = 2, b = 2, d = 1
            \]
            \[
                \log_b a = \log_2 2 = 1 = d
            \]
            \[
                T(n) = \Theta(n \log n)
            \]
        \end{proof}
    \item $T(n) = 3T(n/2) + n \log n$
        \begin{proof}
            \text{Dolne ograniczenie}
            \[
                T(n) = 3T(n/2) + n \implies^{\text{Master Theorem}} T(n) = \Theta(n^{\log_2 3})
            \]
            \text{Górne ograniczenie}
            \[
                T(n) = 3T(n/2) + n^{1.1} \implies^{\text{Master Theorem}} T(n) = \Theta(n^{1.1})
            \]
        \end{proof}
\end{itemize}

\subsubsection{zadanie 5}
Zaprojektuj algorytm wczytujący z wejścia tablicę liczb $A[1], \ldots, A[N]$ i przygotowujący tablicę $B$ tak, że na jej podstawie będzie potrafił odpowiadać na pytania:
\begin{enumerate}
    \item ile wynosi suma elementów tablicy $A$ od miejsca $i$ do miejsca $j$ włącznie, dla $i < j$.
    \item Jaka jest złożoność czasowa Twojego algorytmu? Ile pamięci zajmuje tablica $B$?
    \item Ile zajmuje odpowiedź na jedno pytanie?
\end{enumerate}

\bigskip
\hrule
\bigskip

Przykładowy algorytm mógłby wyglądać następująco:
\begin{algorithm}
    \caption{Algorytm do zadania 5.}
    \begin{algorithmic}[1]
        \State $B[1] = A[1]$
        \For{$i = 2$ to $N$}
            \State $B[i] = B[i-1] + A[i]$
        \EndFor
        \Procedure{Sum}{i, j} %uwaga na edge case
        \If {$i=1$}
            \State \Return $B[j]$
        \Else
            \State \Return $B[j] - B[i-1]$
        \EndIf
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

Co tu się dzieje?
\begin{itemize}
    \item W pierwszej pętli obliczamy sumy prefiksowe tablicy $A$ i zapisujemy je w tablicy $B$.
    \item W procedurze \texttt{Sum} zwracamy różnicę między dwoma elementami tablicy $B$.
\end{itemize}

\begin{itemize}
    \item Złożoność czasowa algorytmu wynosi $\Theta(n)$.
    \item Tablica $B$ zajmuje $\Theta(n)$ pamięci.
    \item Odpowiedź na jedno pytanie zajmuje $\Theta(1)$ czasu.
\end{itemize}

\subsubsection{zadanie 6}
Pokaż, jak grać w grę w "10 pytań", w której wiadomo, że wybrana liczba jest dodatnia, ale nie jest na początku znane górne ograniczenie jej wartości. Ile pytań potrzebujesz, żeby zgadnąć dowolną liczbę (liczba pytań może zależeć od wielkości liczby)?

\bigskip
\hrule
\bigskip

W grze "10 pytań" możemy zadać pytania w stylu "czy liczba jest większa od $x$?". W ten sposób możemy zredukować przestrzeń poszukiwań. W pierwszym pytaniu zapytajmy, czy liczba jest większa od $1$. Jeśli tak, to zapytajmy, czy liczba jest większa od $2$. W ten sposób możemy zredukować przestrzeń poszukiwań do $2^k$ dla pewnego $k$. W ten sposób możemy znaleźć dowolną liczbę w $k$ pytaniach.
\begin{algorithm}
    \caption{Algorytm do zadania 6.}
    \begin{algorithmic}[1]
        \State{$k = 0$}
        \While{$2^k < x$}
            \State{$k = k + 1$}
        \EndWhile
        \State{$p=2^{k-1}$}
        \State{$q=2^k$}
        \Procedure{BinarySearch}{p, q}
    \end{algorithmic}
\end{algorithm}


\subsubsection{zadanie 7}
Używając algorytmu \textbf{divide-and-conquer} do mnożenia liczb wykonaj mnożenie dwóch liczb binarnych 11011, 1010.

\bigskip
\hrule
\bigskip

Algorytm \textbf{divide-and-conquer} do mnożenia liczb działa w następujący sposób:
\begin{enumerate}
    \item Podziel liczby na dwie równe części.
    \item Rekurencyjnie pomnóż te części.
    \item Połącz wyniki.
\end{enumerate}

Mnożenie dwóch liczb binarnych $11011$ i $1010$ możemy zrealizować w następujący sposób:
\begin{enumerate}
    \item Podziel liczby na dwie równe części: $1101$, $1$ oraz $10$, $10$.
    \item Rekurencyjnie pomnóż te części: $1101 \cdot 10 = 11010$.
    \item Połącz wyniki: $11010 + 110100 = 1000000$.
\end{enumerate}

\begin{algorithm}
\caption{Algorytm do zadania 7 (pokazany na wykładzie)}
    \begin{algorithmic}[1]
        \Procedure{Mul}{x, y}
        \State{$n = \max\{|x|, |y|\}$}
        \If{$n = 1$}
        \State \Return{$x \cdot y$}
        \EndIf
        \State{$x_L, x_R = \text{LetfMost}\left\lceil\frac{n}{2}\right\rceil, \text{RightMost}\left\lceil\frac{n}{2}\right\rceil \text{bits of $x$}$}
        \State{$y_L, y_R = \text{LetfMost}\left\lceil\frac{n}{2}\right\rceil, \text{RightMost}\left\lceil\frac{n}{2}\right\rceil \text{bits of $y$}$}
        \State{$p_1 = \text{Mul}(x_L, y_L)$}
        \State{$p_2 = \text{Mul}(x_R, y_R)$}
        \State{$p_3 = \text{Mul}(x_L + x_R, y_L + y_R)$}
        \State \Return{$p_1 \cdot 2^{2n} + (p_3 - p_1 - p_2) \cdot 2^n + p_2$}
    \end{algorithmic}
\end{algorithm}

\subsection{Lista 3}
robiona na zajęciach \date{2025-03-24}

\subsubsection{zadanie 1}
Podaj algorytm scalający $k$ posortowanych list tak aby powstała jedna posortowana lista $nb$ (liczba wszystkich elementów na listach to n) działający w czasie $O(n \log k)$.

\bigskip
\hrule
\bigskip

Algorytm ten można zrealizować w następujący sposób:
\begin{algorithm}
    \caption{Algorytm do zadania 1.}
    \begin{algorithmic}[1]
        \Procedure{MergeLists}{$L_1, L_2, \ldots, L_k$}
        \State{$n = \sum_{i=1}^{k} |L_i|$}
        \State{$B = \text{tablica} [1 \ldots n]$}
        \State{$\text{heap} = \text{MinHeap}$}
        \For{$i = 1$ to $k$}
            \State{$\text{heap}.\text{insert}(L_i.\text{pop}())$}
        \EndFor
        \For{$i = 1$ to $n$}
            \State{$B[i] = \text{heap}.\text{pop}()$}
            \State{$\text{heap}.\text{insert}(L_i.\text{pop}())$}
        \EndFor
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\subsubsection{zadanie 2}
Zdefiniujmy algorytm \textit{k-MergeSort} jako uogólnienie algorytmu sortowania przez scalanie. Różni się od omawianego na wykładzie algorytmu sortowania przez scalanie tym, że dzieli sortowana tablice rekurencyjnie na k równych części (zakładamy, że liczba elementów w tablicy jest potęgą k $(n = k^l)$). \\
Używając wyniku z zadania 1 proszę wykazać dla jakiego k algorytm ma najmniejsza asymptotyczną złożoność obliczeniową liczby porównań (górne ograniczenie $O(\cdot)$).

\bigskip
\hrule
\bigskip

Algorytm \textit{k-MergeSort} spełnia rekurencję:
\[
    T(n) = kT(\frac{n}{k}) + \Theta(n \log k)
\]
gdzie $\Theta(n \log k)$ to koszt scalania $k$ posortowanych list (zadanie 1). Z \textbf{Master Theorem} otrzymujemy, że algorytm ma złożoność:
\[
    T(n) = \Theta(n \log_k n)
\]

\subsubsection{zadanie 3}
Załóżmy że tablica $A = [a_1, \dots, a_n]$ jest do pewnego momentu $k$ posortowana malejąco i dalej rosnąco (tzn. dla $\forall i < k: a_i > a_{i+1}$ oraz $\forall i \geq k: a_i < a_{i+1}$). Zaprojektuj algorytm znajdujący minimalny element w tablicy $A$, którego złożoność obliczeniowa będzie wynosić $O(\log n)$. Udowodnij poprawność działania zaproponowanego algorytmu.

\bigskip
\hrule
\bigskip

\begin{itemize}
    \item Algorytm ten można zrealizować w następujący sposób:
        \begin{algorithm}
            \caption{Algorytm do zadania 3.}
            \begin{algorithmic}[1]
                \Procedure{FindMin}{$A, p, q$}
                \If{$p = q$}
                \State \Return{$A[p]$}
                \EndIf
                \State{$m = \left\lfloor\frac{p+q}{2}\right\rfloor$}
                \If{$A[m] > A[m+1]$}
                \State \Return{$\text{FindMin}(A, m+1, q)$}
                \Else
                \State \Return{$\text{FindMin}(A, p, m)$}
                \EndIf
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \item Przykład:
        \begin{itemize}
            \item $A = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]$
            \item $m = 5$
            \item $A[m] = 5$, $A[m+1] = 4$
            \item $\text{FindMin}(A, 6, 10)$
            \item $m = 8$
            \item $A[m] = 2$, $A[m+1] = 1$
            \item $\text{FindMin}(A, 9, 10)$
            \item $m = 9$
            \item $A[m] = 1$, $A[m+1] = 1$
            \item \textbf{Zwracamy} $A[m] = 1$
        \end{itemize}
    \item Złożoność obliczeniowa: Algorytm spełnia rekurencję:
        \[
            T(n) = T\left(\frac{n}{2}\right) + \Theta(1)
        \]
        Z \textbf{Master Theorem} otrzymujemy, że algorytm ma złożoność:
        \[
            T(n) = \Theta(\log n)
        \]
        \qed
\end{itemize}

\subsubsection{zadanie 4}
Zastąpienie użycia QuickSort'a, dla tablic małych rozmiarów, algorytmem InsertionSort jest częstym sposobem na zwiększenie efektywności algorytmu rozwiązującego problem sortowania. Pokaż, że jeśli zmodyfikujesz bazowy przypadek rekurencji w algorytmie QuickSort w taki sposób, że dla tablic o $\leq k$ elementach wywoływany będzie InsertionSort (zamiast rekurencyjnego wywołania QuickSort'a), to wartość oczekiwana liczby porównań będzie wynosić $\Theta\left(nk + n \log \frac{n}{k}\right)$. Jakie $k$ należy wybrać, aby zminimalizować tę złożoność?

\bigskip
\hrule
\bigskip

Należy rozważyć wartość oczekiwaną zmiennej losowej $\text{compInsertSort}_n \equiv R_n$
\[
    \mathbb{E}[R_n]=\frac{n(n-1)}{4}
\]
Niech $T_n$ -- liczba porównan w \textit{Hybrid Sorcie} oraz
\[
    X_k^n = \begin{cases}
        1 \impliedby \text{partition podzielił tablice na} \mid k \mid n-k \text{split($k,n-k$)}\\
        0 \impliedby \text{w przeciwnym przypadku}
    \end{cases}
\]
bedzie funkcja indykatorową. Wtedy
\[
    T_n = \sum_{k=0}^{n-1} X_k \cdot \left(T_k + T_{n-l} + n-1 \right)
\]
Przyjmimy oznaczenie $\mathbb{E}[T_n]=t_n$, wtedy
\begin{equation*}\begin{aligned}
    t_n &= \sum_{k=0}^{n-1} \mathbb{E}[X_k] \cdot \left(t_k + t_{n-k} + n-1 \right) \\
        &= \sum_{k=0}^{n-1} \frac{1}{n} \cdot \left(t_k + t_{n-k} + n-1 \right) \\
        &= \frac{1}{n} \sum_{k=0}^{n-1} \left(t_k + t_{n-k} + n-1 \right) \\
        &= \frac{1}{n} \sum_{k=0}^{n-1} t_k + \frac{1}{n} \sum_{k=0}^{n-1} t_{n-k} + \frac{1}{n} \sum_{k=0}^{n-1} n-1 \\
        &= \frac{2}{n} \sum_{k=0}^{n-1} t_k + n-1
\end{aligned}\end{equation*}
Mnożąc obie strony równania przez $n$ otrzymujemy
\[
    n t_n = 2 \sum_{k=0}^{n-1} t_k + n^2 - n
\]
Zamieniając $n \rightarrow n-1$ otrzymujemy
\[
    (n-1) t_{n-1} = 2 \sum_{k=0}^{n-2} t_k + n^2 - 3n + 2
\]
Odejmując stronami otrzymujemy
\[
    n t_n - (n-1) t_{n-1} = 2t_{n-1} + 2n - 2
\]
Upraszczając:
\[
    n t_n = (n+1) t_{n-1} + 2(n - 1)
\]
Dzieląc obie strony przez $n(n+1)$ otrzymujemy
\[
    \frac{t_n}{n+1} = \frac{t_{n-1}}{n} + \frac{2(n-1)}{n(n+1)}
\]
teraz przyjmujemy oznaczenie $\varphi_n = \frac{t_n}{n+1}$, wtedy
\[
    \varphi_n = \varphi_{n-1} + \frac{2(n-1)}{n(n+1)}
\]
należy rozwiązać powyższą rekurencję itracyjnie, biorąc pod uwagę, że w pewnym momencie następuje zmiana z algorytmu \textit{QuickSort} na \textit{InsertionSort}, niech $k$ tym momentem, wtedy
\begin{equation*}\begin{aligned}
    \varphi_n &= \varphi_{n-1} + \frac{2(n-1)}{n(n+1)} =\\
              &= \sum_{i=k+1}^{n} \frac{2(i-1)}{i(i+1)} + \frac{k(k-1)}{4(k+1)} \\
              &= \sum_{i=k+1}^{n} \left(\frac{2}{i+1} - \frac{1}{i}\right) + \frac{k(k-1)}{4(k+1)} =\\
              &= 2\sum_{i=k+1}^{n} \frac{1}{i+1} - \sum_{i=k+1}^{n} \frac{1}{i} + \frac{k(k-1)}{4(k+1)} =\\
              &= 2\left( H_{n+1} - H_{k} \right) - \left( H_{n} - H_{k} \right) + \frac{k(k-1)}{4(k+1)} =\\
              &= H_{n} + \frac{2}{n+1} -  H_{k} - \frac{k(k-1)}{4(k+1)}
\end{aligned}\end{equation*}
Podstawiając $t_n = (n+1) \varphi_n$ otrzymujemy
\begin{equation*}\begin{aligned}
    t_n &= (n+1) \left( H_{n} + \frac{2}{n+1} -  H_{k} - \frac{k(k-1)}{4(k+1)} \right) = \\
        &= (n+1) H_{n} + 2 - (n+1) H_{k} - \frac{k(k-1)}{4(k+1)}(n+1)
\end{aligned}\end{equation*}
Ostatecznie prowadzi to do asymptotycznego wzoru na liczbę porównań
\[
    \mathbb{E}[T_n] = \Theta\left(nk + n \log \frac{n}{k}\right)
\]
\qed



\subsubsection{zadanie 5}
Załóżmy, że masz do wyboru jeden z trzech algorytmów rozwiązujących postawiony Ci problem wielkości $n$:
\begin{enumerate}
  \item \textbf{Algorytm A}: rozwiązuje problem dzieląc go rekurencyjnie na 5 pod-problemów o połowę mniejszych i scalając ich rozwiązania w czasie $\Theta(n \log n)$.
  \item \textbf{Algorytm B}: rozwiązuje problem dzieląc go rekurencyjnie na 2 pod-problemy rozmiaru $n - 1$ i scala ich rozwiązania w czasie stałym.
  \item \textbf{Algorytm C}: rozwiązuje problem dzieląc go rekurencyjnie na 9 pod-problemów rozmiaru $n/3$ i scalając ich rozwiązania w czasie $\Theta(n^2)$.
\end{enumerate}

Jaka jest złożoność obliczeniowa tych algorytmów? Który z nich byś wybrał? Odpowiedź uzasadnij.

\bigskip
\hrule
\bigskip


\begin{enumerate}
    \item Spełniona zostaje rekurencja:
        \[
            T(n) = 5T\left(\frac{n}{2}\right) + \Theta(n\log n)
        \]
        Ograniczmy ją sobie z dołu i z góry:
        \[
            T(n) = \Omega(n\log n)
        \]
        \[
            T(n) = O(n\log n)
        \]
        Zatem z \textbf{Master Theorem} otrzymujemy, że złożoność obliczeniowa algorytmu A wynosi \\$\Theta(n\log n)$.
    \item Spełniona zostaje rekurencja:
        \[
            T(n) = 2T(n-1) + \Theta(1)
        \]
        Ograniczmy ją sobie z dołu i z góry:
        \[
            T(n) = \Omega(n)
        \]
        \[
            T(n) = O(n)
        \]
        Zatem z \textbf{Master Theorem} otrzymujemy, że złożoność obliczeniowa algorytmu B wynosi $\Theta(n)$.
    \item Spełniona zostaje rekurencja:
        \[
            T(n) = 9T\left(\frac{n}{3}\right) + \Theta(n^2)
        \]
        Ograniczmy ją sobie z dołu i z góry:
        \[
            T(n) = \Omega(n^2)
        \]
        \[
            T(n) = O(n^2)
        \]
        Zatem z \textbf{Master Theorem} otrzymujemy, że złożoność obliczeniowa algorytmu C wynosi $\Theta(n^2)$.
\end{enumerate}

\subsubsection{zadanie 6}
Powiedzmy, że masz do wykonania $n$ zadań, gdzie każde z nich wymaga $t_j$ minut pracy. Chcesz wykonać wszystkie zadania maksymalizując zadowolenie przełożonego poprzez minimalizację średniego czasu zakończenia każdego zadania. Uzasadnij, w jakiej kolejności powinieneś wykonywać zadania.

\bigskip
\hrule
\bigskip

Możemy przyjąć, że mamy tablicę $T=[t_1, t_2, \ldots, t_n]$ z czasami trwania zadań. Możemy zrealizować algorytm w następujący sposób:
\begin{itemize}
    \item Zadania powinny być wykonywane w kolejności rosnącej czasu pracy. Posortować tablicę $T$ rosnąco i wykonywać zadania pokoleji.
    \item Dla każdego zadania $j$ zakończenie zadania $j$ w czasie $t_j$ minimalizuje średni czas zakończenia każdego zadania. Dlaczego się tak dzieje?\\
        Rozpatrzmy najpierw mały przykład: \\
        Mamy 3 zadania
        \[
            T= [t_1=3, t_2=5, t_3=4].
        \]
        Posortowane zadania to
        \[
            T= [t_1=3, t_3=4, t_2=5].
        \]
        teraz czasy zakonczenia wykonywania zadań, to
        \[
            t= [t_1, t_1+t_3, t_1+t_3+t_2]
        \]
        \[
            t=[3, 3+4, 3+4+5]
        \]
        Średni czas zakończenia zadania to
        \[
            \frac{3+7+12}{3}=7\frac{1}{3}
        \]
        Rozważmy teraz ogólny problem:
        \begin{itemize}
            \item \textbf{Input}: tablica z długoścami trwania wykonywania zadań $T=[t_1, t_2, \dots, t_n]$
            \item \textbf{Output}: pewna permutacja tablicy $T$, która minimalizuje średni czas zakończenia zadania -- $\sigma(T)$
            \item \textbf{Problem}: należy znaleść pewne minimum:
                \[
                    \min \left\{ f(\sigma(T)): \sigma \in S_n \right\}
                \]
                gdzie $f: \mathcal{P}\left(\mathbb{N}\right) \rightarrow \mathbb{R}$ jest funkcją postaci:
            \[
                f(T) = \frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{i} t_j
            \]
            można to zapisać jako:
            \[
                f(T) = \frac{nt_1 + (n-1)t_2 + \dots + 2t_{n-1} + t_n}{n}
            \]
        \item \textbf{Rozwiązanie}: minimalizacja funkcji $f$ polega na posortowaniu tablicy $T$ rosnąco, nalezy jednak udowodnić, że permutacja $\sigma$ sortująca tablicę $T$ jest optymalna.
            %należy wziąść dowolną permutację $\tau$ i biorac permutacje $\tau'$, ktora rozni sie od tau tylko miejscami i i j, i < j, tak, ze t_i > t_j, to mozemy pokazac, ze f(tau') > f(tau), co oznacza i iterujac ten proces dojdziemy do permutacji optymalnej -- sortujacej
            \textbf{Dowód:} \\[1mm]
            Załóżmy, że istnieje optymalna permutacja $\tau$, w której występuje para indeksów $i < j$ taka, że $t_i > t_j$. Rozważmy nową permutację $\tau'$, otrzymaną przez zamianę miejscami zadań $i$ oraz $j$, czyli:
            \[
                \tau' = (\dots, t_j, \dots, t_i, \dots)
            \]
            Przyjrzyjmy się wpływowi tej zamiany na funkcję celu
            \[
                f(T) = \frac{1}{n}\sum_{k=1}^{n} \sum_{l=1}^{k} t_{\tau(l)}.
            \]
            W wyniku zamiany, zadania $t_i$ i $t_j$ pojawiają się na pozycjach $i$ oraz $j$, odpowiednio, co wpływa na sumy częściowe w następujący sposób:
            \[
                \Delta = f(\tau') - f(\tau) = (t_i - t_j)(j-i).
            \]
            Skoro $j-i > 0$ oraz przy założeniu $t_i > t_j$, mamy
            \[
                \Delta > 0.
            \]
            Oznacza to, że funkcja celu $f$ jest mniejsza dla permutacji $\tau'$ niż dla $\tau$, czyli:
            \[
                f(\tau') < f(\tau).
            \]
            Sprzeczność z założeniem, że $\tau$ była optymalna, wynika z faktu, iż zawsze można dokonać zamiany pary, gdzie wcześniejsze zadanie trwa dłużej niż późniejsze, co zmniejsza średni czas zakończenia. \\[1mm]
            W związku z tym, aby nie istniały żadne takie "inwersje", permutacja optymalna musi spełniać warunek
            \[
                t_{\tau(1)} \le t_{\tau(2)} \le \dots \le t_{\tau(n)},
            \]
            czyli musi być uporządkowana rosnąco. \\[1mm]
            \textbf{Wniosek:} Minimalizacja średniego czasu zakończenia zadań jest osiągana przez sortowanie tablicy $T$ w porządku rosnącym.
        \end{itemize}
\end{itemize}

\subsubsection{zadanie 7}
Stwórz algorytm znajdujący najczęściej powtarzający się element w $n$ elementowej tablicy (unikając sortowania tablicy), mający złożoność $O(n \log n)$ (zakładamy, że ten element powtarza się ponad $\frac{n}{2}$ razy).

\bigskip
\hrule
\bigskip

\begin{algorithm}[H]
    \caption{Algorytm do zadania 7.}
    \begin{algorithmic}[1]
        \Procedure{Dominat}{$A, p, q$}
        \If{$p = q$}
        \State \Return{$A[p]$}
        \EndIf
        \State{$dom_L = \text{Dominat}(A, p, \left\lfloor\frac{p+q}{2}\right\rfloor)$}
        \State{$dom_R = \text{Dominat}(A, \left\lfloor\frac{p+q}{2}\right\rfloor + 1, q)$}
        \If{$dom_L = dom_R$}
        \State \Return{$dom_L$}
        \EndIf
        \State{$count_L = \text{count}(A, p, q, dom_L)$}
        \State{$count_R = \text{count}(A, p, q, dom_R)$}
        \If{$count_L > count_R$}
        \State \Return{$dom_L$}
        \Else
        \State \Return{$dom_R$}
        \EndIf
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

Złożoność obliczeniowa algorytmu:
\[
    T(n) = 2T\left(\frac{n}{2}\right) + \Theta(n)
\]
Z \textbf{Master Theorem} otrzymujemy, że złożoność obliczeniowa algorytmu wynosi $\Theta(n \log n)$.\\
Jest to problem z kategorii \textit{Majority element}.

Algorytm działający w czasie liniowym:
\begin{algorithm}[H]
    \caption{Algorytm do zadania 7. działajcy w czasie liniowym.}
    \begin{algorithmic}[1]
        \Procedure{Majority}{$A$}
        \State{$count = 0$}
        \State{$candidate = None$}
        \For{$i = 0$ to $n-1$}
            \If{$count = 0$}
                \State{$candidate = A[i]$}
            \EndIf
            \If{$A[i] = candidate$}
                \State{$count = count + 1$}
            \Else
                \State{$count = count - 1$}
            \EndIf
        \EndFor
        \State \Return{$candidate$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\subsubsection{zadanie 8}
Doktor Freud ma wahania nastrojów, które zapisuje sobie ilustrując nastrój danego dnia nieujemną liczbą całkowitą. Po $n$ dniach zgromadził tablice $n$ liczb opisujących swój nastrój i postanowił znaleść (spójny) przedział czasu (dni), w których był najszczęśliwszy. Doktor Freud zdefiniował sobie szczęśliwość przedziału czasu jako sumę wartości występujących w dniach tego przedziału przemnożony przez najmniejszą wartość występującą w zadanym przedziale. Stwórz algorytm D\&C znajdujący najszczęśliwszy przedział w złożoności $O(n \log n)$

\bigskip
\hrule
\bigskip

%TODO sprobowac zrobic samemu

\subsubsection{zadanie 9}
Wykaż, że nie istnieje algorytm sortujący, który działa w czasie liniowym dla co najmniej połowy z $n!$ możliwych danych wejściowych długości $n$. Czy odpowiedź ulegnie zmianie jeśli zapytamy o ułamek $\frac{1}{n}$ lub $\frac{1}{n^2}$ wszystkich permutacji?

\bigskip
\hrule
\bigskip

Do pokazania tego faktu skożystamy z drzewa decyzyjnego, podobnie jak na wykładzie. Rozważmy nierówność gdzie $h$-wysokość drzewa, $l$-# liści
\begin{enumerate}
    \item
        \[
            \frac{n!}{2} \leq l \leq 2^h \implies 2^{h+1} \geq n! \implies h \geq \log_2 n! -1
        \]
        zatem $h = \Omega (n \log n)$
    \item
        \[
            \frac{n!}{n} \leq 2^h \implies h \geq \log_2 n! - \log_2 n \implies h = \Omega(n \log n)
        \]
    \item
        \[
            \frac{n!}{2^n} \leq 2^h \implies h+n \geq \log_2 n! \implies h \geq log_2 n! - n \implies h = \Omega(n \log n)
        \]
\end{enumerate}

\subsubsection{zadanie 10}
Zaprojektuj algorytm, który sortuje $n$ liczb całkowitych z przedziału od $1$ do $n^2$ w czasie $O(n)$.

\bigskip
\hrule
\bigskip

\subsection{lista 5}
\subsubsection{zadanie 6}
Zaproponuj strukturę danych \mathcal{Q} dla dynamicznych zbiorów liczb, w której można wykonywać operację Min − Luka wyznaczającą odległość między dwoma najbliższymi sobie liczbami w \mathcal{Q}. Jeśli np. $\mathcal{Q} = \{1, 5, 9, 15, 18, 22\}$, to Min − Luka(\mathcal{Q}) daje w wyniku $18 − 15 = 3$. Zaimplementuj jak najefektywniej operacje Insert, Delete, Search oraz Min − Luka i wykonaj analizę ich złożoności czasowej.

\bigskip
\hrule
\bigskip

Do implementacji struktury użyjemy wzbogaconych drzew RBT o mdb i mdt, gdzie
\[
    mdt = \min\{key- Parent.key, S.key-key\}
\]
oraz
\[
    mdb = \min\{mdt, x.left.mdb, x.right.mdb\}
\]


\end{document}
